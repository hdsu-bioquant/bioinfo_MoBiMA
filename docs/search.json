[{"path":"index.html","id":"introduction","chapter":"Introduction","heading":"Introduction","text":"R powerful programming language analysis data.\nR versatile, free, easy understand.\n, unlike programming languages, R quite “human-friendly” dynamic, thus require compiling “machine-readable”.\ntype R commands console script.course, learn:Basic R programming data analysis (start, go though sections present Introduction)use R work different datasets apply statistics (Week One)annotate, analyse, obtain, pre-process, visualize genomic data (Week Two)perform RNA-seq analysis (Week Three)","code":""},{"path":"index.html","id":"where-to-start","chapter":"Introduction","heading":"Where to start","text":"","code":""},{"path":"index.html","id":"log-into-the-server","chapter":"Introduction","heading":"Log into the server","text":"use RStudio Server running virtual machine.\ninstructions connect:can find list students .\nCheck name, user name connect.","code":""},{"path":"index.html","id":"if-you-are-running-machine-with-linux-or-macos","chapter":"Introduction","heading":"if you are running machine with Linux or MacOS","text":"open terminal type following commant: ssh <USER>@194.94.113.18 -p 30210 -L 8787:127.0.0.1:8787; replace <USER> user name Google Sheetin browser, go urllog user name password provided .can now start working RStudio!","code":""},{"path":"index.html","id":"if-you-are-running-machine-with-windows","chapter":"Introduction","heading":"if you are running machine with Windows","text":", user install Putty.\nGUI, following settings needed:main panel:\nHost name (IP address): @194.94.113.18\nPort: 30210 (check right port Google Sheet)\nConnection type: SSH\nHost name (IP address): @194.94.113.18Port: 30210 (check right port Google Sheet)Connection type: SSHIn panel “Connection / SSH / Tunnel”:\nSource port: 8787\nDestination port: 127.0.0.1:8787\nCheck options “Local” “Auto”\nSource port: 8787Destination port: 127.0.0.1:8787Check options “Local” “Auto”click “Add”Now click “Open”browser, go urllog user name password provided .can now start working RStudio!","code":""},{"path":"index.html","id":"installing-r-and-rstudio","chapter":"Introduction","heading":"Installing R and RStudio","text":"Alternatively, can also choose install R RStudio.\nRstudio commonly used IDE (Integrated Development Environment) R, use R version installed computer.\nRStudio meant make R programming quite visual way easier beginner.Start installing R RStudio.RStudio composed 4 main windows/panels.\nEditor, Environment, Console, Files window/panel.\nOrder can vary.","code":""},{"path":"index.html","id":"courses","chapter":"Introduction","heading":"Courses","text":"recommended take introductory class R get know language start coding .\nStart going Chapter Getting Started Data R.\ncover basics next sections.can find resources :R Tutorial Beginners guru99R courses Babraham BioinformaticsR Data ScienceR markdown RStudio cheatsheetYoutube tutorial ggplot2 one developers","code":""},{"path":"index.html","id":"installing-and-configuring-cyberduck","chapter":"Introduction","heading":"Installing and configuring Cyberduck","text":"order access remote files (example opening pdf file), need install additional tool, called cyberduck (https://cyberduck.io/).Install Cyberduck according operating systemOnce installed, open CyberduckOn top left, click Neue Verbindung(new connection)new window, make following changes:select sftp dropdown menuin server box, type 194.94.113.18in port box, type 30210in Benutzername (Username), type username (like user1)password box: (ask !)Click Verbinden (Connect)","code":""},{"path":"index.html","id":"good-practice","chapter":"Introduction","heading":"Good practice","text":"Two notes advice good practice now journey start:Commenting: Comment code go #! way avoid forgetting meaning given line. also important want share code others. Trust , remember writing today class one week now, best reminder.Clean tidy code: See weird line middle script/markdown? meant guideline code length. code long crosses line, please find way make shorter. addition, avoid making chunks long make separate markdowns scripts class topic, keep everything organised easy track.","code":""},{"path":"index.html","id":"r-markdown","chapter":"Introduction","heading":"R Markdown","text":"R Markdown simple formatting syntax authoring HTML, PDF, MS Word documents.\nworking R Markdown course.\ndetails using R Markdown see http://rmarkdown.rstudio.com https://www.markdownguide.org/basic-syntax/.R instructions markdown file written “chunk”, one .\nChunks can added using +C icon top right corner panel/editor.\nalso keyboard shortcut .happens editor remove second parenthesis?\nsee red cross appearing?\ncan click read comment.\nhappens press “enter” second parenthesis missing?R smart usually let know know simple syntax mistake like one code.\nR also try guess variable function names input, click TAB auto-complete.","code":"\n# Sentences written after \"#\" are comments. Comments are ignored during the execution of the code.\n# print \"Hello world!\" on the screen.\nprint(\"Hello world!\")## [1] \"Hello world!\""},{"path":"index.html","id":"r-basics","chapter":"Introduction","heading":"R Basics","text":"","code":""},{"path":"index.html","id":"variable-assignment-in-r","chapter":"Introduction","heading":"Variable assignment in R","text":"can start testing one basic tasks - variable assignment.\nVariable assignments R can done using either <- symbol, = symbol.can now run print() print(hello).\nNote assignment variables (hello) listed “Environment” window RStudio.R case sensitive.\nhello known variable, Hello !can remove variables …","code":"\na <- \"Hello world!\"\n# or\nhello = \"Hello world!\"\nrm(a)"},{"path":"index.html","id":"classes","chapter":"Introduction","heading":"Classes","text":"way orange fruit Malta country, R variable corresponds class holds given structure.\nClasses Let us say generate new variable (name).\ncan know ?function str() gives structure variable.chr stands character, class variable.function class() gives class variable.","code":"\nname = \"Marie\"\nstr(name)##  chr \"Marie\"\nclass(name)## [1] \"character\""},{"path":"index.html","id":"logical-operations","chapter":"Introduction","heading":"Logical operations","text":"can perform tests simple variables using ==, >,< operators:output test either FALSE/TRUE.\ncan test using numbers :","code":"\nname == \"Harry\"## [1] FALSE\n33 > 23## [1] TRUE"},{"path":"index.html","id":"vectors","chapter":"Introduction","heading":"Vectors","text":"can make vector using c().\nTry numbers.want identify specific number vector position, can use square brackets.\nLike :","code":"\nvar1 = c(3,7,12,2)\nvar1[1]     # outputs the first element\nvar1[2:3]   # outputs the 2nd to 3rd elements\nvar1[-3]    # outputs all except the 3rd element"},{"path":"index.html","id":"matrixes","chapter":"Introduction","heading":"Matrixes","text":"matrix two-dimensional array numbers.\ndefined using matrix() function follows: Take note difference byrow=FALSE byrow=TRUE.understand parameters (“nrow”, “ncol”, “byrow”) matrix() function?\nhappens specify “byrow” TRUE FALSE?can ask dimensions matrix (see later also data frames) using function dim().\nX.access elements matrix, similar vector 2 indexes:","code":"\nx <- c(1, 2, 3, 4, 5, 6)\n\nX <- matrix(data = x, nrow = 2, ncol = 3, byrow = TRUE)\nX <- matrix(data = x, nrow = 2, ncol = 3, byrow = FALSE)\ndim(X)\nX[1, 2]                     # [1] 3  -> element in the first row and second column\nX[1:2, 1]                     # [1] 1 2 \nX[2, ]                      # [1] 2 4 6  -> all elements of the second row\nlength(X[1:2, 1])             # [1] 2  -> X[1:2, 1] is a vector of length 2"},{"path":"index.html","id":"dataframes","chapter":"Introduction","heading":"Dataframes","text":"Data frames like matrices, can contain multiple types values mixed.\ncan create data frame using data.frame() function.\ncan also convert matrix data frame .data.frame() function.can prepare three vectors length (example 4 elements) create data frame:can access specific elements similar manner matrix .\nTry examples:","code":"\nName <- c(\"Leah\", \"Alice\", \"Jonas\", \"Paula\")\nAge <- c(21, 22, 20, 22)\nCourse <- c(\"Mathematics\", \"Physics\", \"Medicine\", \"Biology\")\nPlace_of_birth <- c(\"USA\", \"Germany\", \"Germany\", \"France\")\n\nStudents <- data.frame(First_Name = Name, \n                       Age = Age, \n                       Course = Course,\n                       Place_of_birth = Place_of_birth)\nStudents##   First_Name Age      Course Place_of_birth\n## 1       Leah  21 Mathematics            USA\n## 2      Alice  22     Physics        Germany\n## 3      Jonas  20    Medicine        Germany\n## 4      Paula  22     Biology         France\n# Columns\nStudents$Age \nStudents[[\"Age\"]]\n\n# Rows or columns\nStudents[, 3]\nStudents[3,]\n\n# Elements\nStudents$Course[3]    # access element 3 of the column \"Course\"\nStudents[2:3, 3]      # select elements 2 and 3 of column 3 "},{"path":"index.html","id":"apply-sapply-lapply","chapter":"Introduction","heading":"Apply, sapply, lapply","text":"Apply() used repeat operation columns/rows matrix data frame.\nTherefore, need specify three parameters:\n1.\nmatrix operation performed\n2.\nWhether repeat operation rows, defined “1”, columns (use “2” instead)\n3.\noperation performed.Let’s look example make clear: want determine minimum row matrix “Mat”.\nTherefore, can use function min().happens change “1” apply function “2”?\nTry !Remember can use help(apply) ?apply get help function!can perform loop elements vector list using sapply() function.sapply() needs two information:\n1.\nvector consider?\n2.\nfunction want apply element vector?sapply() return vector containing results.instance, can calculate square root every element vector using function sqrt():Instead using built-functions, can also write little function combine sapply(), apply() lapply() (see ):Besides, can use sapply() list, example calculate length every element using length():can see, returns vector length every list element.However, sometimes can useful keep results stored list.\nTherefore, can use lapply() instead.\nLet’s look difference sapply() lapply() using example :understand difference?","code":"\n#First, we build the matrix \nm <- rnorm(30)   # generate 30 random numbers for the matrix\nMat <- matrix(data = m, nrow = 6)\nMat##             [,1]       [,2]       [,3]       [,4]        [,5]\n## [1,] -1.02514989  0.3439573  0.5845802  0.1796378  0.83292126\n## [2,]  0.05968511  1.0870897  0.1410992 -0.6287266  0.53946227\n## [3,]  0.94046011  1.4550564 -1.8675303  0.2393706 -0.04080227\n## [4,] -0.23025249 -0.1561688  0.9957699 -0.1927845  0.80546112\n## [5,] -1.09576303  1.2134179 -0.4029791  0.6562391 -0.94874904\n## [6,]  0.24562082  0.3362266  1.4434458 -1.5193605  0.99635501\n#Next, we want to determine the minimum value of each row: \napply(Mat, 1, min)## [1] -1.0251499 -0.6287266 -1.8675303 -0.2302525 -1.0957630 -1.5193605\nx <- c(1:5)\nsapply(x, sqrt)## [1] 1.000000 1.414214 1.732051 2.000000 2.236068\n# sapply()\nz <- c(1:5)\nsapply(z, function(x) {x*2}) ## [1]  2  4  6  8 10\n# apply()\nMat ##             [,1]       [,2]       [,3]       [,4]        [,5]\n## [1,] -1.02514989  0.3439573  0.5845802  0.1796378  0.83292126\n## [2,]  0.05968511  1.0870897  0.1410992 -0.6287266  0.53946227\n## [3,]  0.94046011  1.4550564 -1.8675303  0.2393706 -0.04080227\n## [4,] -0.23025249 -0.1561688  0.9957699 -0.1927845  0.80546112\n## [5,] -1.09576303  1.2134179 -0.4029791  0.6562391 -0.94874904\n## [6,]  0.24562082  0.3362266  1.4434458 -1.5193605  0.99635501\napply(Mat, 1, function(x) {sum(x*2)}) #the operation is performed on every row of \"Mat\"; every element is multiplied by two and the sum of the row is calculated## [1]  1.831893  2.397219  1.453109  2.444050 -1.155668  3.004575\nList1 <- list(color = c(\"blue\", \"red\"), size = 5, state = c(TRUE, FALSE, TRUE, TRUE))\n\nsapply(List1, length)## color  size state \n##     2     1     4\nlapply(List1, length)## $color\n## [1] 2\n## \n## $size\n## [1] 1\n## \n## $state\n## [1] 4"},{"path":"index.html","id":"for-loops","chapter":"Introduction","heading":"For Loops","text":"Besides apply-family, can use loops iterating sequence:","code":"\nx <- c(\"a\", \"b\", \"c\")\n\nfor (i in x) {\n  print(i)\n}## [1] \"a\"\n## [1] \"b\"\n## [1] \"c\"\n#or \nfor (i in 1:4) {\n  print(i*3)\n}## [1] 3\n## [1] 6\n## [1] 9\n## [1] 12"},{"path":"index.html","id":"if-statements","chapter":"Introduction","heading":"If Statements","text":"can use statement execute block code , condition TRUE.Modify “” “b” see output changes!can also add else statement.\ncase, condition FALSE, else condition tried:Besides, can add else statement executed previous conditions TRUE.understand works?\nhappen = 10 b = 10?\nTry !","code":"\na <- 5\nb <- 10\n\nif (a < b) {\n  print(\"a is smaller than b\")\n}## [1] \"a is smaller than b\"\na <- 10\nb <- 10\n\nif (a < b) {\n  print(\"a is smaller than b\")\n} else if (a == b) {\n  print(\"a is equal to b\")\n}## [1] \"a is equal to b\"\na <- 10\nb <- 5\n\nif (a < b) {\n  print(\"a is smaller than b\")\n} else if (a == b) {\n  print(\"a is equal to b\")\n} else {\n  print(\"a is greater than b\")\n}## [1] \"a is greater than b\""},{"path":"index.html","id":"data-wrangling-with-tidyverse","chapter":"Introduction","heading":"Data wrangling with tidyverse","text":"","code":""},{"path":"index.html","id":"introducing-tidyverse","chapter":"Introduction","heading":"Introducing tidyverse","text":"Tidyverse set packages often used manipulate data.\ncan think tidyverse variation original R, meant grammar specific data manipulation.can install :load usingYou can find R packages CRAN.\nCRAN large repository containing almost 20 thousand packages!\ncourse, just use CRAN packages.\ninstall , use install.packages().learn couple functions framework make next tasks lot easier.\nTidyverse works tibble class, similar dataframe class, faster tidier.Start loading default tibble including sleeping patterns different mammals.\nalso includes information animal order, genus, diet, body weight.\n, run following:can start inspecting msleep.can print first rows dataframe using head().\nTry .","code":"\ninstall.packages(\"tidyverse\")\nlibrary(tidyverse)## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n## ✔ dplyr     1.1.3     ✔ readr     2.1.4\n## ✔ forcats   1.0.0     ✔ stringr   1.5.0\n## ✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n## ✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n## ✔ purrr     1.0.2     \n## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n## ✖ dplyr::filter() masks stats::filter()\n## ✖ dplyr::lag()    masks stats::lag()\n## ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\ndata(msleep)\nglimpse(msleep)\nhead(msleep)## # A tibble: 6 × 11\n##   name    genus vore  order conservation sleep_total sleep_rem sleep_cycle awake\n##   <chr>   <chr> <chr> <chr> <chr>              <dbl>     <dbl>       <dbl> <dbl>\n## 1 Cheetah Acin… carni Carn… lc                  12.1      NA        NA      11.9\n## 2 Owl mo… Aotus omni  Prim… <NA>                17         1.8      NA       7  \n## 3 Mounta… Aplo… herbi Rode… nt                  14.4       2.4      NA       9.6\n## 4 Greate… Blar… omni  Sori… lc                  14.9       2.3       0.133   9.1\n## 5 Cow     Bos   herbi Arti… domesticated         4         0.7       0.667  20  \n## 6 Three-… Brad… herbi Pilo… <NA>                14.4       2.2       0.767   9.6\n## # ℹ 2 more variables: brainwt <dbl>, bodywt <dbl>"},{"path":"index.html","id":"filter-based-on-conditions","chapter":"Introduction","heading":"Filter based on conditions","text":"Now know dataset looks like, can start .\nLet us see information domestic pig.\ncan using filter() genus column:also use %>% operator .\npipe operator use often.understand piping, let’s imagine making three layer cake factory.\nInstead manually carrying cake one station another add layers frosting, conveyor belt transporting cake stations automatically make process much efficient.\nPiping avoids create multiple intermediate objects achieve final result performing multiple functions.Try filtering columns based numeric conditions.\nUse filter() see 7 animals datasets body weight superior 200 (kg).","code":"\nmsleep %>%\n  filter(genus == \"Sus\")## # A tibble: 1 × 11\n##   name  genus vore  order   conservation sleep_total sleep_rem sleep_cycle awake\n##   <chr> <chr> <chr> <chr>   <chr>              <dbl>     <dbl>       <dbl> <dbl>\n## 1 Pig   Sus   omni  Artiod… domesticated         9.1       2.4         0.5  14.9\n## # ℹ 2 more variables: brainwt <dbl>, bodywt <dbl>"},{"path":"index.html","id":"select-specific-columns","chapter":"Introduction","heading":"Select specific columns","text":"Now, let’s say want select specific columns.\nUse select() function select columns name, order, bodywt (body weight).","code":"\nbody_weights = msleep %>%\n  select(name, order, bodywt)\n\nhead(body_weights)## # A tibble: 6 × 3\n##   name                       order         bodywt\n##   <chr>                      <chr>          <dbl>\n## 1 Cheetah                    Carnivora     50    \n## 2 Owl monkey                 Primates       0.48 \n## 3 Mountain beaver            Rodentia       1.35 \n## 4 Greater short-tailed shrew Soricomorpha   0.019\n## 5 Cow                        Artiodactyla 600    \n## 6 Three-toed sloth           Pilosa         3.85"},{"path":"index.html","id":"operations-on-groups","chapter":"Introduction","heading":"Operations on groups","text":"can assess average (mean()) body weight animal order present.\n, use summarise() group_by() functions body_weights generated previously.group_by(): Assigns group given set rows (observations).summarise(): Makes calculations given group based row values assigned .","code":"\nbody_weights %>% \n  group_by(order) %>%\n  summarise(mean_bweight = mean(bodywt, na.rm = T)) # na.rm = T will exclude missing values (NA)## # A tibble: 19 × 2\n##    order           mean_bweight\n##    <chr>                  <dbl>\n##  1 Afrosoricida          0.9   \n##  2 Artiodactyla        282.    \n##  3 Carnivora            57.7   \n##  4 Cetacea             342.    \n##  5 Chiroptera            0.0165\n##  6 Cingulata            31.8   \n##  7 Didelphimorphia       1.03  \n##  8 Diprotodontia         1.36  \n##  9 Erinaceomorpha        0.66  \n## 10 Hyracoidea            3.06  \n## 11 Lagomorpha            2.5   \n## 12 Monotremata           4.5   \n## 13 Perissodactyla      305.    \n## 14 Pilosa                3.85  \n## 15 Primates             13.9   \n## 16 Proboscidea        4600.    \n## 17 Rodentia              0.288 \n## 18 Scandentia            0.104 \n## 19 Soricomorpha          0.0414"},{"path":"index.html","id":"reshaping-data","chapter":"Introduction","heading":"Reshaping data","text":"tidyverse, objects working sometimes tidy format.\nthree rules make dataset tidy:variable column.observation row.value cell.Image sourceHow can convert ?\nmammalian sleep dataset used earlier already tidy format, generate example.testing new diet 7 cats, thus registering weight 1 month diet.example, can see 2 different variables unit (weight), gatherable columns.\ncan make dataframe tidy using gather() tidyr package:understand differences?\nNote now name shows twice.","code":"\ncat.weights <- data.frame(CatName=c('Muffin', 'Lily', 'Mittens', 'Oreo', 'Loki', 'Fluffy', 'Honey'),\n                          month0=c(4.3, 4.5, 5, 4.4, 3.8, 5.5, 4.5),\n                          month1=c(4.4, 4.5, 4.9, 4.3, 3.9, 5.4, 4.6))\n\ncat.weights##   CatName month0 month1\n## 1  Muffin    4.3    4.4\n## 2    Lily    4.5    4.5\n## 3 Mittens    5.0    4.9\n## 4    Oreo    4.4    4.3\n## 5    Loki    3.8    3.9\n## 6  Fluffy    5.5    5.4\n## 7   Honey    4.5    4.6\nlibrary(tidyr)\n\ncat.weights %>% \n  gather(key = Time, # column name given to the gathered column names\n         value = Weight, # column name will be given to the values\n         month0:month1) # columns to gather##    CatName   Time Weight\n## 1   Muffin month0    4.3\n## 2     Lily month0    4.5\n## 3  Mittens month0    5.0\n## 4     Oreo month0    4.4\n## 5     Loki month0    3.8\n## 6   Fluffy month0    5.5\n## 7    Honey month0    4.5\n## 8   Muffin month1    4.4\n## 9     Lily month1    4.5\n## 10 Mittens month1    4.9\n## 11    Oreo month1    4.3\n## 12    Loki month1    3.9\n## 13  Fluffy month1    5.4\n## 14   Honey month1    4.6"},{"path":"index.html","id":"plotting-using-ggplot2","chapter":"Introduction","heading":"Plotting using ggplot2","text":"One many advantages tidyverse compatible ggplot2.\nThanks wonders %>% operator, can generate basic plot instantly gathered/tidy tibble without creating intermediate objects.structure ggplot() always based two essential elements: aesthetics (aes()) plot layers (named geom_something(). aes() can found within ggplot() function geom_something(). multiple options geom_X() depending information plotting.\ncan find possibilities .aes() function can harbor information axis (x, y) aesthetics.\nLet us see example within scatter plot next.course work ggplot(), plots can also generated another framework.\nalternative ggplot() plot().\nBeware looking solutions data visualization problems.","code":""},{"path":"index.html","id":"scatter-plot","chapter":"Introduction","heading":"Scatter plot","text":"example, want see relationship sleep time (total, hours) body weights, use formula:x-axis looks little squished, hard view many trends.\ncan log-transform scale using log() function.\nTry !can try play aesthetics plot.\nexample, can use colour easthetic colour dots based vore/diet.","code":"\nggplot(msleep,                    # start by picking the dataframe to plot\n       aes(x = bodywt,            # column to use for x-axis\n           y = sleep_total)) +    # column to use for y-axis\n  geom_point()                    # plot layer for points\nggplot(msleep,                    \n       aes(x = log(bodywt),            # column to use for x-axis\n           y = sleep_total,            # column to use for y-axis\n           colour = vore)) +           # column used for colouring dots\n  geom_point() "},{"path":"index.html","id":"histogram","chapter":"Introduction","heading":"Histogram","text":"Let us start making histogram, work distributions course.\nwant see global distribution sleep time (total, hours) animals dataset, use geom_histogram().\nLike :cases, data tidy tibble rather vector example.\ncases, structure slightly different.\nCheck can plot distribution random set numbers.Try changing option bins.\nsee?","code":"\nggplot(msleep,                    # start by picking the dataframe to plot\n       aes(x = sleep_total)) +    # column to use for y-axis\n  geom_histogram()                # plot layer for histogram\nrand.numbers = rnorm(100) # we will learn this function later on\n\nggplot() +   \n  geom_histogram(aes(x = rand.numbers),  # aes goes in the plot layer for histogram\n                 bins = 50)"},{"path":"index.html","id":"boxplot","chapter":"Introduction","heading":"Boxplot","text":"next example, creating boxplot sleep time (total, hours) dietary group (vore).See NA x-axis?\ncourse (week one) learn clean data avoid missing values like ones.Try geom_ layers available.\nexample, geom_violin() can used similar way geom_boxplot().","code":"\nggplot(msleep, # we use na.omit here to hide NAs from the plot\n       aes(x = vore,\n           y = sleep_total)) +\n  geom_boxplot()"},{"path":"index.html","id":"common-errors","chapter":"Introduction","heading":"Common errors","text":"Error library(viridis) : package called ‘viridis’:Solution: install required package using install.packages(\"package\") load using library(package).Error gather : find function \"gather\":Solution: load package function belongs .\ncan see running ??gatherError fortify(): ! data must <data.frame>, object coercible fortify(), <uneval> object.:Solution: aes() within ggplot() geom_...().","code":""},{"path":"index.html","id":"introductory-exercises","chapter":"Introduction","heading":"Introductory exercises","text":"","code":""},{"path":"index.html","id":"exercise-1-vectors","chapter":"Introduction","heading":"Exercise 1: Vectors","text":"Create vector named “test_scores” containing test scores 75, 90, 65, 68 83 students.Create vector named “test_scores” containing test scores 75, 90, 65, 68 83 students.Create vector “passing” contains TRUE test scores 70 FALSE scores equal 70.\nHint: Remember can perform tests vector elements.\nUse sum() check many students passed test.Create vector “passing” contains TRUE test scores 70 FALSE scores equal 70.\nHint: Remember can perform tests vector elements.\nUse sum() check many students passed test.(expert) Can create vector “high_scores” contains scores 70, using results obtained 1.\n2.?(expert) Can create vector “high_scores” contains scores 70, using results obtained 1.\n2.?","code":""},{"path":"index.html","id":"exercise-2-matrices","chapter":"Introduction","heading":"Exercise 2: Matrices","text":"Create matrix looks like :Select different parts matrix, example:\nelement second row first column\nsecond third element second column\nelements last column\nTry something else!\nSelect different parts matrix, example:element second row first columnthe second third element second columnall elements last columnTry something else!Add new column matrix using cbind().Add new column matrix using cbind().Can name columns matrix “”, “B”, “C” “D” using colnames()?Can name columns matrix “”, “B”, “C” “D” using colnames()?","code":"\n#      [,1] [,2] [,3]\n# [1,]    2    4    6\n# [2,]    8    10   12\n# [3,]    3    5    7\n# [4,]    2    9    11"},{"path":"index.html","id":"exercise-3-data-frames","chapter":"Introduction","heading":"Exercise 3: Data Frames","text":"Create data frame vectors, three columns five rows:Select column “Fruit”.\nmany Fruits ?\nHint: Remember function sum().Select column “Fruit”.\nmany Fruits ?\nHint: Remember function sum().Add column named “Berry” containing TRUE (products berries) FALSE data frame.Add column named “Berry” containing TRUE (products berries) FALSE data frame.","code":"\nProduct_name <- c(\"orange\", \"strawberry\", \"broccoli\", \"blueberry\", \"cucumber\")\nFruit <- c(TRUE, TRUE, FALSE, TRUE, FALSE)\nColor <- c(\"orange\", \"red\", \"green\", \"blue\", \"green\")"},{"path":"lecture-files.html","id":"lecture-files","chapter":"1 Lecture files","heading":"1 Lecture files","text":"Day 1 - data types/graphs/correlation [slides]Day 2 - clustering / principal compoment analysis [slides]","code":""},{"path":"week-1-data-analysis.html","id":"week-1-data-analysis","chapter":"Week 1 : data analysis","heading":"Week 1 : data analysis","text":"","code":""},{"path":"week-1-data-analysis.html","id":"day-1-descriptive-statistics-and-data-types","chapter":"Week 1 : data analysis","heading":"Day 1: Descriptive statistics and data types","text":"Today, learn perform basic tasks dataframe/tibble, descriptive statistics, perform data cleaning, plotting.","code":""},{"path":"week-1-data-analysis.html","id":"data-features-and-where-to-find-them","chapter":"Week 1 : data analysis","heading":"1.0.1 Data features and where to find them","text":"","code":""},{"path":"week-1-data-analysis.html","id":"load-the-data","chapter":"Week 1 : data analysis","heading":"1.0.1.1 Load the data","text":"diabetes dataset, using practical class downloaded online repository.\nload R sneak peek looks like console.\nfollowing see several functions give us information dataset.","code":"\ndat = as_tibble(read.delim('https://tinyurl.com/y4fark9g')) # Load the dataset\nhead(dat, 10) # Look at the first 10 lines of the table## # A tibble: 10 × 19\n##       id  chol stab.glu   hdl ratio glyhb location     age gender height weight\n##    <int> <int>    <int> <int> <dbl> <dbl> <chr>      <int> <chr>   <int>  <int>\n##  1  1000   203       82    56  3.60  4.31 Buckingham    46 female     62    121\n##  2  1001   165       97    24  6.90  4.44 Buckingham    29 female     64    218\n##  3  1002   228       92    37  6.20  4.64 Buckingham    58 female     61    256\n##  4  1003    78       93    12  6.5   4.63 Buckingham    67 male       67    119\n##  5  1005   249       90    28  8.90  7.72 Buckingham    64 male       68    183\n##  6  1008   248       94    69  3.60  4.81 Buckingham    34 male       71    190\n##  7  1011   195       92    41  4.80  4.84 Buckingham    30 male       69    191\n##  8  1015   227       75    44  5.20  3.94 Buckingham    37 male       59    170\n##  9  1016   177       87    49  3.60  4.84 Buckingham    45 male       69    166\n## 10  1022   263       89    40  6.60  5.78 Buckingham    55 female     63    202\n## # ℹ 8 more variables: frame <chr>, bp.1s <int>, bp.1d <int>, bp.2s <int>,\n## #   bp.2d <int>, waist <int>, hip <int>, time.ppn <int>"},{"path":"week-1-data-analysis.html","id":"dimensions-and-naming","chapter":"Week 1 : data analysis","heading":"1.0.1.2 Dimensions and naming","text":"1. dimension dataset (.e. many rows/columns data)2. column names datasetProbably confused column names mean.\ndescription values look ","code":"\n# Dimension\ndim(dat)## [1] 403  19\n# Number of columns\nncol(dat)\n# Number of rows\nnrow(dat)\ncolnames(dat) # Similarly rownames() for rows##  [1] \"id\"       \"chol\"     \"stab.glu\" \"hdl\"      \"ratio\"    \"glyhb\"   \n##  [7] \"location\" \"age\"      \"gender\"   \"height\"   \"weight\"   \"frame\"   \n## [13] \"bp.1s\"    \"bp.1d\"    \"bp.2s\"    \"bp.2d\"    \"waist\"    \"hip\"     \n## [19] \"time.ppn\""},{"path":"week-1-data-analysis.html","id":"numerical-features","chapter":"Week 1 : data analysis","heading":"1.0.1.3 Numerical features","text":"3. extract minimum maximum age patients dataset?Can find height weight?4. overall summary entire dataset look like?Can explain see run summary() function?Feel free play around syntax feel comfortable .\ncan open window View(dat) compare results.","code":"\nmin(dat$age)\nmax(dat$age)\nrange(dat$age)\nsummary(dat)"},{"path":"week-1-data-analysis.html","id":"data-cleaning","chapter":"Week 1 : data analysis","heading":"1.0.2 Data cleaning","text":"often first thing one needs data science project clean raw data transform format readily understood easy use downstream analysis.\nprocess usually involves: –Removing empty value rows/columnsRemoving unused unnecessary rows/columnsReordering data matrixKeeping columns uniformly numeric (age, weight etc) string (names, places etc) logical (TRUE/FALSE, 1/0)Handling strange caveats data specific like replacing , ., ; numbers etcLets clean diabetes dataWe make id column row names dataset;remove bp.2s bp.2d columns mostly missing values (see summary );also remove column time.ppn required analysis;reorder columns data qualitative quantitative values separated.perform cleanup, need couple important functions, first discuss:filteris.namutateacross%%","code":""},{"path":"week-1-data-analysis.html","id":"filter","chapter":"Week 1 : data analysis","heading":"1.0.2.1 filter()","text":"filter() used dataset filter rows satisfying condition specify like saw previously (Introduction).\nLet’s look example. filtering senior individuals dataset.can also filter based conditions, like location, sex, among others.\ncases, can also use () filter values. syntax different…… works vectors classes. Let’s see next example.selected animals “animals” vector correspond three individuals “number” vector.","code":"\ndat_seniors = dat %>%\n  filter(age <= 65)\ndat[dat$age <= 65,]\n# number of animals you have\nnumber = c(2,3,4,5,1,2,5)\n# Let's create a different vector (of the same length)\nanimals = c(\"cat\", \"dog\", \"cow\", \"parrot\", \"zebra\", \"sparrow\", \"lizard\")\n# Let's use the \"which()\" function now\nanimals[which(number > 2)]## [1] \"dog\"    \"cow\"    \"parrot\" \"lizard\""},{"path":"week-1-data-analysis.html","id":"is.na","chapter":"Week 1 : data analysis","heading":"1.0.2.2 is.na()","text":".na() used determine NA values present given object. can try simple example one variable assigned NA.can vectors obtained dat. class output ?","code":"\nx = 2\nis.na(x)## [1] FALSE\ny = NA\nis.na(y)## [1] TRUE\nis.na(dat$glyhb)"},{"path":"week-1-data-analysis.html","id":"mutate","chapter":"Week 1 : data analysis","heading":"1.0.2.3 mutate()","text":"mutate() often used create new column based another column dataframe. Let us use function mutate two new columns including weight kilograms height centimeters. conversion pounds kilograms can done multiplying weight pounds 0.454. covert height centimeters need multiply height (inches) 2.54.","code":"\ndat %>%\n  mutate(weight.kg = weight * 0.454,        # you can generate both columns using the same mutate!\n         height.cm = height * 2.54) %>%     # we do not need to save this output\n  select(id, weight.kg, height.cm)## # A tibble: 403 × 3\n##       id weight.kg height.cm\n##    <int>     <dbl>     <dbl>\n##  1  1000      54.9      157.\n##  2  1001      99.0      163.\n##  3  1002     116.       155.\n##  4  1003      54.0      170.\n##  5  1005      83.1      173.\n##  6  1008      86.3      180.\n##  7  1011      86.7      175.\n##  8  1015      77.2      150.\n##  9  1016      75.4      175.\n## 10  1022      91.7      160.\n## # ℹ 393 more rows"},{"path":"week-1-data-analysis.html","id":"across","chapter":"Week 1 : data analysis","heading":"1.0.2.4 across()","text":"across() often used together mutate() another helper function, like everywhere(), starts_with(), ends_with(), contains().\nLater, use across() together functions learned previously remove NAs like :much unpack :rowwise() ensures next operations applied row.mutate() adds new column called na_count dataframe.across(everything()) selects columns current row.sum(.na(...)) calculates sum missing values row.Try run previous example without rowwise(). look like?","code":"\ndat %>%\n  rowwise() %>%\n  mutate(na_count = sum(is.na(across(everything()))))"},{"path":"week-1-data-analysis.html","id":"in","chapter":"Week 1 : data analysis","heading":"1.0.2.5 %in%","text":"operator check elements first vector inside second vector.","code":"\nc('Frodo','Sam') %in% c('Pippin','Sam','Frodo','Merry')## [1] TRUE TRUE"},{"path":"week-1-data-analysis.html","id":"ready-for-the-cleaning","chapter":"Week 1 : data analysis","heading":"1.0.2.6 Ready for the cleaning!","text":"first column dataframe column name “id”.\nrows just numbered, without names.\ngoing rename rows using column “id”. function column_to_rownames() allows us efficiently.Keep mind rownames must unique!na_count column include number NAs per row. understand works?\nfinally apply filter keep rows less 2 NAs.also remove na_count problematic columns (bp.2s, bp.2d time.ppn) selecting ones . can using !, character can used invert results. Let us try select().Next, can re-order remaining columns, order put categorical columns first, numerical columns . can use select order columns , need combine () functions verify class columns, like .character() .numeric().simple example:can apply principle re-ordering:Now lets look cleaned data:Hold , ordering selection columns looks right, seems certain rows missing values still (like glyhb column 3 NA values still).\nLets remove rows missing value using na.omit().\nRemember, 1 row = 1 patient.many patients removed associated missing values?Now cleaned data missing values, columns cleanly ordered column right formatCan identify types data (continuous, discrete etc) column represents ?","code":"\n# set the row names using the column id\ndat = dat %>%\n  column_to_rownames(var = 'id')\ndat = dat %>%\n  rowwise() %>%\n  mutate(na_count = sum(is.na(across(everything())))) %>%\n  filter(na_count <= 2)\ndat = dat %>%\n  select(!c(na_count, time.ppn, bp.2d, bp.2s))\n# Create a character and numeric \nname = c(\"Antonia\")\nage = c(23)\n\n# Verify if the previous object are from the character/numeric classes\nis.character(name)\nis.character(age)\nis.numeric(age)\ndat <- dat %>%\n  select(\n    # Select categorical columns\n    where(is.character), \n    # Select numerical columns\n    where(is.numeric)\n  )\n\n# OR you can use the indexes too, but if you more than 10-20 columns, that is not ideal\n# dat = dat[,c(8,6,11,9,10,14,15,2,5,1,3,4,12,13)]\nsummary(dat)##    location            gender             frame                chol      \n##  Length:377         Length:377         Length:377         Min.   : 78.0  \n##  Class :character   Class :character   Class :character   1st Qu.:179.0  \n##  Mode  :character   Mode  :character   Mode  :character   Median :204.0  \n##                                                           Mean   :208.2  \n##                                                           3rd Qu.:230.0  \n##                                                           Max.   :443.0  \n##                                                                          \n##     stab.glu          hdl             ratio            glyhb       \n##  Min.   : 48.0   Min.   : 12.00   Min.   : 1.500   Min.   : 2.680  \n##  1st Qu.: 81.0   1st Qu.: 38.00   1st Qu.: 3.200   1st Qu.: 4.390  \n##  Median : 90.0   Median : 46.00   Median : 4.200   Median : 4.860  \n##  Mean   :107.3   Mean   : 50.36   Mean   : 4.538   Mean   : 5.594  \n##  3rd Qu.:108.0   3rd Qu.: 59.00   3rd Qu.: 5.400   3rd Qu.: 5.622  \n##  Max.   :385.0   Max.   :120.00   Max.   :19.300   Max.   :16.110  \n##                                                    NA's   :3       \n##       age           height          weight          bp.1s      \n##  Min.   :19.0   Min.   :52.00   Min.   : 99.0   Min.   : 90.0  \n##  1st Qu.:34.0   1st Qu.:63.00   1st Qu.:151.0   1st Qu.:122.0  \n##  Median :45.0   Median :66.00   Median :174.0   Median :136.0  \n##  Mean   :46.9   Mean   :66.02   Mean   :178.1   Mean   :137.4  \n##  3rd Qu.:60.0   3rd Qu.:69.00   3rd Qu.:200.0   3rd Qu.:148.0  \n##  Max.   :92.0   Max.   :76.00   Max.   :325.0   Max.   :250.0  \n##                                                                \n##      bp.1d            waist            hip       \n##  Min.   : 48.00   Min.   :26.00   Min.   :30.00  \n##  1st Qu.: 75.00   1st Qu.:33.00   1st Qu.:39.00  \n##  Median : 82.00   Median :37.00   Median :42.00  \n##  Mean   : 83.69   Mean   :37.95   Mean   :43.08  \n##  3rd Qu.: 92.00   3rd Qu.:41.25   3rd Qu.:46.00  \n##  Max.   :124.00   Max.   :56.00   Max.   :64.00  \n##                   NA's   :1       NA's   :1\ndat = dat %>%\n  na.omit()\nsummary(dat)##    location            gender             frame                chol      \n##  Length:367         Length:367         Length:367         Min.   : 78.0  \n##  Class :character   Class :character   Class :character   1st Qu.:179.0  \n##  Mode  :character   Mode  :character   Mode  :character   Median :204.0  \n##                                                           Mean   :207.5  \n##                                                           3rd Qu.:229.0  \n##                                                           Max.   :443.0  \n##     stab.glu          hdl             ratio            glyhb       \n##  Min.   : 48.0   Min.   : 12.00   Min.   : 1.500   Min.   : 2.680  \n##  1st Qu.: 81.0   1st Qu.: 38.00   1st Qu.: 3.200   1st Qu.: 4.390  \n##  Median : 90.0   Median : 46.00   Median : 4.200   Median : 4.860  \n##  Mean   :107.3   Mean   : 50.28   Mean   : 4.536   Mean   : 5.602  \n##  3rd Qu.:108.0   3rd Qu.: 59.00   3rd Qu.: 5.400   3rd Qu.: 5.630  \n##  Max.   :385.0   Max.   :120.00   Max.   :19.300   Max.   :16.110  \n##       age            height          weight          bp.1s      \n##  Min.   :19.00   Min.   :52.00   Min.   : 99.0   Min.   : 90.0  \n##  1st Qu.:34.00   1st Qu.:63.00   1st Qu.:151.0   1st Qu.:121.5  \n##  Median :45.00   Median :66.00   Median :174.0   Median :136.0  \n##  Mean   :46.68   Mean   :66.05   Mean   :178.1   Mean   :137.1  \n##  3rd Qu.:60.00   3rd Qu.:69.00   3rd Qu.:200.0   3rd Qu.:148.0  \n##  Max.   :92.00   Max.   :76.00   Max.   :325.0   Max.   :250.0  \n##      bp.1d           waist            hip       \n##  Min.   : 48.0   Min.   :26.00   Min.   :30.00  \n##  1st Qu.: 75.0   1st Qu.:33.00   1st Qu.:39.00  \n##  Median : 82.0   Median :37.00   Median :42.00  \n##  Mean   : 83.4   Mean   :37.93   Mean   :43.04  \n##  3rd Qu.: 92.0   3rd Qu.:41.50   3rd Qu.:46.00  \n##  Max.   :124.0   Max.   :56.00   Max.   :64.00"},{"path":"week-1-data-analysis.html","id":"visualizing-data-distribution","chapter":"Week 1 : data analysis","heading":"1.0.3 Visualizing data distribution","text":"section also learn essential functions plot data intuitive useful way using ggplot2 package, just like introductory section tidyverse.","code":""},{"path":"week-1-data-analysis.html","id":"histograms","chapter":"Week 1 : data analysis","heading":"1.0.3.1 Histograms","text":"can plot column “stab.glu” histogram using hist() function:Add parameter bins = 50 lines code (inside geom_histogram) see happens.\nTry different values bins like 10, 20, 75, 100. Can interpret differences?\ngood bad thing histograms?","code":"\nggplot(dat,\n       aes(x = stab.glu)) +\n  geom_histogram() +\n  labs(x = \"Stabilized Glucose concentration in blood\",  # add labels to the x-axis\n       title = \"Glucose concentration\")                  # add title"},{"path":"week-1-data-analysis.html","id":"density-plots","chapter":"Week 1 : data analysis","heading":"1.0.3.2 Density plots","text":"density plots, use geom_density() function estimate probability density function given variable.","code":"\nggplot(dat,\n       aes(x = stab.glu)) +\n  geom_density() +\n  labs(x = \"Stabilized Glucose concentration in blood\",  # add labels to the x-axis\n       title = \"Glucose concentration\")                  # add title"},{"path":"week-1-data-analysis.html","id":"boxplots","chapter":"Week 1 : data analysis","heading":"1.0.3.3 Boxplots","text":"boxplot() function produces boxplot given variable:Can explain features graph, upper/lower whisker, 25% quantile, …?","code":"\nggplot(dat,\n       aes(x = stab.glu)) +\n  geom_boxplot() +\n  labs(x =\"Stabilized Glucose concentration in blood\")"},{"path":"week-1-data-analysis.html","id":"qq-plots","chapter":"Week 1 : data analysis","heading":"1.0.3.4 QQ-plots","text":"can use QQ-plots either (1) compare two distributions, (2) compare distribution theoretical distribution (typically normal distribution).can example compare distribution blood pressure values check normally distributedNow can use function geom_qq() generate QQ-plot distribution standard normal distribution:Using additional command geom_qq_line(), can add straight line goes first third quartile:, distribution normal??Now let’s compare quantiles cholesterol values biological sex.\nNotes ggplot() : Rather ggplot(dataset, aes(...)) use ggplot() + geom_xx(aes(...)) situations data wish plot dataframe.","code":"\n## Let's first make a histogram\nggplot(dat,\n       aes(x = bp.1s)) +\n  geom_histogram(bins = 50)\nggplot(dat,\n       aes(sample = bp.1s)) +     # we use sample= inside aes for the QQ-plot\n  geom_qq()                       # creates the QQ-plot\nggplot(dat,\n       aes(sample = bp.1s)) +     # we use sample= inside aes for the QQ-plot\n  geom_qq() +\n  geom_qq_line(colour = 'red')                  # adds in the QQ-line on top\n# We can use \"filter()\" to filter the cholesterol values for men and women\ndat.male = dat %>%\n  filter(gender == 'male')\n\ndat.female = dat %>%\n  filter(gender == 'female')\n\n# Compute the quantiles (note the \"na.rm\" option to ignore missing NA values!)\nq.male = quantile(dat.male$bp.1s, \n                  probs=seq(0,1,by=0.05), \n                  na.rm=TRUE)\nq.female = quantile(dat.female$bp.1s, \n                    probs=seq(0,1,by=0.05),\n                    na.rm=TRUE)\n\n# Now plot against each other!\nggplot() +\n  geom_point(aes(x = q.male, y = q.female)) +\n  labs(title = \"Quantiles\", x = \"Male quantiles\", y = \"Female quantiles\")"},{"path":"week-1-data-analysis.html","id":"correlation","chapter":"Week 1 : data analysis","heading":"1.0.4 Correlation","text":"","code":""},{"path":"week-1-data-analysis.html","id":"measuring-the-centrality-in-data","chapter":"Week 1 : data analysis","heading":"1.0.4.1 Measuring the centrality in data","text":"begin, think back mean, median quantiles saw boxplot. remember terms mean? asymmetrical distribution influence mean median?\nalready seen summary() quantile() functions R can compute mean, median quantiles given data.Calculate mean median continuous numeric data diabetes dataset measure difference . () difference mean median? (b) think larger differences almost difference others?","code":"\nmean(dat$stab.glu) \nmedian(dat$stab.glu) \nquantile(dat$stab.glu) "},{"path":"week-1-data-analysis.html","id":"association-between-variables","chapter":"Week 1 : data analysis","heading":"1.0.4.2 Association between variables","text":"Often common step data analysis project find associations variables present dataset. associations helps us decipher underlying structure data.instance, diabetes dataset expect high correlation free blood glucose levels glycosylated blood levels waist hip sizes. One common ways measuring associations correlations.Let us start producing scatter plot pair variables:suspect two variables relationship? scatter plot pairs numerical variables!now can compute correlation two variables. can compute Pearson correlation Spearman correlation:Spearman correlation seems much lower, right? understand , can scatter plot ranks two variables:understand usage ranks ? Run `rank()`` vector like c(3,5,10,1,23) see output looks like.Associations among simplest forms structure data! important remember Association imply correlation Correlation imply causation. Take look page view common logical fallacies. see ","code":"\nggplot(dat,\n       aes(x = stab.glu, y = glyhb)) +\n  geom_point() +\n  labs(x='Stabilized glucose', y='Glycosylated hemoglobin')\n## compute the Pearson correlation\ncor(dat$stab.glu, dat$glyhb, method='pearson')## [1] 0.7411355\n## compute the Spearman correlation\ncor(dat$stab.glu, dat$glyhb, method='spearman')## [1] 0.5214866\nggplot(dat,\n       aes(x = rank(stab.glu), y = rank(glyhb))) +\n  geom_point() +\n  labs(x='Rank - Stabilized glucose', y='Rank - Glycosylated hemoglobin')"},{"path":"week-1-data-analysis.html","id":"exercises","chapter":"Week 1 : data analysis","heading":"1.0.5 EXERCISES","text":"","code":""},{"path":"week-1-data-analysis.html","id":"exercise-1-data-features","chapter":"Week 1 : data analysis","heading":"1.0.5.1 Exercise 1: Data features","text":"Try obtain result head() function using slicing dataset “dat” using row indexes.Try obtain result head() function using slicing dataset “dat” using row indexes.Print last element last column “dat” using dim() function instead using numerals.Print last element last column “dat” using dim() function instead using numerals.","code":""},{"path":"week-1-data-analysis.html","id":"exercise-2-visualization-and-correlation","chapter":"Week 1 : data analysis","heading":"1.0.5.2 Exercise 2: Visualization and correlation","text":"Visualize cholesterol levels patients histogram using geom_histogram() function.Visualize cholesterol levels patients histogram using geom_histogram() function.Visualize cholesterol levels male patients histogram using geom_histogram().\n(expert): Mark median, first third quartile vertical lines using geom_vline(). values defined using xintercept (value x vertical line intercept).\nmark median, first third quantile female patients graph different color.\ncan tell differences values?Visualize cholesterol levels male patients histogram using geom_histogram().\n(expert): Mark median, first third quartile vertical lines using geom_vline(). values defined using xintercept (value x vertical line intercept).\nmark median, first third quantile female patients graph different color.\ncan tell differences values?association “hip” “waist” data frame “dat”? Use geom_point() function scatter plot values ranks (determined rank() function). Compute Pearson Spearman correlation values.association “hip” “waist” data frame “dat”? Use geom_point() function scatter plot values ranks (determined rank() function). Compute Pearson Spearman correlation values.","code":""},{"path":"week-1-data-analysis.html","id":"going-further-expert","chapter":"Week 1 : data analysis","heading":"1.0.5.3 Going further (expert)","text":"Select numerical columns (using select((.numeric))), apply function cor() directly: %>% cor() . happens? output? Store result command new variable named “.cor”. Plot heatmap results using pheatmap() function “pheatmap” package. Remember first install activate package using library(\"pheatmap\").Select numerical columns (using select((.numeric))), apply function cor() directly: %>% cor() . happens? output? Store result command new variable named “.cor”. Plot heatmap results using pheatmap() function “pheatmap” package. Remember first install activate package using library(\"pheatmap\").Find highest lowest Pearson correlation value result exercise 2.2. pair variables correspond? Plot corresponding scatter plots using geom_point()! Hint: finding highest Pearson correlation value, use diag() function set diagonal values (=1) “.cor” NA.Find highest lowest Pearson correlation value result exercise 2.2. pair variables correspond? Plot corresponding scatter plots using geom_point()! Hint: finding highest Pearson correlation value, use diag() function set diagonal values (=1) “.cor” NA.","code":""},{"path":"week-1-data-analysis.html","id":"day-2-dimensionality-reduction-and-unsupervised-learning","chapter":"Week 1 : data analysis","heading":"1.1 Day 2: Dimensionality reduction and unsupervised learning","text":"","code":""},{"path":"week-1-data-analysis.html","id":"preparing-the-data","chapter":"Week 1 : data analysis","heading":"1.1.1 Preparing the data","text":"Unsupervised clustering one basic data analysis techniques. allows identify groups (clusters) observations (: patients) variables. Unsupervised means using prior knowledge groups variables associations. K-means clustering good example unsupervised learning method categorizes sample based uniquely data.part, use dataset gene expression data TCGA (Cancer Genome Atlas) project. project sequenced several thousand samples cancer patients 30 cancer types. use subset data, containing 200 samples (=patients, columns) , expression 300 genes (= rows) measured.","code":""},{"path":"week-1-data-analysis.html","id":"load-data","chapter":"Week 1 : data analysis","heading":"1.1.1.1 Load data","text":"start reading gene expression data. columns samples rows genes. matrix, allows numerical operations conducted directly.WARNING: problem loading data, please download file, store disk, open following command:Next load clinical annotation file gene expression data explore itSame : issues running previous readRDS command, download file, save disk load withYou can check number samples tumor type using table() function, applied specific column (, one column…)","code":"\nbrca.exp = readRDS(url('https://www.dropbox.com/s/qububmfvtv443mq/brca.exp.rds?dl=1'))\ndim(brca.exp)## [1] 100 200\n#brca.exp = readRDS(\"xxxx\") # xxxx should be replaced with the path to the downloaded file in your device\nbrca.anno = readRDS(url('https://www.dropbox.com/s/9xlivejqkj77llc/brca.anno.rds?dl=1'))\nhead(brca.anno)##                 Age ER_status HER2_status Classification\n## TCGA-BH-A1EO-01  68  Positive    Negative      Luminal A\n## TCGA-E2-A14N-01  37  Negative    Negative     Basal-like\n## TCGA-AN-A0FF-01  32  Positive    Negative      Luminal B\n## TCGA-A2-A04V-01  39  Positive    Negative      Luminal A\n## TCGA-AN-A0XP-01  69  Positive    Negative      Luminal A\n## TCGA-C8-A12U-01  46  Positive    Negative      Luminal B\n### brca.anno = readRDS(xxx)\ntable(brca.anno$HER2_status)## \n## Equivocal  Negative  Positive \n##         4       174        22"},{"path":"week-1-data-analysis.html","id":"data-transformation","chapter":"Week 1 : data analysis","heading":"1.1.1.2 Data transformation","text":"see distribution data extremely squeezed due outliers high low values. need make data homogeneous, downstream analysis affected large magnitude numbers.carry following data processing steps. steps use rather arbitrary values, come visually inspecting data!Thresholding: cap values 95th percentileThresholding: cap values 95th percentileHomogenization: base-2 logarithmic transformation entire datasetHomogenization: base-2 logarithmic transformation entire datasetScaling: standardize data across genes mean = 0 variance = 1.Scaling: standardize data across genes mean = 0 variance = 1.start modifying data, store original data frame variable, case problems can revert back initial data!!ThresholdingHomogenization ScalingWe perform step log-transforming data. able use operation data still matrix.add +1 ?Next, scale data plot distribution. efficient, need convert data tibble first, make tidy, plot.Conversion tibble can done using as_tibble(brca.exp, rownames = NA), rownames = NA meant keep original rownames (case, gene names) new tibble, although invisible.Check :addition, gather(key = \"sample\", value = \"expression\") converts tibble long format, “sample” represents original column names, “expression” represents values present initial matrix.Compare density plot pre-processing steps using strategy.","code":"\nbrca.exp.original = brca.exp # keeps the original as matrix\n## what is the value of the 95th percent percentile?\nq95 = quantile(brca.exp,probs=0.95)\n\n## set all values above to this value\nbrca.exp[brca.exp>q95] = q95\nbrca.exp = log2(brca.exp+1)\nrownames(as_tibble(brca.exp, rownames = NA))[1:5]## [1] \"TFF1\"    \"C4orf7\"  \"AGR3\"    \"GABRP\"   \"C1orf64\"\n## scaling\nbrca.exp = scale(brca.exp)\n\n## plotting the density\nas_tibble(brca.exp, rownames = NA) %>%\n  gather(key = \"sample\", \n         value = \"expression\") %>%\n  ggplot(aes(x = expression)) +\n  geom_histogram() +\n  labs(title = \"Transformed data\")\nas_tibble(brca.exp.original, rownames = NA) %>%\n  gather(key = \"sample\", value = \"expression\") %>%\n  ggplot(aes(x = expression)) +\n  geom_histogram() +\n  labs(title = \"Untransformed data\")"},{"path":"week-1-data-analysis.html","id":"k-means-clustering","chapter":"Week 1 : data analysis","heading":"1.1.2 k-means clustering","text":"Another widely used method grouping observations k-means clustering. Now cluster dataset using k-means explore underlying structure data. dataset, different clusters represent different batches, different tumour subtypes, among features.","code":""},{"path":"week-1-data-analysis.html","id":"performing-k-means","chapter":"Week 1 : data analysis","heading":"1.1.2.1 Performing k-means","text":"use function kmeans() R matrix. can check options usage help panel right. parameter centers indicates many clusters requested.t mean command?Just type km console check results generated. Play around centers parameter. See cluster assignments typing table(km$cluster)","code":"\nkm = kmeans(x=t(brca.exp), \n            centers = 2, \n            nstart = 10)"},{"path":"week-1-data-analysis.html","id":"quality-of-the-clustering","chapter":"Week 1 : data analysis","heading":"1.1.2.2 Quality of the clustering","text":"can judge quality clustering computing intra-cluster distances, .e. sum (squared) distances pairs objects belonging cluster. called within sum squares (WSS). better clustering, smaller WSS . However, also automatically decreases increasing k.WSS request number clusters equal number data points? can check WSS particular clustering typingrun k-means k=2 k=7 clusters, k check WSS value. WSS evolve increasing k?can also run little loop test different k. Loops important structures programming language.\ncan test simple scenario . Check output simple loop looks like.understand difference 2 previous loops? Try make loop.Now can make one test k 2 7:see obvious “elbow” “kink” curve?? Another criteria quality clustering silhouette method.run silhouette method, need compute pairwise distances objects (.e. patients) data matrix. done dist function, can take different metrics (euclidean, …)now compute silhouette specific k-means clustering:Run various values k compare silhouette scores\nCompare Elbow method!","code":"\nkm$tot.withinss## [1] 4123.459\nk_to_test = c(2:7)\n\nfor (i in 1:length(k_to_test)) {\n  print(i) # We can print the indexes\n}## [1] 1\n## [1] 2\n## [1] 3\n## [1] 4\n## [1] 5\n## [1] 6\nfor (i in 1:length(k_to_test)) {\n  print(k_to_test[i]) # or the actual elements\n}## [1] 2\n## [1] 3\n## [1] 4\n## [1] 5\n## [1] 6\n## [1] 7\nkm_wws = numeric() # we start by creating an empty vector\n\n# To write in the position 1, we use i\n# to find the 1st element, we use k_to_test[i]\nfor (i in 1:length(k_to_test)) {\n  km_wws[i] = kmeans(x=t(brca.exp), \n                     centers = k_to_test[i])$tot.withinss\n}\n\n# We can plot the k against WSS using geom_line\nggplot() +\n  geom_line(aes(x = k_to_test, y = km_wws)) +\n  labs(x=\"Number of clusters K\",\n       y=\"Total within-clusters sum of squares\")\n## compute the patient-patient distance matrix (this is why we transpose using the `t()` function)\nD = dist(t(brca.exp))\nlibrary(cluster)## \n## Attaching package: 'cluster'## The following object is masked _by_ '.GlobalEnv':\n## \n##     animals\nkm = kmeans(x=t(brca.exp), centers = 3, nstart = 10)\ns = silhouette(km$cluster,D)\n\n# Let us use the basic R function plot() to see the results\nplot(s)"},{"path":"week-1-data-analysis.html","id":"hierarchical-clustering","chapter":"Week 1 : data analysis","heading":"1.1.3 Hierarchical clustering","text":"Clustering method group together similar observations separating dissimilar ones. cluster samples cancer dataset see samples cluster together separately. Hierarchical clustering generate discrete clusters datapoints, rather creates dendrogram indicates magnitude similitude samples. Data Scientist decide right amount clusters.","code":""},{"path":"week-1-data-analysis.html","id":"determine-the-most-variable-genes","chapter":"Week 1 : data analysis","heading":"1.1.3.1 Determine the most variable genes","text":"performing clustering, usually reduce number genes used, informative. example, genes show mostly constant expression across samples useful distinguish samples, right? One simple method select genes show high variance across samples.now want find top 25% highest varianceSo let us select rows (genes) variance higher equal q75:","code":"\nbrca.exp.tibble = as_tibble(brca.exp, rownames=NA) %>%\n  rownames_to_column(\"gene\")\n\n## create a new column with the variance for all genes across all samples\nbrca.exp.var = brca.exp.tibble %>%\n  rowwise() %>%\n  mutate(variance = var(c_across(starts_with(\"TCGA\")))) \n# only includes the columns starting with TCGA\n## what is the 75% quantile of the variance?\nq75 = quantile(brca.exp.var$variance, probs = 0.75)\nq75##       75% \n## 0.4934196\n## only select the genes with a variance in the top 25%\ntopVariantGenes <- brca.exp.var %>%\n  filter(variance >= q75)\n\nprint(topVariantGenes$gene)##  [1] \"TFF1\"      \"C4orf7\"    \"AGR3\"      \"GABRP\"     \"C1orf64\"   \"TFF3\"     \n##  [7] \"ABCC11\"    \"PGR\"       \"FABP7\"     \"SERPINA11\" \"VGLL1\"     \"A2ML1\"    \n## [13] \"ELF5\"      \"PROM1\"     \"CYP4Z2P\"   \"SLC6A14\"   \"CAPN8\"     \"ABCC8\"    \n## [19] \"SYTL5\"     \"SFRP1\"     \"ART3\"      \"GABBR2\"    \"PPP1R14C\"  \"HORMAD1\"  \n## [25] \"LOC84740\""},{"path":"week-1-data-analysis.html","id":"computing-the-correlation-between-all-patients","chapter":"Week 1 : data analysis","heading":"1.1.3.2 Computing the correlation between all patients","text":"Let us start filtering highly variable genes. can directly calculate Spearman correlation.want display correlation matrix heatmap, can use pheatmap function :cell heatmap represents correlation value sample row sample column. correlation sample always 1 (red diagonal).function automatically determines clustering trees rows columns (identical, since correlation matrix symmetrical!)","code":"\nbrca.exp.highvar.cor = brca.exp.tibble %>%\n  filter(gene %in% topVariantGenes$gene) %>%        # from the whole list, select only high variable\n  select(where(is.numeric)) %>%                     # get only numerical columns\n  cor(method=\"spearman\")                            # create correlation-based distance matrix\nlibrary(pheatmap)\npheatmap(brca.exp.highvar.cor, \n         show_rownames = FALSE, \n         show_colnames = FALSE)"},{"path":"week-1-data-analysis.html","id":"including-clinical-annotations-in-the-heatmap","chapter":"Week 1 : data analysis","heading":"1.1.3.3 Including clinical annotations in the heatmap","text":"nice representation, order interpret clustering, need add additional (clinical) information interpret clustering structure. , use annotation data frame containing columns number clinical features.clinical annotation stored brca.anno data frame.\ncan now plot heatmap, using annotation dataframe add additional informationHow interpret dendrogram? clusters observe make sense? parameters samples cluster together? many meaningful clusters can observe? see relation distribution data clusters ?function pheatmap accepts parameter clustering_method indicate alternative linkage methods; try linkage methods (check available pheatmap help page, can accessed typing ?pheatmap console!)","code":"\npheatmap(brca.exp.highvar.cor,\n         annotation_row = brca.anno,\n         show_rownames = FALSE, \n         show_colnames = FALSE)"},{"path":"week-1-data-analysis.html","id":"principal-component-analysis","chapter":"Week 1 : data analysis","heading":"1.1.4 Principal component analysis","text":"now use principal component analysis explore dataset, identify directions (.e. principal components) maximal variance. Principal components analysis finds n-dimensional vectors (Principal Components) direction largest variance, thereby allowing describe n-dimensional dataset just dimensions.many principal components obtain? Compare dimension matrix using dim() function!happen transpose matrix t(…) prcomp function?Principal components ranked amount variance explain. can visualized using scree plot, indicating much variance PC explains: standard deviation explained principal component contained pca$sdev vector:see standard deviation indeed going ! Let us now plot proportion total variance explained PCPrincipal component analysis represents data point (: patient) new space coordinates principal components. Check following output:can now display data points (.e. patients) first two principal components. addition, can color dots according certain clinical parameters:Choose different PCs plot. Can still observe two clusters corresponding ER_status patients?","code":"\npca = topVariantGenes %>%\n  select(where(is.numeric)) %>%\n  t() %>% # do not forget to transpose the data!\n  prcomp(center = FALSE, scale = FALSE) # We set these as false as we have already scaled our data\nsummary(pca)## Importance of components:\n##                          PC1    PC2     PC3     PC4     PC5     PC6     PC7\n## Standard deviation     3.719 2.1886 0.89796 0.86195 0.69326 0.62793 0.61645\n## Proportion of Variance 0.559 0.1936 0.03259 0.03003 0.01942 0.01593 0.01536\n## Cumulative Proportion  0.559 0.7526 0.78514 0.81516 0.83458 0.85052 0.86588\n##                            PC8     PC9    PC10    PC11    PC12   PC13    PC14\n## Standard deviation     0.59717 0.55556 0.53665 0.52007 0.50060 0.4900 0.45974\n## Proportion of Variance 0.01441 0.01247 0.01164 0.01093 0.01013 0.0097 0.00854\n## Cumulative Proportion  0.88029 0.89276 0.90440 0.91533 0.92546 0.9352 0.94370\n##                           PC15    PC16   PC17    PC18    PC19    PC20    PC21\n## Standard deviation     0.44696 0.43284 0.4009 0.39542 0.38101 0.35781 0.34271\n## Proportion of Variance 0.00807 0.00757 0.0065 0.00632 0.00587 0.00517 0.00475\n## Cumulative Proportion  0.95178 0.95935 0.9658 0.97216 0.97803 0.98320 0.98795\n##                           PC22    PC23    PC24    PC25\n## Standard deviation     0.30347 0.28452 0.26505 0.23424\n## Proportion of Variance 0.00372 0.00327 0.00284 0.00222\n## Cumulative Proportion  0.99167 0.99494 0.99778 1.00000\npca$sdev##  [1] 3.7190077 2.1886234 0.8979609 0.8619466 0.6932587 0.6279258 0.6164472\n##  [8] 0.5971745 0.5555603 0.5366514 0.5200662 0.5006037 0.4899632 0.4597383\n## [15] 0.4469622 0.4328397 0.4008966 0.3954205 0.3810141 0.3578084 0.3427089\n## [22] 0.3034732 0.2845180 0.2650462 0.2342352\nvariance = (pca$sdev)^2\nprop.variance = variance/sum(variance)\nnames(prop.variance) = 1:length(prop.variance)\n\n# We make a data.frame from the prop.variance and the PC it corresponds to \n# we can obtain the PCs using names()\ndata.frame(proportion = prop.variance, \n           PCs = as.numeric(names(prop.variance))) %>%\n  ggplot(aes(x = PCs, y = proportion)) +\n  geom_col() +                               # to make the barplot\n  labs(y='Proportion of variance')           # we only plot the first 20 PCs\nhead(pca$x)\n# We start by creating a dataframe and combining it with the annotation\npca_with_annot = as.data.frame(pca$x) %>%\n  merge(brca.anno, by = 0) # by = 0 makes use of the rownames as common information\n\n## Now the object is in a ggplot2 friendly format\nggplot(pca_with_annot,\n       aes(x = PC1, y = PC2, colour = ER_status)) +\n  geom_point() +\n  scale_colour_manual(values = c(\"grey\", \"red\", \"navy\")) # scale_colour_manual can be used to change colours"},{"path":"week-1-data-analysis.html","id":"exercises-1","chapter":"Week 1 : data analysis","heading":"1.1.5 EXERCISES","text":"","code":""},{"path":"week-1-data-analysis.html","id":"exercise-1-variance","chapter":"Week 1 : data analysis","heading":"1.1.5.1 Exercise 1: Variance","text":"exercise set given data cleanup steps taken, don’t need put results.Make heatmap reduced matrix “topVariantGenes” using pheatmap() function pheatmap library (forget select numerical columns). Check parameters might change style heatmap (column names, row names, etc..). heatmap different heatmap section 2?Make heatmap reduced matrix “topVariantGenes” using pheatmap() function pheatmap library (forget select numerical columns). Check parameters might change style heatmap (column names, row names, etc..). heatmap different heatmap section 2?Repeat selection top variable genes (apply quantile used generate “topVariantGenes”), using median absolute deviation (MAD) using mad() function instead sd() function, store brca.topMADRepeat selection top variable genes (apply quantile used generate “topVariantGenes”), using median absolute deviation (MAD) using mad() function instead sd() function, store brca.topMADExtract gene names topVariantGenes brca.topMAD check many overlap using intersect() function.Extract gene names topVariantGenes brca.topMAD check many overlap using intersect() function.","code":""},{"path":"week-1-data-analysis.html","id":"exercise-2-hierarchical-clustering","chapter":"Week 1 : data analysis","heading":"1.1.5.2 Exercise 2: Hierarchical clustering","text":"section 2, computed correlation matrix, used matrix build clustering tree.Try different linkage methods using clustering_method parameter see topology dendrogram changes!Try building distance matrix lead different topologies dendrogram, depending linkage method used! Show dendrograms built different linkage methods!","code":""},{"path":"week-1-data-analysis.html","id":"exercise-3-pca","chapter":"Week 1 : data analysis","heading":"1.1.5.3 Exercise 3: PCA","text":"Display patients first two principal components (available pca_with_annot) using geom_point(). Color patients PCA plot according HER2_status.Display patients first two principal components (available pca_with_annot) using geom_point(). Color patients PCA plot according HER2_status.(optional) Color patients PCA plot according Classification; probably need define colors… can check available colors (optional) Color patients PCA plot according Classification; probably need define colors… can check available colors ","code":""},{"path":"week-1-data-analysis.html","id":"going-further-expert-1","chapter":"Week 1 : data analysis","heading":"1.1.5.4 Going further (expert)","text":"Instead performing k-means whole gene expression matrix, can run k-means space patient represented first k principal components.Run k-means different numbers clusters (1-10) patients using first 2, 4, 6,… principal components (.e. first columns pca_with_annot). Use elbow method evaluate within sum squares (WSS) evolves. optimal number clusters?Run k-means different numbers clusters (1-10) patients using first 2, 4, 6,… principal components (.e. first columns pca_with_annot). Use elbow method evaluate within sum squares (WSS) evolves. optimal number clusters?Represent patients PCA plot previously, color according cluster belong ! Run kmeans two clusters merge k-means results.Represent patients PCA plot previously, color according cluster belong ! Run kmeans two clusters merge k-means results.","code":""},{"path":"week-1-data-analysis.html","id":"day-3-probability-distributions","chapter":"Week 1 : data analysis","heading":"1.2 Day 3: Probability distributions","text":"","code":""},{"path":"week-1-data-analysis.html","id":"probability-distributions","chapter":"Week 1 : data analysis","heading":"1.2.1 Probability distributions","text":"previous Exercise Sheet learnt unsupervised learning, like hierarchical clustering especially PCA. among fundamental methods Data Analysis. Today learn another milestone statistics: probability distributions.Figure sourceDistributions represented sparse lines represent outcomes discrete (example roll dice always discrete integer values 1 6). Distributions represented dense lines represent outcomes can continuous .e real numbers (example height people living Heidelberg).R -built functions almost probability distributionsAll functions probability distributions follow common scheme, root name function prefixed either p, d, q r. example Normal distribution four variants function available - pnorm, dnorm, qnorm rnorm., can find specific help functions:respective help documentation, find details functions.\nProbably, difficult distinguish remember pnorm() qnorm() (respectively ppois() qpois(), etc …).\ngoing look deeply following.","code":"\n# Get the list of all probability distribution related functions\nhelp(distributions)\n?rnorm\n# or\n?rpois\n# etc ..."},{"path":"week-1-data-analysis.html","id":"getting-to-know-the-various-functions","chapter":"Week 1 : data analysis","heading":"1.2.1.1 Getting to know the various functions","text":"Let us get grasp functions actually . familiar cumulative distribution function, let’s take look inverse first. work Normal distribution. calculate cumulative probability values 1,2,3,4 three different distributions, using one functions described previously.\nShort hint: p like cumulative P-robability. function going use?understand cumulative distribution functions change way ?Now, distributions, calculate inverse cdf (inverse cumulative distribution function) cumulative probabilities 25%, 50% 75%. use qnorm() function (q- like quantile):Try 100% distributions. Can explain result? expect result different ones?Now know output p- q- functions, let’s look d- like density probability functions. continuous distribution, value function specific value 0. probability function used discrete distribution binomial distribution (function dbinom()). first calculate probability getting 5 events 5 binomial distribution probability 0.5. , calculate odds getting 5 5.probability getting 5 10? getting 5 10?Suppose distribution body height described normal distribution mean = 1.75 m standard deviation sd = 0.15. probability someone taller 1.9 m? can use parameter lower.tail function pnorm() .\nprobability someone smaller 1.60m?Let’s look distribution using dnorm() function:Let’s finally look r- like random functions. , unlike others, don’t return single probabilities values rather generate random distribution values. can use generate normal distribution mean = 10, sd = 5.Can generate Poisson distribution binomial distribution?","code":"\n# Distribution 1: Mean = 2, Standard Deviation = 1\npnorm(1:4,mean=2,sd=1)## [1] 0.1586553 0.5000000 0.8413447 0.9772499\n# Distribution 2: Mean = 2, Standard Deviation = 2\npnorm(1:4,mean=2,sd=2)## [1] 0.3085375 0.5000000 0.6914625 0.8413447\n# Distribution 2: Mean = 4, Standard Deviation = 1\npnorm(1:4,mean=4,sd=1)## [1] 0.001349898 0.022750132 0.158655254 0.500000000\n# Distribution 1: Mean = 2, Standard Deviation = 1\nqnorm(c(0.25,0.5,0.75),mean=2,sd=1)## [1] 1.32551 2.00000 2.67449\n# Distribution 2: Mean = 2, Standard Deviation = 2\nqnorm(c(0.25,0.5,0.75),mean=2,sd=2)## [1] 0.6510205 2.0000000 3.3489795\n# Distribution 3: Mean = 4, Standard Deviation = 1\nqnorm(c(0.25,0.5,0.75),mean=4,sd=1)## [1] 3.32551 4.00000 4.67449\n# probability of 5 out of 5\ndbinom(5, size=5, prob=0.5) # size = number of trials; prob = probability of success on each trial## [1] 0.03125\n# probability of NOT getting 5 out of 5\n1-dbinom(5, size=5, prob=0.5) ## [1] 0.96875\n# or\npbinom(4, size=5, prob=0.5) # Remember, pbinom() returns the \"cumulative probability\" of ...## [1] 0.96875\n## taller than 1.9\npnorm(1.9, mean=1.75, sd=0.15, lower.tail=FALSE)## [1] 0.1586553\n# or\n1-pnorm(1.9, mean=1.75, sd=0.15)## [1] 0.1586553\n## smaller than 1.6\npnorm(1.6, mean=1.75, sd=0.15, lower.tail=TRUE)## [1] 0.1586553\n# equivalent to \npnorm(1.6, mean=1.75, sd=0.15) # lower.tail is set to TRUE by default (check the help page using `?pnorm`)## [1] 0.1586553\nx = seq(1, 2.5, by=0.01)\ny = dnorm(x, mean=1.75, sd=0.15)\n\nggplot() +\n  geom_line(aes(x = x, y = y),\n            colour = \"red\", linewidth = 0.8) +\n  geom_vline(xintercept = c(1.6, 1.9),\n             linetype = 'dashed')\n## normal distribution\nx = rnorm(n=1000,mean=10,sd=5)\n\nggplot() +\n  geom_histogram(aes(x=x), bins = 20,\n                 fill = \"orange\") +\n  labs(y = \"Frequency\") "},{"path":"week-1-data-analysis.html","id":"normalgaussian-distribution","chapter":"Week 1 : data analysis","heading":"1.2.1.2 Normal/Gaussian distribution","text":"normal Gaussian distribution given :\\[P(x) = \\frac{1}{{\\sigma \\sqrt {2\\pi } }} \\cdot e ^ \\frac{-(x- \\mu)^2}{{2\\sigma ^2 }} \\] \\(\\mu\\) mean distribution \\(\\sigma\\) standard deviation.standard normal distribution special case normal distribution values \\(\\mu = 0\\) \\(\\sigma = 1\\). Thus, equation Normal distribution simplifies :\\[P(x) = \\frac{1}{{\\sqrt {2\\pi } }} \\cdot e ^ \\frac{-x^2}{2} \\] Now \\(x\\) can easily solve equation since \\(\\pi\\) \\(e\\) known constants.","code":""},{"path":"week-1-data-analysis.html","id":"visualization","chapter":"Week 1 : data analysis","heading":"1.2.1.3 Visualization","text":"Let’s generate three random normal distributions different means standard deviations visualize togetherPlay mean sd parameters visualize distributions (plain lines) well corresponding histograms.","code":"\n## Use the function `dnorm()` to plot the density distribution\nx = seq(-10, 30, by=.1)\nd1 = dnorm(x, mean=0, sd=1)\nd2 = dnorm(x, mean=10, sd=1)\nd3 = dnorm(x, mean=20, sd=1)\n\n# Compare with the histogram build from 1000 random number drawn from the standard normal distribution\nr1 = rnorm(n=1000, mean=0, sd =1)  # random distributions of values\nr2 = rnorm(n=1000, mean=10, sd=1)\nr3 = rnorm(n=1000, mean=20, sd=1)\n\n# Histogram visualization\n# We will start by converting the rnorm outputs into one single df\ndata.frame(vals = c(r1, r2, r3), # column vals is actual numbers\n           Specs = c(rep(\"mean=0, sd=1\", 1000), # column Specs is mean/sd \n                     rep(\"mean=10, sd=1\", 1000),\n                     rep(\"mean=20, sd=1\", 1000))\n             ) %>%\n  ggplot(aes(x = vals, \n             colour = Specs)) +\n  geom_histogram(bins = 50) + \n  geom_freqpoly(bins = 50) # geom_freqpoly adds in the line on top of the histogram"},{"path":"week-1-data-analysis.html","id":"application-on-a-real-dataset","chapter":"Week 1 : data analysis","heading":"1.2.1.4 Application on a real dataset","text":"Now use Normal distribution make predictions gene expression TP53 lung cancer. TP53 commonly mutated gene across multiple cancer types especially lung cancers. read table (import) containing measurements TP53 expression levels 586 patients.","code":"\ntp53.exp = read.table(\"https://www.dropbox.com/s/rwopdr8ycmdg8bd/TP53_expression_LungAdeno.txt?dl=1\", \n                      header=T, sep=\"\\t\")[,1:2]\nsummary(tp53.exp)##    Samples          TP53_expression \n##  Length:586         Min.   :  92.6  \n##  Class :character   1st Qu.: 911.8  \n##  Mode  :character   Median :1313.3  \n##                     Mean   :1380.8  \n##                     3rd Qu.:1778.1  \n##                     Max.   :4703.9  \n##                     NA's   :69"},{"path":"week-1-data-analysis.html","id":"data-cleaning-and-central-values","chapter":"Week 1 : data analysis","heading":"1.2.1.4.1 Data cleaning and central values","text":"remove missing values calculate mean standard deviation TP53 gene expression.","code":"\ntp53.exp = tp53.exp %>%\n  na.omit() \n\nm.tp53 = mean(tp53.exp$TP53_expression) # mean\nm.tp53## [1] 1380.822\ns.tp53 = sd(tp53.exp$TP53_expression) # standard deviation\ns.tp53## [1] 719.5934"},{"path":"week-1-data-analysis.html","id":"modeling-using-a-normal-distribution","chapter":"Week 1 : data analysis","heading":"1.2.1.4.2 Modeling using a normal distribution","text":"Let’s see well normal distribution \\(\\mu = 1380.822\\) (m.tp53) \\(\\sigma = 719.5934\\) (s.tp53) can approximate real distribution TP53 expression.\nassume population mean standard deviation similar calculated since measure expression TP53 every lung cancer patient world.Make normal distribution parameters","code":"\n# distribution of the measured data\nggplot(tp53.exp,\n       aes(x = TP53_expression)) +\n  geom_density() + \n  xlim(-1500, 6000)\nx = seq(0,5000,by=5)\nd.pred = dnorm(x, mean = m.tp53, sd = s.tp53)\n\n# Now plot both, predicted and measured data\nggplot() +\n  geom_density(aes(x = tp53.exp$TP53_expression)) +\n  geom_line(aes(x = x, \n                y = d.pred),\n                colour = \"red\") +\n  geom_vline(xintercept = c(quantile(tp53.exp$TP53_expression, probs = c(0.1, 0.9)))) +\n  geom_vline(xintercept = c(qnorm(p = 0.1, mean = m.tp53, sd = s.tp53), \n                            qnorm(p = 0.9, mean = m.tp53, sd = s.tp53)),\n             colour = 'red', linetype = 'dotted') "},{"path":"week-1-data-analysis.html","id":"data-prediction-using-the-normal-distribution-model","chapter":"Week 1 : data analysis","heading":"1.2.1.4.3 Data prediction using the normal distribution model","text":"Using normal distribution \\(\\mu = 1380.822\\) (m.tp53) \\(\\sigma = 719.5934\\) (s.tp53), ask following questions -– (Q1) probability observing expression TP53 less 1000?– (Q2) probability observing expression TP53 greater 1000?","code":"\npnorm(q=1000, mean =m.tp53, sd = s.tp53) # returns the cumulative probability## [1] 0.2983271\n1 - pnorm(q=1000, mean =m.tp53, sd = s.tp53) ## [1] 0.7016729\n# is same as\npnorm(q=1000, mean =m.tp53, sd = s.tp53, lower.tail = FALSE) # or pnorm(q=1000, mean =m.tp53, sd = s.tp53, lower.tail = F)## [1] 0.7016729"},{"path":"week-1-data-analysis.html","id":"evaluating-the-quality-of-the-predictions","chapter":"Week 1 : data analysis","heading":"1.2.1.4.4 Evaluating the quality of the predictions","text":"Let’s check good predictions compared real data.\n– (Q1) probability observing expression TP53 less 1000?– (Q2) probability observing expression TP53 greater 1000?say predictions pretty good !! Now, let’s try break model. Re-execute code different \\(q\\) values q=100, q=500, q=4000, q=4500 etc. values think model perform well. HINT: Look tails distribution!, using normal distribution \\(\\mu = 1380.822\\) \\(\\sigma = 719.5934\\), ask value TP53 expression 10% 90% quantiles:Let’s check good predictions compared real data.predictions pretty good!","code":"\nsum(tp53.exp$TP53_expression < 1000)/nrow(tp53.exp)## [1] 0.2978723\nsum(tp53.exp$TP53_expression > 1000)/nrow(tp53.exp)## [1] 0.7021277\n# What is the probability of observing the expression of TP53 to be less than q?\nq = c(100,500,1000,4000,4500)\n\n# for loop used to calculate and store the predicted (a) and real (b) values\npred = numeric()\nmeas = numeric()\nfor (i in 1:length(q)){\n  pred[i] = pnorm(q=q[i], mean = m.tp53, sd = s.tp53) \n  meas[i] = sum(tp53.exp$TP53_expression < q[i])/nrow(tp53.exp)\n}\n\n# Change into a dataframe \nmodel_mat = data.frame(pred, meas, q)\nmodel_mat##         pred        meas    q\n## 1 0.03754417 0.001934236  100\n## 2 0.11046576 0.104448743  500\n## 3 0.29832708 0.297872340 1000\n## 4 0.99986358 0.992263056 4000\n## 5 0.99999270 0.996131528 4500\nqnorm(p = 0.1, mean = m.tp53, sd = s.tp53)## [1] 458.6258\nqnorm(p = 0.9, mean = m.tp53, sd = s.tp53)## [1] 2303.018\nquantile(tp53.exp$TP53_expression, \n         probs = c(0.1, 0.9))##       10%       90% \n##  478.2844 2240.1913"},{"path":"week-1-data-analysis.html","id":"visualization-1","chapter":"Week 1 : data analysis","heading":"1.2.1.4.5 Visualization","text":"can also visualize simple graph:Compare black red vertical lines (real vs predicted).Re-execute code p=0.25, p=0.5, p=0.75 etc check good predictions .","code":"\n# Model prediction:\nx = seq(0,5000,by=5)\nd.pred = dnorm(x,mean = m.tp53, sd = s.tp53)\n\n# Model and measured data and predicted versus measured 0.1 and 0.9 quantiles:\nggplot() +\n  geom_line(aes(x = x, \n                y = d.pred),\n            colour = \"red\") +\n  geom_density(aes(x = tp53.exp$TP53_expression)) "},{"path":"week-1-data-analysis.html","id":"visualization-using-a-q-q-plot","chapter":"Week 1 : data analysis","heading":"1.2.1.4.6 Visualization using a Q-Q plot","text":"Now, let’s plot sample quantiles theoretical quantiles check similarity two. called quantile - quantile plot Q-Q plot, familiar (see Exercises Sheet 1).","code":"\nq = seq(0,1,0.01) # Creating a vector of quantiles\n\n# Find values corresponding to these quantiles in the real data\nq.observed = quantile(tp53.exp$TP53_expression, probs = q)\n\n# Find values corresponding to these quantiles in the theoretical normal distribution\nq.theoretical = qnorm(p = q, mean = m.tp53, sd = s.tp53)\n\n# # Correlate the above two values\nggplot(tp53.exp,\n       aes(sample = TP53_expression)) +\n  geom_qq() +\n  geom_qq_line(colour = 'red')\n## Would be the same as:\n# ggplot() +\n#   geom_point(aes(x = q.theoretical, \n#                  y = q.observed), size = 1) +\n#   geom_abline(intercept = 0, slope = 1, \n#               size = 1, color = \"red\") "},{"path":"week-1-data-analysis.html","id":"binomial-distribution","chapter":"Week 1 : data analysis","heading":"1.2.1.5 Binomial distribution","text":"binomial distribution can defined -\\[P(x) = \\frac{n!}{x!(n-x)!}\\cdot p^x \\cdot (1-p)^{n-x}\\]\\(x\\) number successes \\(n\\) experiments \\(p\\) probability success.\\(mean = n \\cdot p\\)\\(variance = np \\cdot (1 - p)\\)\\(sd = \\sqrt{np \\cdot (1 - p)}\\)design experiment follows -experiment repeated independent one anotherEach experiment just two outcomesThe probability success constant change experimentsWe can example compute probability 7 heads series 10 throws coin:can compute probability get 7 heads using function pbinom(). Remember parameter “lower.tail” used specify whether calculate probability observing x fewer successes (lower.tail = TRUE) probability observing x successes (lower.tail = FALSE):Beware syntax means strictly 6, .e. 7 !!compute probability get less 5? qbinom(0.3,size=10,prob=0.5,lower.tail=FALSE) represent?","code":"\n# x = number of successes; size = number of trials; prob = probability of success on each trial\ndbinom(x=7, size=10, prob = 0.5) ## [1] 0.1171875\npbinom(6, size=10,\n       prob=0.5,\n       lower.tail=FALSE) ## [1] 0.171875"},{"path":"week-1-data-analysis.html","id":"confidence-interval","chapter":"Week 1 : data analysis","heading":"1.2.2 Confidence interval","text":"confidence interval describes interval containing (unknown) expectation value distribution 95% confidence. means 100 random realizations random variable, true expectation value \\(\\mu\\) indeed interval.Let us try simulation: consider random variable distributed according Poisson distribution \\[P(x) = \\frac{{e^{ - \\lambda } \\lambda ^x }}{{x!}}\\] , know true value expectation value. want get estimate \\(\\lambda\\), check confidence interval contains true expectation value.example, farmer expects collect 75 eggs hens per hour.now collects 100 days eggs \\(N=8\\) times day (time one hour). want compute mean \\(m_N\\) \\(N=8\\) realizations determine 95% confidence interval, check, often expectation value \\(\\mu\\) inside confidence interval.Remember 95% CI given \\[[m_N-t_{95,N-1}\\frac{\\sigma}{\\sqrt{N}},m_N+t_{95,N-1}\\frac{\\sigma}{\\sqrt{N}}]\\] \\(t_{95,N-1}\\) critical value \\(t\\)-distribution \\(n-1\\) degrees freedom.Let’s start creating samples:lapply() function R stands “list apply”. used apply function element list vector produces list length output.previous example, input vector (1:100).\n, lapply(v, function()):\n+ v: list vector want apply function .\n+ function: function want apply element X. case, applying rpois 100 times.output, lapply returns list element specified function applied .Run View(X) see object looks like. Try using lapply obtain mean elements X using lapply(X, function(){mean()}) lapply(X, mean).Now, calculate mean standard deviation respective samples:Next, determine upper lower bounds 95% CI. Remember confidence interval based \\(t\\)-distribution. degrees freedom distribution sample size -1 (\\(N\\)-1=7 case)Finally, determine whether sample mean found within 95% CI :, orange/grey bars represent confidence interval, dot mean sample values, dotted line \\lambda represents true expectation value. Whenever true expectation value within CI, bar orange, , bar grey. often true expectation value outside CI? Count grey bars!happens 5 times, fits pretty well expected 5%.Repeat simulation, now samples \\(N=24\\) (100 times)\nobserve?\noften true expectation value outside CI? Change 90% CI check works!","code":"\nlambda = 75\n# size of the sample\nN = 8\n\n# we now draw 100 times samples of size N=8\n## rpois is the function used to generate the Poisson distribution\nX = lapply(1:100, function(i) {\n  rpois(N, lambda = lambda)\n  })\n# we compute the sample means\nXm = sapply(X,mean)\n# and the sample standard deviations\nXsd = sapply(X,sd) \ndf = N-1\ntc = qt(c(0.975),df) # this is the critical value for the t-distribution for df = N-1 degrees of freedom and 95% CI\n\nXl = Xm-tc*Xsd/sqrt(N) # upper bound of the 95% CI\nXh = Xm+tc*Xsd/sqrt(N) # lower bound of the 95% CI\n## vector of TRUE/FALSE if the real expectation value lambda is inside the interval\ni.ok =  as.factor(Xl < lambda & Xh > lambda)\n\nplot_data <- data.frame(\n  n = 1:100,\n  Xm = Xm,\n  Xl = Xl,\n  Xh = Xh,\n  i.ok = i.ok\n) \n\n# Plot using ggplot2\nggplot(plot_data, \n       aes(x = n, y = Xm, \n           color = `i.ok`)) +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = Xl, ymax = Xh, color = i.ok), width = 0.2) +\n  geom_hline(yintercept = lambda, linetype = \"dashed\", color = \"black\") +\n  scale_color_manual(values = c(\"grey\", \"orange\")) +\n  labs(y = \"Mean values\",\n       title = paste(\"Mean values and confidence intervals, N=\",N),\n       color = \"95% CI Status\") "},{"path":"week-1-data-analysis.html","id":"exercises-2","chapter":"Week 1 : data analysis","heading":"1.2.3 EXERCISES","text":"","code":""},{"path":"week-1-data-analysis.html","id":"exercise-1-probability-distributions","chapter":"Week 1 : data analysis","heading":"1.2.3.1 Exercise 1: Probability distributions","text":"expression TP53 observed 10th percentile? expression TP53 observed 90th percentile?expression TP53 observed 10th percentile? expression TP53 observed 90th percentile?(optional) distribution types can become similar normal distribution certain conditions. Plot histogram 1000 random numbers drawn Poisson distribution lambda = 1, 10, 100, 1000. observe?(optional) distribution types can become similar normal distribution certain conditions. Plot histogram 1000 random numbers drawn Poisson distribution lambda = 1, 10, 100, 1000. observe?","code":""},{"path":"week-1-data-analysis.html","id":"exercise-2-confidence-intervals","chapter":"Week 1 : data analysis","heading":"1.2.3.2 Exercise 2: Confidence intervals","text":"buying 10 packs gummy bears. particularly like red ones green ones. pack contains 6 different colors expect equally distributed. 84 pieces per 200g pack.expected amount red green gummy bears?expected amount red green gummy bears?selected 10 packs according colors see pack. home, counted following bears per pack:selected 10 packs according colors see pack. home, counted following bears per pack:red ones: 12 16 17 12 16 13 11 18 13 19for green ones: 11 10 15 16 12 14 13 10 13 17\nselection procedure success? words, expected value bellow (congrats!), within (bad luck!) 95% CI?","code":""},{"path":"week-1-data-analysis.html","id":"day-4-hypothesis-testing","chapter":"Week 1 : data analysis","heading":"1.3 Day 4: Hypothesis testing","text":"section, go hypothesis testing. start see formulate hypotheses test .\naddition, want learn use interpret hypothesis tests.work diabetes dataset used previously.Load required packagesCheck content dataset using summary functionHow can inspect differences weight men women? can start ploting two histograms density plots representing weight biological sex.distributions look different shape, right? think mean located plot?can add means plot vertical lines.think mean male weight lower 180?\nthink mean female weight really different mean male weights?\nthink mean male weight higher females?","code":"\ndat = read.delim('https://tinyurl.com/y4fark9g')\n\n# set the row names using the column id\nrownames(dat) = dat$id\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tibble)\nsummary(dat)##        id             chol          stab.glu          hdl        \n##  Min.   : 1000   Min.   : 78.0   Min.   : 48.0   Min.   : 12.00  \n##  1st Qu.: 4792   1st Qu.:179.0   1st Qu.: 81.0   1st Qu.: 38.00  \n##  Median :15766   Median :204.0   Median : 89.0   Median : 46.00  \n##  Mean   :15978   Mean   :207.8   Mean   :106.7   Mean   : 50.45  \n##  3rd Qu.:20336   3rd Qu.:230.0   3rd Qu.:106.0   3rd Qu.: 59.00  \n##  Max.   :41756   Max.   :443.0   Max.   :385.0   Max.   :120.00  \n##                  NA's   :1                       NA's   :1       \n##      ratio            glyhb         location              age       \n##  Min.   : 1.500   Min.   : 2.68   Length:403         Min.   :19.00  \n##  1st Qu.: 3.200   1st Qu.: 4.38   Class :character   1st Qu.:34.00  \n##  Median : 4.200   Median : 4.84   Mode  :character   Median :45.00  \n##  Mean   : 4.522   Mean   : 5.59                      Mean   :46.85  \n##  3rd Qu.: 5.400   3rd Qu.: 5.60                      3rd Qu.:60.00  \n##  Max.   :19.300   Max.   :16.11                      Max.   :92.00  \n##  NA's   :1        NA's   :13                                        \n##     gender              height          weight         frame          \n##  Length:403         Min.   :52.00   Min.   : 99.0   Length:403        \n##  Class :character   1st Qu.:63.00   1st Qu.:151.0   Class :character  \n##  Mode  :character   Median :66.00   Median :172.5   Mode  :character  \n##                     Mean   :66.02   Mean   :177.6                     \n##                     3rd Qu.:69.00   3rd Qu.:200.0                     \n##                     Max.   :76.00   Max.   :325.0                     \n##                     NA's   :5       NA's   :1                         \n##      bp.1s           bp.1d            bp.2s           bp.2d       \n##  Min.   : 90.0   Min.   : 48.00   Min.   :110.0   Min.   : 60.00  \n##  1st Qu.:121.2   1st Qu.: 75.00   1st Qu.:138.0   1st Qu.: 84.00  \n##  Median :136.0   Median : 82.00   Median :149.0   Median : 92.00  \n##  Mean   :136.9   Mean   : 83.32   Mean   :152.4   Mean   : 92.52  \n##  3rd Qu.:146.8   3rd Qu.: 90.00   3rd Qu.:161.0   3rd Qu.:100.00  \n##  Max.   :250.0   Max.   :124.00   Max.   :238.0   Max.   :124.00  \n##  NA's   :5       NA's   :5        NA's   :262     NA's   :262     \n##      waist           hip           time.ppn     \n##  Min.   :26.0   Min.   :30.00   Min.   :   5.0  \n##  1st Qu.:33.0   1st Qu.:39.00   1st Qu.:  90.0  \n##  Median :37.0   Median :42.00   Median : 240.0  \n##  Mean   :37.9   Mean   :43.04   Mean   : 341.2  \n##  3rd Qu.:41.0   3rd Qu.:46.00   3rd Qu.: 517.5  \n##  Max.   :56.0   Max.   :64.00   Max.   :1560.0  \n##  NA's   :2      NA's   :2       NA's   :3\nggplot(dat,\n       aes(x = weight, \n           fill = gender)) +\n  geom_density(alpha = 0.5)\n# We can use \"filter()\" to filter the cholesterol values for men and women\ndat.male = dat %>%\n  filter(gender == 'male')\n\ndat.female = dat %>%\n  filter(gender == 'female')\n\n# we will calculate the mean weight by sex.\nmean.men <- mean(dat.male$weight, na.rm = TRUE)\nmean.women <- mean(dat.female$weight, na.rm = TRUE)\nggplot(dat,\n       aes(x = weight, \n           fill = gender)) +\n  geom_density(alpha = 0.5) +\n  geom_vline(xintercept = mean.men, colour = \"cyan\") +\n  geom_vline(xintercept = mean.women, colour = \"salmon\")"},{"path":"week-1-data-analysis.html","id":"tests-of-mean","chapter":"Week 1 : data analysis","heading":"1.3.1 Tests of mean","text":"Now, want use statistical test check ,males mean weight significantly lower specific value (one-sample, one-tailed test)significant difference mean weights two groups (two-sample, two-sided test)male significantly higher mean weight females (two-sample, one-sided test)Note use word significant previous statements!exactly mean tests t-test (Wilcoxon test) designed !\nperform t-test.","code":""},{"path":"week-1-data-analysis.html","id":"one-sample-t-test","chapter":"Week 1 : data analysis","heading":"1.3.1.1 One sample t-test","text":"Using one-sample t-test, can check values differ significantly target value.\nexample, sample 10 chocolate bars, test significantly differ expected weight 100 g:perform one- two-sided test?function t.test() offers three alternative options: two.sided, less greater.\n, want test whether mean weight 10 chocolate bars different expected weight 100 g, want perform two sided test use alternative two.sided.essential clearly formulate H0 H1 hypothesis.\ntwo alternative equivalent ways .\nEither:H0: expectation value random variable “Weight chocolate bar” equal 100 g.H1: expectation value random variable “Weight chocolate bar” different 100 g.orH0: mean weight chocolate bar significantly different 100 g.H1: mean weight chocolate bar significantly different 100 g.Note difference two formulations, ask help questions !interpret result?\nCan reject H0 hypothesis?Using \\(\\alpha=0.05\\), H0 hypothesis can rejected p-value 0.05244 (p-value >= 0.05).\n\\(\\alpha=0.05\\), mean weight chocolate bars significantly different 100 g.Using \\(\\alpha=0.1\\), H0 hypothesis can rejected (p-value < 0.1).\n\\(\\alpha=0.1\\), mean weight chocolate bars significantly different 100 g.… mean \\(\\alpha\\) chosen respect results t.test!!!running t.test, formulate null hypothesis H0 alternative hypothesis H1 decide alpha value.\nRemember, alpha represent false positive rate: H0 hypothesis (test two identical distributions), proportion tests detect difference two groups (p-value < \\(\\alpha\\)).Beware get confused one-/two-sample tests, one-/two-sided tests!Regarding mean weight males, like check whether males mean weight significantly lower 180.\nwell, perform one-sample test, mu = 180.\nHowever, perform one-sided test using alternative option less.hypotheses can formulated :H0: expectation value random variable “Weight male patients” equal greater 180.H0: expectation value random variable “Weight male patients” equal greater 180.H1: expectation value random variable “Weight male patients” less 180.H1: expectation value random variable “Weight male patients” less 180.interpret result \\(\\alpha=0.05\\)?\nCan reject H0 hypothesis?","code":"\nbars = c(103,103,97,102.5,100.5,103,101.3,99.5,101,104) # weights of 10 chocolate bars\nchocbar.mean = 100 # expected weight\nt.test(x = bars, mu = chocbar.mean, alternative = \"two.sided\")## \n##  One Sample t-test\n## \n## data:  bars\n## t = 2.233, df = 9, p-value = 0.05244\n## alternative hypothesis: true mean is not equal to 100\n## 95 percent confidence interval:\n##   99.98067 102.97933\n## sample estimates:\n## mean of x \n##    101.48\nt.test(dat.male$weight, mu = 180, alternative = \"less\")## \n##  One Sample t-test\n## \n## data:  dat.male$weight\n## t = 0.64094, df = 167, p-value = 0.7388\n## alternative hypothesis: true mean is less than 180\n## 95 percent confidence interval:\n##      -Inf 186.8629\n## sample estimates:\n## mean of x \n##  181.9167"},{"path":"week-1-data-analysis.html","id":"two-sample-t-test-2-sided","chapter":"Week 1 : data analysis","heading":"1.3.1.2 Two-sample t-test (2-sided)","text":"Now, compare mean weights males females.According previous histogram, females different mean weight males.\ncan tested using two-sample two.tailed t.test.hypotheses can formulated :H0: expectation value random variable “Weight male patients” equal expectation value random variable “Weight female patients”.H0: expectation value random variable “Weight male patients” equal expectation value random variable “Weight female patients”.H1: expectation value random variable “Weight male patients” different expectation value random variable “Weight female patients”.H1: expectation value random variable “Weight male patients” different expectation value random variable “Weight female patients”.interpret result \\(\\alpha=0.05\\)?\nCan reject H0 hypothesis?","code":"\nggplot(dat,\n       aes(x = weight, \n           fill = gender)) +\n  geom_density(alpha = 0.5) +\n  geom_vline(xintercept = mean.men, colour = \"cyan\") +\n  geom_vline(xintercept = mean.women, colour = \"salmon\")\nt.test(dat.male$weight, \n       dat.female$weight) # remember, alternative = \"two.sided\" per default.## \n##  Welch Two Sample t-test\n## \n## data:  dat.male$weight and dat.female$weight\n## t = 1.8453, df = 372.45, p-value = 0.06579\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  -0.4875828 15.3465572\n## sample estimates:\n## mean of x mean of y \n##  181.9167  174.4872"},{"path":"week-1-data-analysis.html","id":"two-sample-t-test-1-sided","chapter":"Week 1 : data analysis","heading":"1.3.1.3 Two-sample t-test (1-sided)","text":"Looking histogram, observers principle see difference mean values weights formulate following hypotheses:H0: expectation value random variable “Weight male patients” equal lower expectation value random variable “Weight female patients”.H0: expectation value random variable “Weight male patients” equal lower expectation value random variable “Weight female patients”.H1: expectation value random variable “Weight male patients” higher expectation value random variable “Weight female patients”.H1: expectation value random variable “Weight male patients” higher expectation value random variable “Weight female patients”.interpret result \\(\\alpha=0.05\\)?Can explain p-value one-tailed t.test lower p-value two-tailed t.test?\nrelation two values?can visualized using t-distribution.\n(see result t.test), t = 1.8453 df = 372.45.one-tailed t.test, p-value area curve t > 1.8453 (alternative greater) t < -1.8453 (alternative less).two-tailed t.test, p-value area curve t > 1.8453 (alternative greater) t < -1.8453 (alternative less).\ntwo times p-value one-sided t.test!IMPORTANTLY, t-test can performed data normally distributed!\ndata normally distributed, need use non-parametric test, like Wilcoxon test.","code":"\nt.test(dat.male$weight, \n       dat.female$weight, alternative = \"greater\")## \n##  Welch Two Sample t-test\n## \n## data:  dat.male$weight and dat.female$weight\n## t = 1.8453, df = 372.45, p-value = 0.0329\n## alternative hypothesis: true difference in means is greater than 0\n## 95 percent confidence interval:\n##  0.7903498       Inf\n## sample estimates:\n## mean of x mean of y \n##  181.9167  174.4872\n# p-value of the two-sided t-test versus p-value of the one-sided t-test: \nt.test(dat.male$weight, \n       dat.female$weight, alternative = \"two.sided\")$p.value # two-tailed## [1] 0.06579432\nt.test(dat.male$weight, \n       dat.female$weight, alternative = \"greater\")$p.value # one-tailed## [1] 0.03289716"},{"path":"week-1-data-analysis.html","id":"wilcoxon-test","chapter":"Week 1 : data analysis","heading":"1.3.1.4 Wilcoxon test","text":"data normally distributed?\ncase, supposed use t-test testing differences mean values!\nLet us us see example.Let us start loading data.check whether distribution expression values gene SOX4 corresponds normal distribution:looks everything normal!\ncase, apply t.test, need apply non-parametric test called Wilcoxon test.\ntest performed values (like t-test) ranks values (remember difference Pearson’s Spearman’s correlations!)Compare obtained p-value p-value obtained used t-test:p-values different!!\ndifference expression AML patients gene significant taking \\(\\alpha=0.05\\)?, trust t-test due non-normality data!\nHence, correct p-value one Wilcoxon test.","code":"\nall.aml = read.delim('http://bioinfo.ipmb.uni-heidelberg.de/crg/datascience3fs/practicals/data/all.aml.cleaned.csv',\n                     header=TRUE)\nall.aml.anno = read.delim(\"http://bioinfo.ipmb.uni-heidelberg.de/crg/datascience3fs/practicals/data/all.aml.anno.cleaned.csv\",\n                          header=TRUE) %>%\n  mutate(id = paste0(\"pat\", Samples))\nexpression = all.aml %>%\n  t() %>%\n  as.data.frame() %>%\n  na.omit()\n\n# Plot histogram \nggplot(expression,\n       aes(x = SOX4)) +\n  geom_histogram(bins = 20)\n# Check if it's normally distributed using a QQ-plot \nggplot(expression,\n       aes(sample = SOX4)) +\n  geom_qq() +\n  geom_qq_line(colour = 'red')\n# divide the gene expression data in two groups according to ALL or AML patients:\n# obtain the AML and rest\naml.patient.id = all.aml.anno %>%\n  filter(ALL.AML == \"AML\")\nother.id = all.aml.anno %>%\n  filter(ALL.AML != \"AML\") # filters for those which are not labeled AML\n\ngene.all = expression %>%\n  rownames_to_column(\"id\") %>%\n  filter(id %in% other.id$id)\n\ngene.aml = expression %>%\n  rownames_to_column(\"id\") %>%\n  filter(id %in% aml.patient.id$id)\n  \n# test for a difference in the mean expression values using the Wilcoxon test:\nwilcox.test(gene.aml$SOX4, gene.all$SOX4)## \n##  Wilcoxon rank sum test with continuity correction\n## \n## data:  gene.aml$SOX4 and gene.all$SOX4\n## W = 260.5, p-value = 0.0001125\n## alternative hypothesis: true location shift is not equal to 0\nt.test(gene.aml$SOX4, gene.all$SOX4)## \n##  Welch Two Sample t-test\n## \n## data:  gene.aml$SOX4 and gene.all$SOX4\n## t = -3.6216, df = 54.611, p-value = 0.000642\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  -2605.3197  -748.9356\n## sample estimates:\n## mean of x mean of y \n##  1653.000  3330.128"},{"path":"week-1-data-analysis.html","id":"proportion-tests","chapter":"Week 1 : data analysis","heading":"1.3.2 Proportion tests","text":"t-test Wilcoxon tests tests mean, meaning comparing means two samples looking significant differences.hypothesis one might want test, related relationship two categorical variables:proportion men significantly higher patients Louisa compared ones Buckingham?proportion smokers 18 Germany significantly higher European countries?proportion test (Fisher Exact Test chi-squared test) used investigate relationship 2 categorical variables, starting contingency table.\nuse dataset clinical information breast cancer patients.Check variables dataset categorical/ordinal/numerical.can now check significant relationship variables.\nexample, can verify choice treatment tamoxifen (variable hormon) related pre-/post-menopausal status (variable meno)First, can build contingency table 2 variables:can compute odds-ratio () two variables.\nmetric measures strength association two events, case treatment tamoxifen menopausal status.’s interpret odds ratio () value:= 1: events independentOR > 1: events positively correlatedOR < 1: events negatively correlatedHow odds-ratio look like transpose matrix?Now can run one-sided Fisher Exact Test (FET).\nH0/H1 hypothesis :H0: odds-ratio significantly larger oneH1: odds-ratio significantly larger oneCheck computation odds-ratio right!\nCompute also two-sided test:Formulate H0/H1 hypothesis!can also use chi-square test answer question.\nChi-square test compares observed number occurrences contingency table expected number occurrences relationship variables.H0: observed expected occurrences significantly differentH1: observed expected occurrences significantly differentNow want verify impact age grade tumor.\ncategorize patients 40 year groups, perform chi-squared test:can determine table expected counts using apply() function.\nworks similarly lapply, can used apply function rows (1), columns (2), :hand, sapply commonly used dataframe tabular formats:compute chi-square test statistic using tab tab.exp tables?","code":"\ndat.brca = read.delim('http://bioinfo.ipmb.uni-heidelberg.de/crg/datascience3fs/practicals/data/gbsg_ba_ca.dat', \n                      stringsAsFactors = FALSE)\n## build contingency table\ntable(dat.brca$meno, dat.brca$hormon)##                 \n##                  had tamoxifen no tamoxifen\n##   Postmenopausal           187          209\n##   premenopausal             59          231\nCT = table(dat.brca$meno,dat.brca$hormon)\n\nOR = (CT[1,1]/CT[1,2])/(CT[2,1]/CT[2,2])\nOR## [1] 3.503122\n## build contingency table\ntab = table(dat.brca$meno,dat.brca$hormon)\n#\n## run the FET\nfisher.test(tab, alternative = 'greater')## \n##  Fisher's Exact Test for Count Data\n## \n## data:  tab\n## p-value = 1.388e-13\n## alternative hypothesis: true odds ratio is greater than 1\n## 95 percent confidence interval:\n##  2.580638      Inf\n## sample estimates:\n## odds ratio \n##   3.496639\nfisher.test(tab)## \n##  Fisher's Exact Test for Count Data\n## \n## data:  tab\n## p-value = 2.475e-13\n## alternative hypothesis: true odds ratio is not equal to 1\n## 95 percent confidence interval:\n##  2.444410 5.049548\n## sample estimates:\n## odds ratio \n##   3.496639\nchisq.test(tab)## \n##  Pearson's Chi-squared test with Yates' continuity correction\n## \n## data:  tab\n## X-squared = 51.416, df = 1, p-value = 7.473e-13\n## contingency table\ntab = table(dat.brca$age>40,\n            dat.brca$grade)\ntab # tumor grades 1,2,3; age under 40 (FALSE) and over 40 (TRUE)##        \n##           1   2   3\n##   FALSE   6  41  26\n##   TRUE   75 403 135\n##\nchisq.test(tab)## \n##  Pearson's Chi-squared test\n## \n## data:  tab\n## X-squared = 6.9515, df = 2, p-value = 0.03094\ntot = apply(tab,2,sum) # this is the total number of occurrences in the 3 categories of \"grade\", independently of \"age\"\ntot##   1   2   3 \n##  81 444 161\nage = apply(tab,1,sum) # this is the total number of persons above/below 40, independently of \"grade\"\nage## FALSE  TRUE \n##    73   613\ntot.proportions = tot/sum(tot) # HO proportions (the proportions of occurrences in the three categories of grade, independently of age)\n\ntab.exp = sapply(tot.proportions,function(x) {x*age}) # expected counts under H0\ntab.exp##               1         2         3\n## FALSE  8.619534  47.24781  17.13265\n## TRUE  72.380466 396.75219 143.86735"},{"path":"week-1-data-analysis.html","id":"exercises-3","chapter":"Week 1 : data analysis","heading":"1.3.3 EXERCISES","text":"","code":""},{"path":"week-1-data-analysis.html","id":"exercise-1-one-sided-t-test","chapter":"Week 1 : data analysis","heading":"1.3.3.1 Exercise 1: One-sided t-test","text":"Consider following graph, formulate hypotheses H0 H1 perform (one-sided) t-test.\nInterpret result using \\(\\alpha=0.05\\).Calculate mean age men.Compare age = 50. Formulate hypotheses H0 H1 perform (one-sided) t-test. Interpret result using \\(\\alpha=0.05\\).","code":""},{"path":"week-1-data-analysis.html","id":"exercice-2-two-sided-t-test","chapter":"Week 1 : data analysis","heading":"1.3.3.2 Exercice 2: Two-sided t-test","text":"Can find interesting differences mean values parameters dataset dat two groups defined location?\nFollow steps:Select two groups according location.\n, check result distinct(location) create two tibbles corresponding possible locations.Select two groups according location.\n, check result distinct(location) create two tibbles corresponding possible locations.Calculate mean values numerical parameters group (ex: age, height, weight, waist, hip, …).\nHint: create first dataframe numeric columns (use select((.numerical))).\nSelect rows corresponding two groups use summarise() group_by loop calculate mean values (grouped location).Calculate mean values numerical parameters group (ex: age, height, weight, waist, hip, …).\nHint: create first dataframe numeric columns (use select((.numerical))).\nSelect rows corresponding two groups use summarise() group_by loop calculate mean values (grouped location).Select one , formulate H0 H1 hypotheses perform (two-sided) t-test.\nInterpret result (\\(\\alpha=0.05\\)).Select one , formulate H0 H1 hypotheses perform (two-sided) t-test.\nInterpret result (\\(\\alpha=0.05\\)).","code":""},{"path":"week-1-data-analysis.html","id":"exercise-3-mean-and-proportion-testing","chapter":"Week 1 : data analysis","heading":"1.3.3.3 Exercise 3: Mean and proportion testing","text":"test use following questions?lotion company figure whether last product likely give acne men rather women.department education wants find whether social science students higher grades science students.biologist needs find whether specific gene likely silenced lactose intolerant people.","code":""},{"path":"week-1-data-analysis.html","id":"going-further-checking-the-normality-of-the-distribution","chapter":"Week 1 : data analysis","heading":"1.3.3.4 Going further: Checking the normality of the distribution","text":"principle, t-tests require data approximately normally distributed.\n, can use non-parametric tests (see next lecture).order check whether data normally distributed , possible perform Shapiro-Wilk normality test (see lecture).\nstatistical test implemented R function shapiro.test(). Try datasets used .p-value inferior chosen \\(\\alpha\\) level (use 0.05), means null hypothesis can rejected. Therefore, data distribution tested normal need use non-parametric test.","code":""},{"path":"week-1-data-analysis.html","id":"day-5-multiple-testing-and-regression","chapter":"Week 1 : data analysis","heading":"1.4 Day 5: Multiple testing and regression","text":"","code":""},{"path":"week-1-data-analysis.html","id":"multiple-testing","chapter":"Week 1 : data analysis","heading":"1.4.1 Multiple testing","text":"previous examples testing one single hypothesis. However, common biology one hypothesis tested.\ntable reminder errors can made testing hypotheses:Decision associated \\(\\alpha\\), p-value 0.05, reject null hypothesis. However, testing multiple hypothesis, prone erroneously rejecting 1st, 2nd… Nth null hypothesis. .e., test 1000 hypotheses simultaneously, expect erroneously reject 50 (5%) just chance!family wise error rate (FWER) probability making least one Type error can control probability using p-value correction.","code":""},{"path":"week-1-data-analysis.html","id":"example","chapter":"Week 1 : data analysis","heading":"1.4.1.1 Example","text":"Imagine trying test effect treatment cell line lab. run gene expression experiment compare treatment control (untreated) panel 20 genes pre-selected.comparison treatment control panel gives set p-values 20 genesBut since ran 20 different tests generate , need correct p-values. can p-values directly using p.adjust() function. Run ?p.adjust() see methods can used.use false discovery rate (FDR) correction example.Check p-values change!many genes change significant non-significant? impact experiment?","code":"\np.vals = c(0.1, 0.0001, 0.04, 1.2e-20, 0.002, 0.01, 0.5, 1, 0.02, 1.9e-5,\n           1.1e-6, 0.03, 0.7, 0.06, 0.01, 0.3, 0.05, 1e-13, 0.032, 0.004)\np.vals_FDR = p.adjust(p.vals, method=\"fdr\")\np.vals_FDR##  [1] 1.250000e-01 4.000000e-04 6.153846e-02 2.400000e-19 6.666667e-03\n##  [6] 2.222222e-02 5.555556e-01 1.000000e+00 4.000000e-02 9.500000e-05\n## [11] 7.333333e-06 5.333333e-02 7.368421e-01 8.000000e-02 2.222222e-02\n## [16] 3.529412e-01 7.142857e-02 1.000000e-12 5.333333e-02 1.142857e-02"},{"path":"week-1-data-analysis.html","id":"regression","chapter":"Week 1 : data analysis","heading":"1.4.2 Regression","text":"last part, want learn build regression model order make predictions certain quantitative variables using quantitative variables. important steps :learning modeltesting model evaluate performances, also check assumptions linearity givenpredict values based new data pointsWe use diabetes data set, build simple linear regression models single explanatory variable.\nfocus predicting cholesterol level.Load data perform basic inspection data:limit dataset numerical variablesFirst, data cleaning remove patients least 1 “NA”. Use na.omit()., can make heatmap visualize correlation values variables dataset.strongest correlations? make sense?Apart displaying raw correlation values, can test statistically significant, .e. significantly positive (one-sided test), negative (one-sided test), non zero (two-sided test). , use cor.test function.example, seems positive correlation stabilized glucose (stab.glu) hip circumference (hip).Read carefully output, make sure understand . Check pairs!","code":"\ntmp = read.table('https://www.dropbox.com/s/zviurze7c85quyw/diabetes_full.csv?dl=1',header=TRUE,sep=\"\\t\") \ndat = tmp %>%\n  select(\n    where(is.numeric)\n  )\nhead(dat)##     id chol stab.glu hdl ratio glyhb age height weight bp.1s bp.1d bp.2s bp.2d\n## 1 1000  203       82  56   3.6  4.31  46     62    121   118    59    NA    NA\n## 2 1001  165       97  24   6.9  4.44  29     64    218   112    68    NA    NA\n## 3 1002  228       92  37   6.2  4.64  58     61    256   190    92   185    92\n## 4 1003   78       93  12   6.5  4.63  67     67    119   110    50    NA    NA\n## 5 1005  249       90  28   8.9  7.72  64     68    183   138    80    NA    NA\n## 6 1008  248       94  69   3.6  4.81  34     71    190   132    86    NA    NA\n##   waist hip time.ppn\n## 1    29  38      720\n## 2    46  48      360\n## 3    49  57      180\n## 4    33  38      480\n## 5    44  41      300\n## 6    36  42      195\n# Select the patients without NAs\ndat = dat %>%\n  na.omit()\nlibrary(pheatmap)\ncor.vals = cor(dat, method = \"spearman\")\n\npheatmap(cor.vals,\n         cluster_cols = FALSE,\n         cluster_rows = FALSE,\n         display_numbers = TRUE)\n## compute correlation\ncor(dat$stab.glu,dat$hip)## [1] 0.03528725\n##\n## test for significance\ncor.test(dat$stab.glu, dat$hip)## \n##  Pearson's product-moment correlation\n## \n## data:  dat$stab.glu and dat$hip\n## t = 0.40873, df = 134, p-value = 0.6834\n## alternative hypothesis: true correlation is not equal to 0\n## 95 percent confidence interval:\n##  -0.1338406  0.2024178\n## sample estimates:\n##        cor \n## 0.03528725"},{"path":"week-1-data-analysis.html","id":"univariate-linear-regression","chapter":"Week 1 : data analysis","heading":"1.4.2.1 Univariate linear regression","text":"Next, assess promising variable predict cholesterol level. Go back correlation heatmap see variables highly correlated .use glycosilated hemoglobin (glyhb) predictor cholesterol level function lm().simple linear regression (one explanatory variable), p-value t-test slope identical p-value F-test global model. longer case including several explanatory variables!way, checked linear regression makes sense case? Remember check :residuals normally distributedthere correlation residuals explanatory variableWhat overall opinion validity regression model ?can now use model predict cholesterol values, compare real cholesterol values, since information dataset.really super convincing, right? Let’s put information model!","code":"\nl.g = lm(chol ~ glyhb, data=dat)\nsummary(l.g)## \n## Call:\n## lm(formula = chol ~ glyhb, data = dat)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -84.458 -32.011  -7.436  20.584 156.407 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  169.514      9.774  17.343  < 2e-16 ***\n## glyhb          8.182      1.517   5.393 3.04e-07 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 44.02 on 134 degrees of freedom\n## Multiple R-squared:  0.1783, Adjusted R-squared:  0.1722 \n## F-statistic: 29.08 on 1 and 134 DF,  p-value: 3.045e-07\n# normal distribution of residuals?\nggplot() +\n  geom_histogram(aes(x = l.g$residuals)) \nggplot() +\n  geom_qq(aes(sample = l.g$residuals)) +\n  geom_qq_line(aes(sample = l.g$residuals),\n               colour = 'red')\n## correlation residuals x-values?\ncorr.gly_resi = cor(dat$glyhb,l.g$residuals)\n\nggplot() +\n  geom_point(aes(x = dat$glyhb, y = l.g$residuals))\nggplot() +\n  geom_point(aes(x = dat$chol, \n                 y = l.g$fitted.values),\n             colour = \"navy\") +\n  labs(x = 'Real values',\n       y = 'Predicted values') +\n  geom_abline(intercept = 0, slope = 1, \n              color = 'red')"},{"path":"week-1-data-analysis.html","id":"multiple-regression-model","chapter":"Week 1 : data analysis","heading":"1.4.2.2 Multiple regression model","text":"Let us include information try predict cholesterol level:note something unexpected report?can see inclusion several explanatory variables improves regression. Check \\(R^2\\) values example!can see variables weight, waist hip reach significance level. Two explanations possibleeither variables indeed non-informative regarding prediction cholesterol levelor mutual correlation 3 variables interferes model.can remove waist hip example, redo regressionWe can see weight variable seems indeed contribute prediction cholesterol. removal strongly correlated variables increased significance. now almost reaches significance 5% level!Check result F-test compare models!can now check prediction better univariate modelBetter? say … determine accuracy, can compute called root mean squared error (RMSE):\\[\nRMSE = \\sqrt{\\frac{1}{n}\\sum_{=1}^n (x_i-\\hat{x_i})^2}\n\\]course, cheating, right? predicting values exactly data used learn model. real machine learning, need perform cross-validation, .e. learn model one part data (training set) validate another set (test set).Let us split dataset test training set randomly:now learn new model train dataset:can compute RMSE training datasetLet use use model predict cholesterol values left test setand compute rmse:course, RMSE higher test dataset, since include data used establishment regression model; however, realistic estimation validity model, indicates well model extended novel, independent data!important topic feature selection, .e. finding optimal minimal set explanatory variables allow predict well output variable.now repeat train/test split 10 times time different random split; plot 10 rmse.train rmse.test values!","code":"\nl.all = lm(chol ~ .,data=dat)\nsummary(l.all)## \n## Call:\n## lm(formula = chol ~ ., data = dat)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -77.81 -10.15  -0.88  10.96  55.28 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  2.662e+01  5.724e+01   0.465   0.6428    \n## id          -2.722e-05  1.410e-04  -0.193   0.8472    \n## stab.glu     6.908e-02  5.848e-02   1.181   0.2398    \n## hdl          2.249e+00  1.500e-01  14.988   <2e-16 ***\n## ratio        2.520e+01  1.354e+00  18.611   <2e-16 ***\n## glyhb        4.451e-01  1.491e+00   0.299   0.7658    \n## age          1.566e-01  1.654e-01   0.947   0.3457    \n## height      -1.294e+00  6.645e-01  -1.947   0.0539 .  \n## weight      -6.610e-02  1.284e-01  -0.515   0.6077    \n## bp.1s        9.021e-02  2.053e-01   0.440   0.6611    \n## bp.1d       -1.778e-01  2.945e-01  -0.604   0.5472    \n## bp.2s       -1.940e-01  2.004e-01  -0.968   0.3350    \n## bp.2d        5.934e-01  2.983e-01   1.989   0.0490 *  \n## waist        1.013e+00  7.203e-01   1.406   0.1623    \n## hip         -5.150e-01  8.398e-01  -0.613   0.5409    \n## time.ppn    -9.561e-03  7.239e-03  -1.321   0.1891    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 21.78 on 120 degrees of freedom\n## Multiple R-squared:  0.8199, Adjusted R-squared:  0.7974 \n## F-statistic: 36.42 on 15 and 120 DF,  p-value: < 2.2e-16\nl.less = lm(chol ~ stab.glu + hdl + glyhb + age + height + weight  + bp.1s + bp.1d,data=dat)\nsummary(l.less)## \n## Call:\n## lm(formula = chol ~ stab.glu + hdl + glyhb + age + height + weight + \n##     bp.1s + bp.1d, data = dat)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -71.763 -29.004  -7.782  26.894 166.868 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 137.126166  75.534099   1.815 0.071819 .  \n## stab.glu     -0.055663   0.109905  -0.506 0.613406    \n## hdl           0.607084   0.227293   2.671 0.008555 ** \n## glyhb         9.914922   2.634210   3.764 0.000254 ***\n## age           0.242536   0.312447   0.776 0.439046    \n## height       -0.632074   0.985494  -0.641 0.522433    \n## weight       -0.058539   0.100419  -0.583 0.560962    \n## bp.1s         0.007349   0.215115   0.034 0.972801    \n## bp.1d         0.387031   0.373868   1.035 0.302539    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 42.44 on 127 degrees of freedom\n## Multiple R-squared:  0.2759, Adjusted R-squared:  0.2303 \n## F-statistic: 6.049 on 8 and 127 DF,  p-value: 1.39e-06\nggplot() +\n  geom_point(aes(x = dat$chol, \n                 y = l.less$fitted.values),\n             colour = \"navy\") +\n  labs(x = 'Real values',\n       y = 'Predicted values') +\n  geom_abline(intercept = 0, slope = 1, \n              color = 'red')\nn = nrow(dat)\nrmse = sqrt(1/n*sum(l.less$residuals^2))\nrmse## [1] 41.01479\nset.seed(1234)\n## take 200 random patients to form the training set\ni.train = sample(1:nrow(dat),100)\n##\ndat.train = dat[i.train,]\ndat.test = dat[-i.train,]\nl.train = lm(chol ~ stab.glu + hdl + glyhb + age + height + weight  + bp.1s + bp.1d,data=dat.train)\nsummary(l.train)## \n## Call:\n## lm(formula = chol ~ stab.glu + hdl + glyhb + age + height + weight + \n##     bp.1s + bp.1d, data = dat.train)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -76.806 -28.259  -6.705  22.934 158.094 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)   \n## (Intercept) 214.29240   95.18482   2.251  0.02677 * \n## stab.glu     -0.04772    0.13938  -0.342  0.73288   \n## hdl           0.24900    0.31841   0.782  0.43624   \n## glyhb        10.14338    3.22686   3.143  0.00225 **\n## age           0.41957    0.39691   1.057  0.29327   \n## height       -1.11670    1.26775  -0.881  0.38072   \n## weight       -0.11006    0.12823  -0.858  0.39298   \n## bp.1s        -0.12595    0.28635  -0.440  0.66110   \n## bp.1d         0.31732    0.50740   0.625  0.53328   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 46 on 91 degrees of freedom\n## Multiple R-squared:  0.2749, Adjusted R-squared:  0.2111 \n## F-statistic: 4.312 on 8 and 91 DF,  p-value: 0.0001965\nn.train = nrow(dat.train)\nrmse.train = sqrt(1/n.train*sum(l.train$residuals^2))\nrmse.train## [1] 43.88137\npred = predict(l.train, newdata = dat.test)\nn.test = nrow(dat.test)\nresiduals = dat.test$chol - pred\nrmse.test = sqrt(1/n.test*sum(residuals^2))\nrmse.test## [1] 34.75285\nset.seed(345)\nRMSE <- sapply(1:10, function(x) {\n  i.train = sample (1:nrow(dat),100)\n  ##\n  dat.train = dat[i.train,]\n  dat.test = dat[-i.train,]\n  ##\n  l.train = lm(chol ~ stab.glu + hdl + glyhb + age + height + weight  + bp.1s + bp.1d,data=dat.train)\n  ##\n  n.train = nrow(dat.train)\n  rmse.train = sqrt(1/n.train*sum(l.train$residuals^2))\n  ##\n  pred = predict(l.train,newdata = dat.test)\n  ##\n  n.test = nrow(dat.test)\n  residuals = dat.test$chol - pred\n  rmse.test = sqrt(1/n.test*sum(residuals^2))\n  RMSE <- c(rmse.train,rmse.test)\n  RMSE\n})\n#\n\nggplot() +\n  geom_point(aes(y = RMSE[1,],\n                 x = 1:10, colour = \"gold\")) +\n  geom_point(aes(y = RMSE[2,],\n                 x = 1:10, colour = \"steelblue\")) +\n  scale_x_continuous(breaks=c(2,4,6,8,10)) +\n  geom_hline(yintercept = mean(RMSE[1,]),\n             colour = \"gold\") +\n  ylim(min(RMSE), max(RMSE +2)) +\n  geom_hline(yintercept = mean(RMSE[2,]),\n             colour = \"steelblue\") +\n  scale_colour_manual(name = \"RMSE\", guide = 'legend',\n                      values =c('gold'='gold','steelblue'='steelblue'), \n                      labels = c('train','test')) +\n  labs(x = \"Iteration\", y = \"RMSE values\")"},{"path":"week-2-sequence-analysis.html","id":"week-2-sequence-analysis","chapter":"Week 2 : sequence analysis","heading":"Week 2 : sequence analysis","text":"","code":""},{"path":"week-2-sequence-analysis.html","id":"day-1-annotathon-identifying-orfs","chapter":"Week 2 : sequence analysis","heading":"1.5 Day 1: Annotathon! Identifying ORFs","text":"Annotathon project aims give students opportunity learn annotate unidentified sequences helping scientific community. course, results produce help progress discovery new species advance knowledge known ones.group affected unknown sequence DNA. mission, accept , analyse sequence gather maximum information can .means already known answer questions . follow bioinformatic process results uniq sequence.","code":""},{"path":"week-2-sequence-analysis.html","id":"getting-ready","chapter":"Week 2 : sequence analysis","heading":"1.5.1 Getting ready","text":"","code":""},{"path":"week-2-sequence-analysis.html","id":"creating-an-account","chapter":"Week 2 : sequence analysis","heading":"1.5.1.1 Creating an account","text":"create account https://annotathon.org/, clic “New account” tab\n\nFollow instructions open new account; make sure select appropriate Team\ncorrect “Team code”GKBIOINFO2024You required enter least one firstname/lastname pair, one email address order receive Annotathon specific notifications. email address secure circumstance made public passed third party. low traffic messages specific course duration mailed address; messages sent course completed.Finally clic “Open account” followed message “Account ‘XYZ’ created”. Use ‘username’ ‘password’ clic “Connect” form top page open Annotathon session. reminded email address validated followed special link included email automatically sent account creation.","code":""},{"path":"week-2-sequence-analysis.html","id":"reading-the-doc","chapter":"Week 2 : sequence analysis","heading":"1.5.1.2 Reading the doc","text":"guide Annotathon, document reading right now contains main informations guidelines. go details, can access complete “Rule Book” directly website.","code":""},{"path":"week-2-sequence-analysis.html","id":"getting-started","chapter":"Week 2 : sequence analysis","heading":"1.5.2 Getting started","text":"","code":""},{"path":"week-2-sequence-analysis.html","id":"getting-the-sequence","chapter":"Week 2 : sequence analysis","heading":"1.5.2.1 Getting the sequence","text":"first step get sequence. , go “Cart”, select sample clic “Add new sequence cart”.now sequence page lot informations sequence. (new look “Cart” page)example, GOS_11742010.1 name sequence analysing.“Actions” section, eye view state annotation, notebook change annotationthe “version” go back previous save annotationthe “Message” section send messages case questions“Genomic Sequence” get actual sequence FASTA format…","code":""},{"path":"week-2-sequence-analysis.html","id":"and-now-what","chapter":"Week 2 : sequence analysis","heading":"1.5.2.2 And now what ??","text":"Well, now time dive world sequence annotation. follow protocol described next chapter. step fill report directly website. start editing report, clic notebook.Every step analysis written following pattern :Protocol :\nDescribe tool used, used …Raw results:\noutput tool used","code":""},{"path":"week-2-sequence-analysis.html","id":"finding-orfs","chapter":"Week 2 : sequence analysis","heading":"1.5.3 Finding ORFs","text":"Steps find Open Reading Frame (ORF)","code":""},{"path":"week-2-sequence-analysis.html","id":"copy-your-fasta-sequence","chapter":"Week 2 : sequence analysis","heading":"1.5.3.1 copy your fasta sequence","text":"\ngo : https://www.ncbi.nlm.nih.gov/orffinder/Paste sequence choose parameters (forget note selected parameters report).study, consider ORFs verify following criteria:contain  STOP codons (basic ORF definition…)contains least 60 codonscan either direct reverse strandscan  frames 1, 2 3 strandcan  complete incomplete 5’ 3’ ends, !first entry annotathon report page, time talk one really important thing avoid frustration : REMEMBER SAVE WORK !!!!!!\n, clic “save annotations” button top page.Gather informations ORFs table like one :, make representation ORFs schematic figure :multiple ORFs, just arbitrary select biggest one. want details part, can refer rule book, skip classification ORF part.now one ORF annotate deeply, remember keep sequence ORF report save work.","code":""},{"path":"week-2-sequence-analysis.html","id":"conserved-protein-domains","chapter":"Week 2 : sequence analysis","heading":"1.5.4 Conserved protein domains","text":"new mission now find ORF contains known domains (comparison databases)\ngo https://www.ebi.ac.uk/jdispatcher/pfa/iprscan5Then copy raw output report make table informations described :analyse results following next guidelines :1. Selected domains (present)2. Rejected domains (present)make choices based output interproscan ressources find internet. remember cite exterior tool also save work.","code":"- Which domains predictions do you select to annotate the ORF? Specifiy their sizes, E-value! Justify your selection!\n- Refer clearly to Table 2 for your detailled analysis.- Why are some functional domains rejected? (high E-value?  No IPR domains?)"},{"path":"week-2-sequence-analysis.html","id":"day-2-annotathon-alignments","chapter":"Week 2 : sequence analysis","heading":"1.6 Day 2: Annotathon! Alignments","text":"","code":""},{"path":"week-2-sequence-analysis.html","id":"blast-homolog-search","chapter":"Week 2 : sequence analysis","heading":"1.6.1 BLAST homolog search","text":"BLAST well known tool identify homologs sequences. use , first go :\nhttps://blast.ncbi.nlm.nih.gov/Blast.cgi?PROGRAM=blastp&PAGE_TYPE=BlastSearch&LINK_LOC=blasthome\n, since already protein sequence use BLASTp (protein BLAST).BLAST done twice, NR database\n, using raw results tool : https://annotathon.org/outils/blast_definition_list.php\ncreate definition list.RESULTS ANALYSIS\ncontinue analysis fragment :homologs (small number homologs NR database, ie <100)gene already present NR biological database (nucleotide BLASTn  ID > 95%)Proposed structure analysis section:1. Overview alignmentsSynthetic description alignment results (number, known functions, quality alignments, …)Give details E-value range, % identity/similarity range, indels, alignment coverage, ….)2. Identification protein homologsJustify E-value thresholds (NR & SP) (Evalue cutoffs, changes putative homolog functions), refer tables 3 & 4.3. Function homologs SWISSPROT analysisFrom SP entries, give details closest homolog functions, specific role important amino acids involved catalytic function, cross analysis protein domain analysis. cases, cite sources web links.","code":""},{"path":"week-2-sequence-analysis.html","id":"blast-taxonomic-report","chapter":"Week 2 : sequence analysis","heading":"1.6.2 BLAST Taxonomic report","text":"Using output BLAST analysis now able construct taxonomic report. , go website https://annotathon.org/outils/blast_tax_report2.php , copy BLAST hits.\nHit Taxonomy Report button. Copy output report. , use second tool : https://annotathon.org/outils/blast_taxonomy_list.php produce summary table.\nhit Taxa table button get summary table like :can change classification level parameter precise prediction","code":""},{"path":"week-2-sequence-analysis.html","id":"selecting-sequences-for-multiple-alignment","chapter":"Week 2 : sequence analysis","heading":"1.6.3 Selecting sequences for multiple alignment","text":"Now, using previous results, create two groups sequences :study group (20 - 30 sequences) composed sequences taxonomic group ORF good E-value (lowest possible).study group (20 - 30 sequences) composed sequences taxonomic group ORF good E-value (lowest possible).external group (10 - 15 sequences) closest homologs outside taxonomic group ORF (Lowest e-value )external group (10 - 15 sequences) closest homologs outside taxonomic group ORF (Lowest e-value )keep amino acide sequences FASTA format.","code":""},{"path":"week-2-sequence-analysis.html","id":"day-3-annotathon-phylogenetic-trees-and-conclusion","chapter":"Week 2 : sequence analysis","heading":"1.7 Day 3: Annotathon! Phylogenetic trees and conclusion","text":"","code":""},{"path":"week-2-sequence-analysis.html","id":"multiple-alignments","chapter":"Week 2 : sequence analysis","heading":"1.7.1 Multiple alignments","text":"Now list sequences can go website multiple sequence alignment : http://www.phylogeny.fr/simple_phylogeny.cgi?workflow_id=b02e40313c3ca8c0e3d4dbc97db10078&tab_index=2The limitation number sequences align simply due computation time multiple alignment programs, well subsequent phylogenetic tree reconstruction. Computation time reasonable around thirty fifty sequences hundred residues.Copy & paste  “ClustalW” formated multiple alignment ‘Multiple Alignement’ Annotathon field.RESULTS ANALYSIS:\n1. Quality multiple alignmentCan confirm sequences really homologs? Similar lengths? many identical positions? many conservative substitutions positions? Number indels? can find conservation sequences within alignment reflects subgroups (groups)?curation GBLOCKS, number conserved homolog positions (informative sites) phylogenetic reconstruction? enough?2. Identification conserved blocksYou can annotate well conserved blocks alignment codes (, B, C etc.) refer analysis.conserved amino acids known actives sites protein family? yes, position alignment, function, activity?3. N C-termini studied ORFAnalysis N-ter/C-term alignment (complete? start codon? potentially missing number amino acids N C-termini?)","code":""},{"path":"week-2-sequence-analysis.html","id":"phylogeny","chapter":"Week 2 : sequence analysis","heading":"1.7.2 Phylogeny","text":"construct phylogenic tree :\nUse previous multiple alignment infer phylogenetic tree using two distinct tree reconstruction approaches:‘distance’ method (e.g. ‘neighbor-joining (NJ)’, ‘BioNJ’ ‘Phylip protdist/neighbor’)‘maximun likelyhood’ method (e.g. ‘PhyML)’)Go place : http://www.phylogeny.fr/alacarte.cgiThen use tool : https://annotathon.org/outils/nw_utils.php\ncreate tree format expected report. (copy NEWICK format tree paste box)Copy & paste  textual tree representation ‘Tree’ Annotathon field. Remember include protocol line ‘Tree’ field includes program name run parameters (ex ‘Phylip / Protdist+neighbor / Randomized input - Random number seed = 11 / rooted : Coccidioides immitis (ascomycetes)’).**Suggested plan analysis section:1. Tree topologies2. Coherence reference trees3. Predict likely taxonomic origin metagenomic ORFAfter analysed phylogenetic tree produced, specify likely taxonomic group (e.g. “Alphaproteobacteria”) belongs organism carrying DNA fragment. specify group ‘Taxonomy’ Annotathon field two options:specify ‘NCBI numerical identifier’ box taxonomic group code (instance 204455  Rhodobacterales, codes found GENBANK records feature table, /db_xref=“taxon:204455” can found querying NCBI taxonomy database using link Help tab)specify exact scientific name group ‘Scientific name’ box (e.g. Rhodobacterales)","code":"- Describe the topology of each tree. What are the monophyletic groups?\n- Do the two independant trees describe the same evolutionary history? the same topology? Similar or different clades? \n- Identify the commonalities as well as the potential incoherencies.- Are the in- and out-groups correctly separated?\n- Are your gene trees coherent with the reference species trees (\"tree of life\")? \n- Identify each discrepancy with the reference species tree, and suggest some explanations (HGTs, gene duplications...).- In which monophyletic clade does the the metagenomic sequence seem to emerge?\n- Propose a hypothetical taxonomic classification for the metagenomic ORF!\n- Provide detailed justification of your hypothesis, do not under/over interpret the infered phylogenetic trees!"},{"path":"week-2-sequence-analysis.html","id":"conclusion","chapter":"Week 2 : sequence analysis","heading":"1.7.3 Conclusion","text":"Write interpretations hypotheses based observations made preceeding “RESULTS ANALYSES”. Imagine trying convince sceptical colleague: use rigorous argumentation, cite precise evidence numerical values ever possible, highlight important findings, cross information independent sources. Remember silico analyses generally constitute final proof, suggestions. Terms “putative”, “suggests” “probably” can show understanding limitations computational biology results.\nMake sure least covered:arguments supporting coding versus non-coding hypothesis; make sure discuss start position ORF (refer FAQ subtlities pitfalls)!functionnal predictions protein, biochemical level (e.g. “ubiquitin conjugation enzyme”), biological role organism level (e.g. “implicated control cell cycle”). Make careful use annotations available sequence homologs conserved domainsyour taxonomic classification hypothesis organism carrying DNA fragment.common pitfalls avoid cost:explain theoretical aims methodology tools used (please consider readers knowledgable matters)describe button clicked (please consider readers highly familiar running BLASTs)write telegraphic styledilute, inflate, digress hope evaluation proportional word volumecopy raw results extenso discussion reader direct access appropriate fieldswrite linearly without structure, purely chronologicallyinsulate analyses (can, certainly , make reference multiple alignment discussing ORF boundaries)conclude without references results observationspropose hypotheses without providing supporting evidencemake approximate statements, citing BLAST homologs conserved domains without citing respective E-valuesConcentrate producing scientific, structured, synthetic rigorous argumentation hold peer scrutiny!","code":""},{"path":"week-2-sequence-analysis.html","id":"day-4-data-formats-and-where-to-find-them","chapter":"Week 2 : sequence analysis","heading":"1.8 Day 4: Data formats and where to find them","text":"Today go commonly used tools databases applied genomic research. learn :Identify common data formats know produced;Search obtain genomic data commonly used databases - GEO;Handle different data formats used transcriptomics epigenomics;Visualize genomic data IGV.","code":""},{"path":"week-2-sequence-analysis.html","id":"meet-igv","chapter":"Week 2 : sequence analysis","heading":"1.8.1 Meet IGV","text":"Alongside next sections, find examples files IGV Viewer use exercises. Access . work Web app, IGV provides local software quite versatile .menu, can select genome locations (format chrN:start-end) search genes.","code":""},{"path":"week-2-sequence-analysis.html","id":"example-1","chapter":"Week 2 : sequence analysis","heading":"1.8.1.1 Example 1","text":"Try IGV using session.\nrecognize differences different histone modifications? Blue represents active histone modifications (linked active transcription). H3K27ac classical marker active gene promoters open chromatin regions. H3K36me3 linked gene bodies.Conversely, red represents repressive histone modifications. H3K27me3 H3K9me3 associated repression/heterochromatin. However, H3K9me3 meant much permanent, H3K27me3 often associated regions can activated development.genome assembly used?Inspect HOXD gene cluster (Find chr2:175,992,177-176,263,537). Based signal tracks, think cluster genes expressed heart?Check different genes (eg. BDP1, NPLOC4, PIK3C3) observed shape H3K27ac (activation) signal promoters.","code":""},{"path":"week-2-sequence-analysis.html","id":"identifying-formats","chapter":"Week 2 : sequence analysis","heading":"1.8.2 Identifying formats","text":"Genomic data generally fit one four classes:Nucleotide sequences (like FASTQ);Read alignments (like BAM files);Annotations (GTF/GFF files);Quantitative outputs (like count tables, peaks, among others).Different formats /classes processed visualised different ways distinct features.Let us start example FASTQ file:FASTQ files text-based (many sequence classes) common format storage sequences quality scores high-throughput sequencing technologies. large files, generally compressed (extension .fastq.gz). opening FASTQ file (using zless less terminal), entry generally look like:One FASTQ file contains millions entries. entry (starting @) composed sequence identifier (case, @A00665:126:HTNC2DRXX ...), sequence , separator (case, +), quality scores (Phred +33 encoded) base called. Sequence identifiers include information allows us know name instrument produced , run identifiers, flowcell identifiers, pair number, index sequence, among others.example paired-end sequencing, meaning two FASTQ files per sample entries match . .e., @A00665:126:HTNC2DRXX also exist FASTQ file 2. sequencing single-end, one FASTQ file produced per sample.Upon alignment reference genome, FASTQ generates BAM file. BAM files compressed formats SAM files (stands Sequence Alignment/Map format), represent read alignments genomic location. SAM files tabular-based include header (start @) alignment lines.Alignment lines, corresponding reads, include different elements shown . One important flag, includes information alignment given read. flag (red) just number represents different combinations read properties can see . Flags can used remove unmapped read pairs failing reads downstream processing.Check link flag 99, 512, 13 mean SAM file.large files, much common see read alignments BAM format. However, BAM files human-readable FASTQ . visualize BAM file later IGV. looks like:FASTQ read alignment formats generally common different kinds assays, output alignments depends assay. Let us explore different formats assay associated .","code":"@@A00665:126:HTNC2DRXX:1:2101:1136:1047 1:N:0:AGGCAGAA+NTAAGGAG\nGNCCTTACTAGACCAATGGGACTTAAACCCACAAACACTTAGTTAACAGCT\n+\nF#:FFFFFFFFF:FFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFF:FF"},{"path":"week-2-sequence-analysis.html","id":"where-do-different-formats-come-from","chapter":"Week 2 : sequence analysis","heading":"1.8.2.1 Where do different formats come from?","text":"Assays meant assess chromatin accessibility, transcription factors, histone modifications typically based peaks, saved BED-like format. BED file structured :example, line detected peak given location genome (indicated chromosome start end). Additional fields (like p-value, log fold change, length, among others) often included depend software generated .Chromatin accessibility, transcription factors, histone modifications can also represented BigWig/bedGraph format. formats, look like summits, represent signal. assays require control distinguish signal noise (ChIP-seq, usually refered Input).Let us see example BigWigs peaks.","code":"chr1    9922    10469   \nchr1    564407  565441  \nchr1    565595  567010  \nchr1    2583259 2588018 \nchr1    2627935 2629661 "},{"path":"week-2-sequence-analysis.html","id":"example-2","chapter":"Week 2 : sequence analysis","heading":"1.8.2.2 Example 2","text":"Load session see example ChIP-seq ATAC-seq correlate .Observe peaks (rectangles coverage tracks) always identified “bumps” BigWig.","code":""},{"path":"week-2-sequence-analysis.html","id":"a-practical-guide-to-geo","chapter":"Week 2 : sequence analysis","heading":"1.8.3 A practical guide to GEO","text":"GEO, standing Gene Expression Omnibus one used repositories microarray, next-generation sequencing, genomic datasets. publishing manuscript, authors must upload sequencing data produced public database make accessible others, GEO often chosen one. reason, GEO includes thousands original submitter-supplied records can accessed place anyone.learn search GEO read GEO record obtain specific information.\nDatasets GEO can found directly valid GEO accession number.","code":""},{"path":"week-2-sequence-analysis.html","id":"using-filters-and-querying","chapter":"Week 2 : sequence analysis","heading":"1.8.3.1 Using filters and querying","text":"Searching datasets always easy although aspects GEO common, others . Submitters freedom adapt descriptions fitting specific submission experiment, can make things harder filtering datasets.can see global aspect GEO search :study section one important, indicates kind assay performed goal. example, apply Expression profiling … looking transcription data Genome binding/occupancy profiling …  looking chromatin accessibility, histone modifications, transcription factor data. addition, can also select type assay (high throughput sequencing, array, Mass Spec, …)., ready dive GEO record. Feel free open one see information inside.","code":""},{"path":"week-2-sequence-analysis.html","id":"geo-records","chapter":"Week 2 : sequence analysis","heading":"1.8.3.2 GEO records","text":"Oftentimes can challenging find data want format need. uploading data GEO somewhat standardized procedure, looking processed formats can tricky.bottom GEO record, can find Supplementary files. typically includes collection (zipped) supplementary files (can access individually selecting single sample).","code":""},{"path":"week-2-sequence-analysis.html","id":"example-3","chapter":"Week 2 : sequence analysis","heading":"1.8.3.2.1 Example 3","text":"example, explore GEO record obtain information data processing .\nSearch dataset identified GSE205807.cell line dataset generated ?species genome assembly used alignment?kind processed files present sample?","code":""},{"path":"week-2-sequence-analysis.html","id":"exercises-4","chapter":"Week 2 : sequence analysis","heading":"1.8.4 EXERCISES","text":"","code":""},{"path":"week-2-sequence-analysis.html","id":"exercise-1.-finding-transcription-factor-targets-using-chip-seq","chapter":"Week 2 : sequence analysis","heading":"1.8.4.1 Exercise 1. Finding transcription factor targets using ChIP-seq","text":"ChIP-seq can used identify genes regulated targeted transcription factors. example, explore dataset obtained human liver cell line HepG2.Download set peaks FOS transcription factor GEO dataset GSE104247.1.1. genome assembly dataset aligned ? Apply necessary changes see IGV.1.2. Load dataset IGV web app. Give couple examples genes likely regulated FOS cell line.1.3. Pick set peaks obtained favorite transcription factor GEO dataset! set peaks observed comparable?","code":""},{"path":"week-2-sequence-analysis.html","id":"exercise-2.-atac-seq-and-dnase-seq-comparison","chapter":"Week 2 : sequence analysis","heading":"1.8.4.1.1 Exercise 2. ATAC-seq and DNase-seq comparison","text":"2.1. Load track BED BigWig format ATAC-seq DNase-seq obtained cell line using Tracks section ENCODE (BED) ENCODE Signals - (Tip: typically plenty material included A549 cell line). Compare called peaks features experiment (peak size, noise, coverage) discuss advantages one .2.2 (Optional) Load 4DN track insulation score cell line. 4D Nucleome Data Portal way quantify number regions interacting within window. chromosomal scale, see correlates () ATAC-seq track? regions present high insulation score?","code":""},{"path":"week-2-sequence-analysis.html","id":"exercise-3.-using-geo-to-look-for-dna-methylation-datasets","chapter":"Week 2 : sequence analysis","heading":"1.8.4.1.2 Exercise 3. Using GEO to look for DNA methylation datasets","text":"3.1. filters apply look dataset DNA methylation array includes rhesus macaque (Macaca mulatta) gorilla (Gorilla gorilla)?3.2. (optional) want find multiple transcriptomics datasets human tissues infected malaria parasites. filters apply?","code":""},{"path":"week-2-sequence-analysis.html","id":"exercise-4.-going-further","chapter":"Week 2 : sequence analysis","heading":"1.8.4.1.3 Exercise 4. Going further","text":"Let us see different type data - interaction data ChIA-PET. Re-open session used example 2. , load ChIA-PET loop track A549 (genome-wide chromatin interactions mediated CTCF) using Tracks > ENCODE .CTCF important element genome organisation ChIA-PET assay used measure chromatin interactions mediated specific factors.Observe interactions chromosomal gene level. can conclude comparison ATAC-seq, CTCF ChIP-seq, ChIA-PET?interaction mediated CTCF always associated CTCF peak? ?","code":""},{"path":"week-2-sequence-analysis.html","id":"day-5-rna-seq---from-fastq-to-count-matrix","chapter":"Week 2 : sequence analysis","heading":"1.9 Day 5: RNA-seq - from FASTQ to count matrix","text":"Today analyze RNA sequencing data.use RNA-seq data paper: Nature volume 571, pages505–509 (2019). raw sequence data stored : https://www.ebi.ac.uk/biostudies/arrayexpress/studies/E-MTAB-6798/sdrfWe take two samples . One 2 week post-conception (2WPC) mouse brain 4WPC brain.can find raw data :start fastq files, align reference genome quantify gene expression. can find workflow .","code":"/vol/data/raw/mouse_brain_2wpb.fastq.gz\n\n\n/vol/data/raw/mouse_brain_4wpb.fastq.gz"},{"path":"week-2-sequence-analysis.html","id":"quality-control","chapter":"Week 2 : sequence analysis","heading":"1.9.1 Quality control","text":"retrieve raw data employ software tool named fastqc examine quality sequence files.next examine quality samples fastqc.start FastQC quality report provides basic statistics evaluation various quality parameters.Question: information get QC report?particular, look (1) per base quality, (2) per sequence quality.Question: FastQC quantify sequence quality?Question: find compare two sequences?","code":"mkdir path/to/your_fastqc_folder\n\nfastqc /vol/data/raw/mouse_brain_4wpb.fastq.gz -o path/to/your_fastqc_folder/\n\nfastqc /vol/data/raw/mouse_brain_2wpb.fastq.gz -o path/to/your_fastqc_folder/\n"},{"path":"week-2-sequence-analysis.html","id":"alignment","chapter":"Week 2 : sequence analysis","heading":"1.9.2 Alignment","text":"Alignment involves process mapping sequences reference genome.use Bowtie2 implement seuqence alignment. Common aligners include BWA, TOPHAT, STAR. differ accuracy, speed, memory usage due different computational algorithms.Question: view BAM files? find BAM files?\nRef: https://bookdown.org/content/24942ad6-9ed7-44e9-b214-1ea8ba9f0224/learning--bam-format.html\nHint: Use samtools view view BAM files.use samtools flagstat evaluate quality aligned bam file.","code":"\n# Create a new directory\n\nmkdir path/to/your_bam_folder\n\n\n# Alignment (It will take for a while)\nbowtie2 -p 2 --no-unal -x /vol/data/raw/chr6 -U /vol/data/raw/mouse_brain_2wpb.fastq.gz | samtools view -bS - > path/to/your_bam_folder/mouse_brain_2wpb_chr6.bam \n\nsamtools flagstat path/to/your_bam_folder/mouse_brain_2wpb_chr6.bam  > path/to/your_bam_folder/mouse_brain_flagstat_2wpb_chr6.txt\n"},{"path":"week-2-sequence-analysis.html","id":"quantification","chapter":"Week 2 : sequence analysis","heading":"1.9.3 Quantification","text":"Now, utilise aligned BAM files quantify gene expression samples. implement tool named htseq-count. various methods exist quantifying gene expression htseq-count. illustration, please refer tutorial (https://htseq.readthedocs.io/en/release_0.11.1/count.html).","code":"\n# Create a new directory\nmkdir path/to/your_count_folder\n# Keep the high quality reads\n\nsamtools view -bq 40 path/to/your_bam_folder/mouse_brain_2wpb_chr6.bam > path/to/your_bam_folder/mouse_brain_2wpb_chr6_Q40.bam\n\nhtseq-count -s no -m intersection-nonempty --nonunique all --format bam path/to/your_bam_folder/mouse_brain_2wpb_chr6_Q40.bam /vol/data/raw/mm39.ncbiRefSeq.chr6.gtf > path/to/your_count_folder/mouse_brain_2wpb_chr6.txt\n"},{"path":"week-2-sequence-analysis.html","id":"visualization-2","chapter":"Week 2 : sequence analysis","heading":"1.9.4 Visualization","text":"","code":"\nlibrary(tidyverse)\nsetwd(\"path/to/your_count_folder\")\n\n\ntb_2wpb <- read_tsv(\"mouse_brain_2wpb_chr6.txt\", col_names = FALSE) %>% \n  rename(Gene = X1, Count = X2) %>% \n  mutate(Sample = \"2wpb\")\n\ntb_4wpb <- read_tsv(\"mouse_brain_4wpb_chr6.txt\", col_names = FALSE) %>% \n  rename(Gene = X1, Count = X2) %>%\n  mutate(Sample = \"4wpb\")\n\ntb_merge <- bind_rows(tb_2wpb, tb_4wpb)\n\ntb_merge %>% ggplot(mapping = aes(x = log10(Count + 1), fill = Sample)) +\n  geom_density(alpha = 0.2)\n"},{"path":"week-2-sequence-analysis.html","id":"excercises","chapter":"Week 2 : sequence analysis","heading":"1.9.5 Excercises","text":"Please explore bam files IGV. find?Please try run whole workflow learnt today mouse chromosome 8. reference genome sequence can found /vol/data/raw. However, encouraged retrieve reference genome databases .Please explore count matrix two samples. find genes express differentially samples? Also, think , project leader, design experiment hypothesis testing strategy opt ? (Note: Formal analysis revealed next week)","code":""},{"path":"project-report-guide.html","id":"project-report-guide","chapter":"Project report guide","heading":"Project report guide","text":"end course, produce report RMarkdown. RMarkdown inherantly ready-publish format. Meaning mistakes text, simply produce PDF one go. Let’s see .","code":""},{"path":"project-report-guide.html","id":"knitting","chapter":"Project report guide","heading":"1.10 Knitting","text":"Create new RMarkdown document start trying . can already call document Report like . can also use layout made available .clicking little blue yarn ball top, can Knit document PDF (PDF option can found right drop arrow).Tips ensure catch errors early :Knit document every often! much code/text verified ’s running , might cause issues later .Read errors : R usually tell error located. times ’s character error characters meanings RMarkdown always clear first.Check missing dependencies like dplyr, tidyr, ggplot2 using packages.Comment code!Keep things short, simplify, avoid wall text/code hardly decipherable.","code":""},{"path":"project-report-guide.html","id":"formatting","chapter":"Project report guide","heading":"1.11 Formatting","text":"New chapter titles subtitles defined # (larger title 1.) #### (smaller title 1.1.1.1). formatting can found .Numerical equations special characters formatting. example: $\\sqrt{13}$ square-root 13 $\\alpha=0.05$ alpha=0.05Here equation:\\[\\begin{equation}\n  f\\left(k\\right) = \\binom{n}{k} p^k\\left(1-p\\right)^{n-k}\n  \\tag{1.1}\n\\end{equation}\\]may refer equation using \\@ref(eq:binom).","code":"\\begin{equation} \n  f\\left(k\\right) = \\binom{n}{k} p^k\\left(1-p\\right)^{n-k}\n  (\\#eq:binom)\n\\end{equation}"},{"path":"project-report-guide.html","id":"new-pages-and-paragraphs","chapter":"Project report guide","heading":"1.11.1 New pages and paragraphs","text":"can use \\newpage create new pages \\par make paragraphs.","code":""},{"path":"project-report-guide.html","id":"chunk-options","chapter":"Project report guide","heading":"1.12 Chunk options","text":"multitude options display R chunks outputs. See . ones likely use :echo=FALSE: hides code resultsresults='hide': hides results codeinclude=FALSE: include chunkeval=FALSE: run chunkSome ones control figure placement size:fig.heightfig.widthfig.align='center'.width Extra: results='hold', .width='50%': allow show 2 figure/plot outputs side side.","code":""},{"path":"project-report-guide.html","id":"external-images","chapter":"Project report guide","heading":"1.13 External images","text":"including external images, need indicate location figure relative document: ![Figure](figure_folder/avocado_image.jpeg)additions like table, check RMarkdown cookbook.Figures tables captions can also cross-referenced elsewhere using \\@ref(fig:chunk-label) \\@ref(tab:chunk-label), respectively.Don’t miss Table 1.1. Reference tables using \\@ref(tab:nice-tab)Table 1.1: nice table!","code":"\nknitr::kable(\n  head(pressure, 5), caption = 'Here is a nice table!',\n  booktabs = TRUE\n)"},{"path":"project-report-guide.html","id":"references","chapter":"Project report guide","heading":"1.14 References","text":"report likely include references. RMarkdown accompanied .bib extension file. Indicate name file beginning report (see example). can see one included example . open file text editor, can see entries structured like :citation format compatible citation managers like BibDesk (available macOS) Mendeley. using one , able add references. Watch reference key Markdown.","code":"@book{ggplot2,\n    author = {Hadley Wickham},\n    isbn = {978-3-319-24277-4},\n    publisher = {Springer-Verlag New York},\n    title = {ggplot2: Elegant Graphics for Data Analysis},\n    url = {https://ggplot2.tidyverse.org},\n    year = {2016},\n    bdsk-url-1 = {https://ggplot2.tidyverse.org}}"},{"path":"project-report-guide.html","id":"citations","chapter":"Project report guide","heading":"1.14.1 Citations","text":"Reference items present bibliography file(s) Markdown using @key [@key]. previous example, key actually ggplot2!","code":""},{"path":"project-report-guide.html","id":"footnotes","chapter":"Project report guide","heading":"1.14.2 Footnotes","text":"Footnotes put inside square brackets caret ^[].","code":""}]
