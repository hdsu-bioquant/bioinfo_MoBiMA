# Week 1 : data analysis {.unnumbered}

## Day 1: Descriptive statistics and data types {.unnumbered}

Today, you will learn how to perform basic tasks on a dataframe/tibble, descriptive statistics, perform data cleaning, and plotting.

### Data features and where to find them

#### Load the data

The diabetes dataset, which we will be using in this practical class will be downloaded from an online repository.
We will load that into R and have a sneak peek into how it looks like with the console.
In the following you will see several functions that give us information about our dataset.

```{r}
dat = as_tibble(read.delim('https://tinyurl.com/y4fark9g')) # Load the dataset
head(dat, 10) # Look at the first 10 lines of the table
```


#### Dimensions and naming

**1.** What is the dimension of our dataset (i.e. how many rows/columns are there in our data)

```{r}
# Dimension
dim(dat)
```

```{r, results='hide'}
# Number of columns
ncol(dat)
# Number of rows
nrow(dat)
```

**2.** What are the column names of our dataset

```{r}
colnames(dat) # Similarly rownames() for rows
```

Probably you are confused about what these column names mean.
For more description on these values [look here](https://biostat.app.vumc.org/wiki/pub/Main/DataSets/Cdiabetes.html)

#### Numerical features 

**3.** How do we extract the minimum and maximum age of patients in our dataset?

```{r, results='hide'}
min(dat$age)
max(dat$age)
range(dat$age)
```

> Can you find out the same for height and weight?

**4.** How does the overall summary of our entire dataset look like?

```{r, results='hide'}
summary(dat)
```

> Can you explain what you see after you run the `summary()` function?


Feel free to play around with this syntax until you feel comfortable with it.
You can open a window with `View(dat)` to compare your results.

------------------------------------------------------------------------

### Data cleaning

Very often the first thing one needs to do before any data science project is to clean up the raw data and transform it into a format that is readily understood and easy to use for all downstream analysis.
This process usually involves: --

-   Removing empty value rows/columns
-   Removing unused or unnecessary rows/columns
-   Reordering the data matrix
-   Keeping columns uniformly numeric (age, weight etc) or string (names, places etc) or logical (TRUE/FALSE, 1/0)
-   Handling strange caveats which are data specific like replacing `,` or `.`, or `;` from numbers etc

Lets do some clean up of our own diabetes data

1.  We will make the `id` column the row names for the dataset;
2.  We will remove the `bp.2s` and `bp.2d` columns as it has mostly missing values (see summary above);
3.  We will also remove the column `time.ppn` which will not be required in our analysis;
4.  We will reorder the columns of the data such that all the qualitative and quantitative values are separated.

To perform this cleanup, we need a couple of important functions, that we will first discuss:

-   `filter`
-   `is.na`
-   `mutate`
-   `accross`
-   `%in%`

#### filter()

`filter()` is used on a dataset to filter rows satisfying a condition you specify like we saw previously (Introduction).
Let's look at an example. We are only filtering for senior individuals in our dataset. 

```{r}
dat_seniors = dat %>%
  filter(age <= 65)
```

We can also filter based on other conditions, like location, sex, among others.
In some cases, we can also use `which()` to filter values. The syntax is different...

```{r, eval=FALSE}
dat[dat$age <= 65,]
```

... but it works for vectors and other classes. Let's see the next example.

```{r, eval = TRUE}
# number of animals you have
number = c(2,3,4,5,1,2,5)
# Let's create a different vector (of the same length)
animals = c("cat", "dog", "cow", "parrot", "zebra", "sparrow", "lizard")
# Let's use the "which()" function now
animals[which(number > 2)]
```

We selected all animals from the "animals" vector that correspond to more than three individuals in the "number" vector.

#### is.na()

`is.na()` is used to determine if NA values are present in a given object. We can try a simple example with one variable being assigned as NA.
```{r}
x = 2
is.na(x)

y = NA
is.na(y)
```

We can do this with vectors obtained from `dat`. What class is the output in? 

```{r, results='hide'}
is.na(dat$glyhb)
```

#### mutate()

`mutate()` is often used to create a new column based on another column of the dataframe. Let us use this function to *mutate* two new columns including the weight in kilograms and the height in centimeters. **The conversion from pounds to kilograms can be done by multiplying weight in pounds by 0.454. To covert height to centimeters we only need to multiply height (inches) by 2.54**.  

```{r}
dat %>%
  mutate(weight.kg = weight * 0.454,        # you can generate both columns using the same mutate!
         height.cm = height * 2.54) %>%     # we do not need to save this output
  select(id, weight.kg, height.cm)
```

#### accross()

`accross()` is very often used together with `mutate()` and another helper function, like `everywhere()`, `starts_with()`, `ends_with()`, or `contains()`.
Later, we will use `accross()` together with the other functions we learned previously to remove NAs like this:

```{r, results='hide', eval=FALSE}
dat %>%
  rowwise() %>%
  mutate(na_count = sum(is.na(across(everything()))))
```

There is much to unpack here:

-  `rowwise()` ensures that the next operations are applied by row.  
-  `mutate()` adds a new column called na_count to the dataframe.  
-  `across(everything())` selects all columns in the current row.  
-  `sum(is.na(...))` calculates the sum of missing values for each row.  

> Try to run the previous example without `rowwise()`. What does it look like?



#### %in%

This is an operator to check which elements of a first vector are inside a second vector.

```{r}
c('Frodo','Sam') %in% c('Pippin','Sam','Frodo','Merry')
```

#### Ready for the cleaning!

The first column of the dataframe is the column with the name "id".
The rows are just numbered, without names.
We are going to rename the rows using the column "id". The function `column_to_rownames()` allows us to do this efficiently.  

```{r, results='hide'}
# set the row names using the column id
dat = dat %>%
  column_to_rownames(var = 'id')
```

> Keep in mind that rownames must be unique!

The *na_count* column will then include the number of NAs per row. Do you understand how it works? 
We finally apply `filter` again to keep only rows with less than or 2 NAs.

```{r}
dat = dat %>%
  rowwise() %>%
  mutate(na_count = sum(is.na(across(everything())))) %>%
  filter(na_count <= 2)
```

We will also remove the *na_count* and some problematic columns (*bp.2s*, *bp.2d* and *time.ppn*) by **selecting the ones which are not these**. We can do this using `!`, as this character can be used to invert results. Let us try it with `select()`. 

```{r}
dat = dat %>%
  select(!c(na_count, time.ppn, bp.2d, bp.2s))
```


Next, we can re-order the remaining columns, in order to put the categorical columns first, and numerical columns after. We can use `select` to order columns too, but we need to combine it with `where()` and functions which verify the class of the columns, like `is.character()` or `is.numeric()`.  

Here is a simple example:
```{r, results='hide'}
# Create a character and numeric 
name = c("Antonia")
age = c(23)

# Verify if the previous object are from the character/numeric classes
is.character(name)
is.character(age)
is.numeric(age)
```

And here we can apply the same principle to the re-ordering:
```{r}
dat <- dat %>%
  select(
    # Select categorical columns
    where(is.character), 
    # Select numerical columns
    where(is.numeric)
  )

# OR you can use the indexes too, but if you more than 10-20 columns, that is not ideal
# dat = dat[,c(8,6,11,9,10,14,15,2,5,1,3,4,12,13)]
```

Now lets look at our cleaned data:

```{r}
summary(dat)
```

Hold up, the ordering and selection of columns looks right, but it seems that there are certain rows that have missing values still (like `glyhb` column has 3 `NA` values still).
Lets remove all rows with any missing value using `na.omit()`.
Remember, 1 row = 1 patient.  

```{r}
dat = dat %>%
  na.omit()
```

> How many patients were removed because they were associated with missing values?

Now our cleaned data has no missing values, columns are cleanly ordered and each column is in the right format

```{r}
summary(dat)
```

> Can you identify which types of data (continuous, discrete etc) each column above represents and why?

------------------------------------------------------------------------

### Visualizing data distribution

In this section you will also learn the essential functions to plot data in an intuitive and useful way using the `ggplot2` package, just like in the introductory section to tidyverse. 

#### Histograms

We can plot the column "stab.glu" as a histogram using the `hist()` function:

```{r, message=F}
ggplot(dat,
       aes(x = stab.glu)) +
  geom_histogram() +
  labs(x = "Stabilized Glucose concentration in blood",  # add labels to the x-axis
       title = "Glucose concentration")                  # add title
```

> Add the parameter `bins = 50` in the above lines of code (inside `geom_histogram`) and see what happens.
> Try different values for `bins` like `10, 20, 75, 100`. Can you interpret the differences?
> Is this a good or bad thing about histograms?

#### Density plots

For density plots, we use the `geom_density()` function to estimate the probability density function for a given variable.

```{r eval = TRUE}
ggplot(dat,
       aes(x = stab.glu)) +
  geom_density() +
  labs(x = "Stabilized Glucose concentration in blood",  # add labels to the x-axis
       title = "Glucose concentration")                  # add title
```

#### Boxplots

The `boxplot()` function produces a boxplot for a given variable:

```{r fig.width=4, fig.height=2}
ggplot(dat,
       aes(x = stab.glu)) +
  geom_boxplot() +
  labs(x ="Stabilized Glucose concentration in blood")
```

> Can you explain all features of this graph, such as upper/lower whisker, 25% quantile, ...? 

#### QQ-plots

We can use **QQ-plots** to either (1) compare two distributions, or (2) compare a distribution with a theoretical distribution (typically the normal distribution).

We can for example compare the distribution of the blood pressure values to check if they are normally distributed

```{r eval=TRUE}
## Let's first make a histogram
ggplot(dat,
       aes(x = bp.1s)) +
  geom_histogram(bins = 50)
```

Now we can use the function `geom_qq()` to generate the **QQ-plot** of this distribution against the standard normal distribution:

```{r eval = TRUE, fig.width=3, fig.height=3}
ggplot(dat,
       aes(sample = bp.1s)) +     # we use sample= inside aes for the QQ-plot
  geom_qq()                       # creates the QQ-plot
```

Using the additional command `geom_qq_line()`, we can add a straight line that goes through the first and third quartile:

```{r fig.width=3, fig.height=3}
ggplot(dat,
       aes(sample = bp.1s)) +     # we use sample= inside aes for the QQ-plot
  geom_qq() +
  geom_qq_line(colour = 'red')                  # adds in the QQ-line on top
```

> So, is the distribution normal??

Now let's compare the quantiles of the cholesterol values by biological sex.
**Notes on `ggplot()` here:** Rather than `ggplot(dataset, aes(...))` we use `ggplot() + geom_xx(aes(...))` for situations where the data we wish to plot is not in a dataframe.

```{r fig.width=3, fig.height=3}
# We can use "filter()" to filter the cholesterol values for men and women
dat.male = dat %>%
  filter(gender == 'male')

dat.female = dat %>%
  filter(gender == 'female')

# Compute the quantiles (note the "na.rm" option to ignore missing NA values!)
q.male = quantile(dat.male$bp.1s, 
                  probs=seq(0,1,by=0.05), 
                  na.rm=TRUE)
q.female = quantile(dat.female$bp.1s, 
                    probs=seq(0,1,by=0.05),
                    na.rm=TRUE)

# Now plot against each other!
ggplot() +
  geom_point(aes(x = q.male, y = q.female)) +
  labs(title = "Quantiles", x = "Male quantiles", y = "Female quantiles")
```

------------------------------------------------------------------------

### Correlation  

#### Measuring the centrality in data

Before you begin, think back to the mean, median and quantiles we saw on the boxplot. Do you remember what these terms mean? How does an asymmetrical distribution influence mean and median? 
We have already seen that the `summary()` and `quantile()` functions in R can compute the mean, median and quantiles of any given data. 

```{r, eval=F, results='hide'}
mean(dat$stab.glu) 
median(dat$stab.glu) 
quantile(dat$stab.glu) 
```
> Calculate the mean and median of other continuous numeric data in the diabetes dataset and measure the difference between them. (a) Why is there a difference between the mean and median? (b) Why do you think there are larger differences for some and almost no difference for others?  


#### Association between variables

Often a common step during any data analysis project is to find associations between variables present in the dataset. Such associations helps us to decipher the underlying structure in the data.  

For instance, in our diabetes dataset we would expect a high correlation between free blood glucose levels and glycosylated blood levels or between waist and hip sizes. One of the most common ways of measuring associations is *correlations*. 

Let us start by producing a **scatter plot** between a pair of variables:

```{r}
ggplot(dat,
       aes(x = stab.glu, y = glyhb)) +
  geom_point() +
  labs(x='Stabilized glucose', y='Glycosylated hemoglobin')
```

> Do you suspect that the two variables have a relationship? Do the scatter plot for other pairs of numerical variables!

We now can compute the correlation of the two variables. We can compute the **Pearson correlation** or the **Spearman correlation**:

```{r}
## compute the Pearson correlation
cor(dat$stab.glu, dat$glyhb, method='pearson')

## compute the Spearman correlation
cor(dat$stab.glu, dat$glyhb, method='spearman')
```

The Spearman correlation seems much lower, right? To understand why, we can do a scatter plot between the **ranks** of the two variables:

```{r}
ggplot(dat,
       aes(x = rank(stab.glu), y = rank(glyhb))) +
  geom_point() +
  labs(x='Rank - Stabilized glucose', y='Rank - Glycosylated hemoglobin')
```

Do you understand the usage of ranks here? Run `rank()`` on a vector like *c(3,5,10,1,23)* to see how the output looks like.

> Associations are among the simplest forms of structure in the data! It is important to remember that *Association does not imply correlation* and *Correlation does not imply causation*. Take a look at this page to view few common logical fallacies. [see here](https://en.wikipedia.org/wiki/Fallacy)

------------------------------------------------------------------------

### EXERCISES

#### Exercise 1: Data features

1.  Try to obtain the same result of the `head()` function by using slicing on the dataset "dat" using row indexes.

2.  Print out the last element in the last column of "dat" using the `dim()` function instead of using numerals.

#### Exercise 2: Visualization and correlation

1.  Visualize the cholesterol levels of all patients with a histogram using the `geom_histogram()` function.

2.  Visualize the cholesterol levels of all **male** patients with a histogram using `geom_histogram()`.
    *(expert):* Mark the median, first and third quartile with vertical lines using `geom_vline()`. The values are defined using `xintercept` (as in which value of x should the vertical line intercept).
    Then mark median, first and third quantile for **female** patients in the same graph with a different color.
    What can you tell from the differences in these values?

3.  Is there an association between "hip" and "waist" on the data frame "dat"? Use the `geom_point()` function to do a scatter plot of the **values** and of the **ranks** (as determined by the `rank()` function). Compute both the **Pearson** and **Spearman** correlation values.
    
#### Going further *(expert)*

1.  Select only the numerical columns (using `select(where(is.numeric))`), and apply the function `cor()` **directly**: `%>% cor()` . What happens? What is the output? Store the result of this command in a new variable named "all.cor". Plot a heatmap of these results using the `pheatmap()` function from the "pheatmap" package. Remember that you first have to install and then activate this package using `library("pheatmap")`.

2.  Find the **highest** and **lowest** Pearson correlation value of the result from exercise 2.2. To which pair of variables do they correspond? Plot the corresponding scatter plots using `geom_point()`! *Hint*: Before finding the highest Pearson correlation value, use the `diag()` function to set all diagonal values (=1) of "all.cor" to NA.


## Day 2: Dimensionality reduction and unsupervised learning

### Preparing the data

**Unsupervised clustering** is one of the most basic data analysis techniques. It allows to identify groups (or clusters) or observations (here: patients) or variables. *Unsupervised* means that you are not using any prior knowledge about groups of variables or associations. **K-means clustering** is a good example of unsupervised learning because the method categorizes sample based uniquely on the data.

In this part, we will use a dataset of gene expression data from the TCGA (The Cancer Genome Atlas) project. This project has sequenced several thousand samples from cancer patients of more than 30 cancer types. We will use a subset of this data, containing 200 samples (=patients, as columns) , for which the expression of 300 genes (= rows) has been measured.

#### Load data

We will start by reading the gene expression data. The columns are the samples and the rows are the genes. This is matrix, which allows some numerical operations to be conducted directly. 

```{r}
brca.exp = readRDS(url('https://www.dropbox.com/s/qububmfvtv443mq/brca.exp.rds?dl=1'))
dim(brca.exp)
```

**WARNING**: If you have problem loading the data, please download [this file](https://www.dropbox.com/s/qububmfvtv443mq/brca.exp.rds?dl=1), store it on your disk, and open it with the following command:

```{r}
#brca.exp = readRDS("xxxx") # xxxx should be replaced with the path to the downloaded file in your device
```

Next we will load the clinical annotation file for this gene expression data and explore it

```{r}
brca.anno = readRDS(url('https://www.dropbox.com/s/9xlivejqkj77llc/brca.anno.rds?dl=1'))
head(brca.anno)
```

Same here: if you have issues running the previous `readRDS` command, download [this file](https://www.dropbox.com/s/z6bzwzgzdhky1qz/brca.anno.rds?dl=1), save it on your disk and load it with

```{r}
### brca.anno = readRDS(xxx)
```

You can check the number of samples for each tumor type using the `table()` function, applied to a specific column (here, there is only one column...)

```{r}
table(brca.anno$HER2_status)
```

#### Data transformation

You will see that the distribution of the data is extremely squeezed due to **outliers** with very high or low values. We will need to make the data more homogeneous, so that our downstream analysis is not affected by these very large magnitude numbers.

We will carry out the following data processing steps. Some of these steps use rather arbitrary values, which come from visually inspecting the data!

1.  **Thresholding:** cap the values to the 95th percentile

2.  **Homogenization:** base-2 logarithmic transformation of the entire dataset

3.  **Scaling:** standardize the data so that across genes the mean = 0 and variance = 1.

**Before we start modifying the data, we will store the original data frame into a variable, so that in case of problems we can revert back to the initial data!!**

```{r}
brca.exp.original = brca.exp # keeps the original as matrix
```

**Thresholding**

```{r}
## what is the value of the 95th percent percentile?
q95 = quantile(brca.exp,probs=0.95)

## set all values above to this value
brca.exp[brca.exp>q95] = q95
```

**Homogenization and Scaling**

We will perform this step by log-transforming the data. We are able to use this operation because the data is still in a matrix.
```{r}
brca.exp = log2(brca.exp+1)
```

> Why do we add +1 ?

Next, we will scale the data and plot its distribution. To do this efficient, we need to convert the data to tibble first, make it tidy, and then plot. 

Conversion to tibble can be done using `as_tibble(brca.exp, rownames = NA)`, where `rownames = NA` is meant to keep the original rownames (in this case, gene names) in the new tibble, although they are invisible. 

Check this out:
```{r}
rownames(as_tibble(brca.exp, rownames = NA))[1:5]
```

In addition, `gather(key = "sample", value = "expression")` converts the tibble to a long format, where "sample" represents the original column names, and "expression" represents the values present in the initial matrix.

```{r, message=F}
## scaling
brca.exp = scale(brca.exp)

## plotting the density
as_tibble(brca.exp, rownames = NA) %>%
  gather(key = "sample", 
         value = "expression") %>%
  ggplot(aes(x = expression)) +
  geom_histogram() +
  labs(title = "Transformed data")
```

Compare to the density plot before these pre-processing steps using the same strategy.

```{r, message=F}
as_tibble(brca.exp.original, rownames = NA) %>%
  gather(key = "sample", value = "expression") %>%
  ggplot(aes(x = expression)) +
  geom_histogram() +
  labs(title = "Untransformed data")
```

------------------------------------------------------------------------

### k-means clustering

Another widely used method for grouping observations is the k-means clustering. Now we will cluster our dataset using k-means and explore the underlying structure of the data. In this dataset, different clusters could represent different batches, different tumour subtypes, among other features.

#### Performing k-means

We use the function `kmeans()` in R on our matrix. You can check the options and usage in the help panel on the right. The parameter `centers` indicates how many clusters are requested.

```{r}
km = kmeans(x=t(brca.exp), 
            centers = 2, 
            nstart = 10)
```

> Just type `km` in your console and check all results generated. Play around with the `centers` parameter. See cluster assignments by typing `table(km$cluster)`

#### Quality of the clustering

We can judge the quality of the clustering by computing the **intra**-cluster distances, i.e. the sum (squared) of all distances between pairs of objects belonging to the same cluster. This is called the **within sum of squares (WSS)**. The better the clustering, the smaller WSS should be. However, it also automatically decreases with increasing k.

> What would be WSS if we request a number of clusters equal to the number of data points? You can check what the WSS is for a particular clustering by typing

```{r}
km$tot.withinss
```

> run k-means for k=2 to k=7 clusters, and for each k check the WSS value. How does WSS evolve with increasing k?

We can also run a little loop to test different k. Loops are very important structures in any programming language. 
We can test a simple scenario before. Check how the output of this simple loop looks like.

```{r}
k_to_test = c(2:7)

for (i in 1:length(k_to_test)) {
  print(i) # We can print the indexes
}

for (i in 1:length(k_to_test)) {
  print(k_to_test[i]) # or the actual elements
}
```

> Do you understand the difference between the 2 previous for loops? Try to make your own for loop. 

Now we can make one to test k from 2 to 7:
```{r}
km_wws = numeric() # we start by creating an empty vector

# To write in the position 1, we use i
# to find the 1st element, we use k_to_test[i]
for (i in 1:length(k_to_test)) {
  km_wws[i] = kmeans(x=t(brca.exp), 
                     centers = k_to_test[i])$tot.withinss
}

# We can plot the k against WSS using geom_line
ggplot() +
  geom_line(aes(x = k_to_test, y = km_wws)) +
  labs(x="Number of clusters K",
       y="Total within-clusters sum of squares")
```

> Do you see an obvious "elbow" or "kink" in the curve?? Another criteria for the quality of the clustering is the **silhouette** method.

To run the silhouette method, we need to compute the pairwise distances between all objects (i.e. patients) in the data matrix. This is done with the `dist` function, which can take different metrics (euclidean, ...)

```{r}
## compute the patient-patient distance matrix (this is why we transpose using the `t()` function)
D = dist(t(brca.exp))
```

We now compute the silhouette for a specific k-means clustering:

```{r, warning=FALSE}
library(cluster)
km = kmeans(x=t(brca.exp), centers = 3, nstart = 10)
s = silhouette(km$cluster,D)

# Let us use the basic R function plot() to see the results
plot(s)
```

------------------------------------------------------------------------

### Hierarchical clustering

Clustering is a method by which we group together similar observations while separating out the dissimilar ones. We will cluster our samples from the cancer dataset to see which samples cluster together or separately. Hierarchical clustering does not generate discrete clusters of datapoints, but rather creates a dendrogram that indicates the magnitude of similitude between samples. Once again is up to the Data Scientist to decide the right amount of clusters.

#### Determine the most variable genes

When performing clustering, we usually reduce the number of genes used, as some of them are not informative. For example, genes that show a mostly constant expression across all samples will not be useful to distinguish the samples, right? One simple method is to select genes that show a **high variance** across all samples.

```{r}
brca.exp.tibble = as_tibble(brca.exp, rownames=NA) %>%
  rownames_to_column("gene")

## create a new column with the variance for all genes across all samples
brca.exp.var = brca.exp.tibble %>%
  rowwise() %>%
  mutate(variance = var(c_across(starts_with("TCGA")))) 
# only includes the columns starting with TCGA
```

We now want to find the top 25% with the highest variance

```{r}
## what is the 75% quantile of the variance?
q75 = quantile(brca.exp.var$variance, probs = 0.75)
q75
```

So let us select all rows (genes) with a variance higher than or equal to `q75`:

```{r}
## only select the genes with a variance in the top 25%
topVariantGenes <- brca.exp.var %>%
  filter(variance >= q75)

print(topVariantGenes$gene)
```

#### Computing the correlation between all patients

Let us start by filtering for only the highly variable genes. Then we can directly calculate Spearman correlation.
```{r}
brca.exp.highvar.cor = brca.exp.tibble %>%
  filter(gene %in% topVariantGenes$gene) %>%        # from the whole list, select only high variable
  select(where(is.numeric)) %>%                     # get only numerical columns
  cor(method="spearman")                            # create correlation-based distance matrix
```

If we want to display the correlation matrix as a heatmap, we can use the `pheatmap` function as before:

```{r}
library(pheatmap)
pheatmap(brca.exp.highvar.cor, 
         show_rownames = FALSE, 
         show_colnames = FALSE)
```

Each cell of this heatmap represents the correlation value between the sample in the row and the sample in the column. The correlation of a sample to itself is always 1 (red diagonal).

The function automatically determines the clustering trees of the rows and columns (which are identical, since the correlation matrix is symmetrical!)

#### Including clinical annotations in the heatmap

This is a nice representation, but in order to interpret this clustering, we need to add some additional (clinical) information to interpret the clustering structure. To do this, we use an annotation data frame containing as columns a number of clinical features.

The clinical annotation is stored in the `brca.anno` data frame.
We can now plot again the heatmap, using the annotation dataframe to add additional information

```{r, fig.width=8, fig.height=7}
pheatmap(brca.exp.highvar.cor,
         annotation_row = brca.anno,
         show_rownames = FALSE, 
         show_colnames = FALSE)
```

> How would you interpret this dendrogram? Do the clusters you observe make any sense? What are the parameters by which the samples cluster together? How many meaningful clusters can you observe? Do you see any relation between the distribution of the data and your clusters ?

> The function `pheatmap` accepts a parameter clustering_method to indicate alternative linkage methods; try other linkage methods (check which are available with the pheatmap help page, which can be accessed by typing `?pheatmap` in the console!)


------------------------------------------------------------------------

### Principal component analysis

We will now use principal component analysis to explore the same dataset, and identify directions (i.e. principal components) with maximal variance. Principal components analysis finds n-dimensional vectors (Principal Components) in the direction of the largest variance, thereby allowing you to describe an n-dimensional dataset with just a few dimensions.

```{r}
pca = topVariantGenes %>%
  select(where(is.numeric)) %>%
  t() %>% # do not forget to transpose the data!
  prcomp(center = FALSE, scale = FALSE) # We set these as false as we have already scaled our data
summary(pca)
```

> How many principal components do you obtain? Compare this to the dimension of the matrix using the `dim()` function!

> What would happen if you would not transpose the matrix with t(...) in the prcomp function?

Principal components are ranked by the amount of variance that they explain. This can be visualized using a **scree plot**, indicating how much variance each PC explains: the **standard deviation** explained by each principal component is contained in the `pca$sdev` vector:

```{r}
pca$sdev
```

We see that the standard deviation is indeed going down! Let us now plot the proportion of total variance explained by each PC

```{r}
variance = (pca$sdev)^2
prop.variance = variance/sum(variance)
names(prop.variance) = 1:length(prop.variance)

# We make a data.frame from the prop.variance and the PC it corresponds to 
# we can obtain the PCs using names()
data.frame(proportion = prop.variance, 
           PCs = as.numeric(names(prop.variance))) %>%
  ggplot(aes(x = PCs, y = proportion)) +
  geom_col() +                               # to make the barplot
  labs(y='Proportion of variance')           # we only plot the first 20 PCs
```

Principal component analysis represents each data point (here: patient) in a new space in which the coordinates are principal components. Check the following output:

```{r, results='hide'}
head(pca$x)
```

We can now display the data points (i.e. patients) in the first two principal components. In addition, we can color the dots according to certain clinical parameters:

```{r, fig.width=6, fig.height=4}
# We start by creating a dataframe and combining it with the annotation
pca_with_annot = as.data.frame(pca$x) %>%
  merge(brca.anno, by = 0) # by = 0 makes use of the rownames as common information

## Now the object is in a ggplot2 friendly format
ggplot(pca_with_annot,
       aes(x = PC1, y = PC2, colour = ER_status)) +
  geom_point() +
  scale_colour_manual(values = c("grey", "red", "navy")) # scale_colour_manual can be used to change colours
```

> Choose different PCs for this plot. Can you still observe the two clusters corresponding to the ER_status of the patients? 

------------------------------------------------------------------------

### EXERCISES

#### Exercise 1: Variance

*For this exercise set it is given that all the data cleanup steps have been taken, you don't need to put them in the results.*

1.  Make a heatmap of the reduced matrix "topVariantGenes" using the `pheatmap()` function of the `pheatmap` library (do not forget to select only for numerical columns). Check for parameters that might change the style of the heatmap (column names, row names, etc..). How is this heatmap different from the heatmap in section 2?

2.  Repeat the selection of top variable genes (apply the same quantile used to generate "topVariantGenes"), but using the median absolute deviation (or MAD) using the `mad()` function instead of the `sd()` function, and store into as `brca.topMAD`

3.  Extract the gene names of `topVariantGenes` and `brca.topMAD` and check how many overlap using the `intersect()` function.

#### Exercise 2: Hierarchical clustering

In section 2, we have computed a correlation matrix, and used this matrix to build a clustering tree.

Try different linkage methods using the `clustering_method` parameter to see if the topology of the dendrogram changes!

2.  Try building a distance matrix which would lead to different topologies of the dendrogram, depending on which linkage method is used! Show the dendrograms built with different linkage methods!

#### Exercise 3: PCA

1.  Display the patients in the first two principal components (available in `pca_with_annot`) using `geom_point()`. Color the patients in the PCA plot according to `HER2_status`.

2.  (optional) Color the patients in the PCA plot according to `Classification`; you will probably need to define some more colors... You can check available colors [here](http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf)

#### Going further *(expert)*

Instead of performing the k-means on the whole gene expression matrix, we can run k-means on the space in which each patient is represented by the first k principal components.

1.  Run k-means with different numbers of clusters (1-10) on the patients using the first 2, 4, 6,... principal components (i.e. the first columns of `pca_with_annot`). Use the elbow method to evaluate how the within sum of squares (WSS) evolves. What is the optimal number of clusters? 

2.  Represent the patients in the PCA plot as previously, but color them according to the cluster they belong to! Run kmeans with two clusters for this and merge the k-means results.


## Day 3: Probability distributions

### Probability distributions

In the previous Exercise Sheet we have learnt more about unsupervised learning, like hierarchical clustering and especially PCA. These are among the fundamental methods of Data Analysis. Today we will learn more about another milestone of statistics: **probability distributions**.

![Various kinds of distributions and the relations among them.](figures/distrib.png)

[Figure source](https://medium.com/@srowen/common-probability-distributions-347e6b945ce4)

Distributions represented by sparse lines represent outcomes which will be discrete (for example the _roll of dice_ will always have discrete integer values from 1 to 6). Distributions represented by dense lines represent outcomes which can be continuous i.e real numbers (for example the height of people living in Heidelberg).

R has in-built functions for almost all probability distributions

```{r, eval=FALSE}
# Get the list of all probability distribution related functions
help(distributions)
```

All of these functions for probability distributions follow the same common scheme, the **root name** of the function prefixed by either of **p**, **d**, **q** and **r**. For example for the Normal distribution we have the four variants of function available - **pnorm**, **dnorm**, **qnorm** and **rnorm**.

Here, you can find some specific help on these functions:

```{r, eval=FALSE}
?rnorm
# or
?rpois
# etc ...
```

In the respective help documentation, you will find details on each of the functions.\
Probably, the most difficult to distinguish or to remember are **pnorm()** and **qnorm()** (respectively **ppois()** and **qpois()**, etc ...).\
We are going to look at them more deeply in the following.

------------------------------------------------------------------------

#### Getting to know the various functions

Let us get a grasp of what these functions actually do. You should be familiar with the cumulative distribution function, let's take a look at that and its inverse first. We will work with a Normal distribution. We calculate the **cumulative probability** for the values 1,2,3,4 in three different distributions, using one of the functions described previously.  
Short hint: **p** like **cumulative P-robability**. Which function are you going to use?

```{r echo=TRUE}
# Distribution 1: Mean = 2, Standard Deviation = 1
pnorm(1:4,mean=2,sd=1)

# Distribution 2: Mean = 2, Standard Deviation = 2
pnorm(1:4,mean=2,sd=2)

# Distribution 2: Mean = 4, Standard Deviation = 1
pnorm(1:4,mean=4,sd=1)
```

> Do you understand why the cumulative distribution functions change the way they do? 

Now, on the same distributions, we calculate the **inverse cdf** (inverse cumulative distribution function) for the cumulative probabilities of 25%, 50% and 75%. We use the `qnorm()` function for this (**q- like quantile**): 

```{r}
# Distribution 1: Mean = 2, Standard Deviation = 1
qnorm(c(0.25,0.5,0.75),mean=2,sd=1)

# Distribution 2: Mean = 2, Standard Deviation = 2
qnorm(c(0.25,0.5,0.75),mean=2,sd=2)

# Distribution 3: Mean = 4, Standard Deviation = 1
qnorm(c(0.25,0.5,0.75),mean=4,sd=1)
```

> Try with 100% on any of the distributions. Can you explain this result? Do you expect the result to be different in the other ones?


Now that you know the output of the p- and q- functions, let's look at **d- like density probability** functions. For any continuous distribution, the value of the function at any specific value is 0. This is why this probability function is used in discrete distribution such as the binomial distribution (function **dbinom()**). We first calculate the probability of getting 5 events out of 5 in a binomial distribution with a probability of 0.5. Then, we calculate the odds of **not** getting 5 out of 5.

```{r}
# probability of 5 out of 5
dbinom(5, size=5, prob=0.5) # size = number of trials; prob = probability of success on each trial

# probability of NOT getting 5 out of 5
1-dbinom(5, size=5, prob=0.5) 
# or
pbinom(4, size=5, prob=0.5) # Remember, pbinom() returns the "cumulative probability" of ...
```

> What is the probability of getting 5 out of 10? And NOT getting 5 out of 10? 


Suppose that the distribution of body height is described by a **normal distribution** with **mean = 1.75 m** and standard deviation **sd = 0.15**. What is the probability that someone is taller than 1.9 m? We can use the parameter **lower.tail** of the function `pnorm()` for this.  
What is the probability that someone is smaller than 1.60m?

```{r}
## taller than 1.9
pnorm(1.9, mean=1.75, sd=0.15, lower.tail=FALSE)
# or
1-pnorm(1.9, mean=1.75, sd=0.15)

## smaller than 1.6
pnorm(1.6, mean=1.75, sd=0.15, lower.tail=TRUE)
# equivalent to 
pnorm(1.6, mean=1.75, sd=0.15) # lower.tail is set to TRUE by default (check the help page using `?pnorm`)
```

Let's have a look at this distribution using the **dnorm()** function:

```{r}
x = seq(1, 2.5, by=0.01)
y = dnorm(x, mean=1.75, sd=0.15)

ggplot() +
  geom_line(aes(x = x, y = y),
            colour = "red", linewidth = 0.8) +
  geom_vline(xintercept = c(1.6, 1.9),
             linetype = 'dashed')
```

Let's finally look at **r- like random** functions. These, unlike the others, don't return single probabilities or values but rather generate a random distribution of values. We can use this to generate a normal distribution with mean = 10, sd = 5.

```{r fig.width=6, fig.height=4}
## normal distribution
x = rnorm(n=1000,mean=10,sd=5)

ggplot() +
  geom_histogram(aes(x=x), bins = 20,
                 fill = "orange") +
  labs(y = "Frequency") 
```

> Can you generate a Poisson distribution and a binomial distribution?

------------------------------------------------------------------------

#### Normal/Gaussian distribution

The normal or the Gaussian distribution is given as:

$$P(x) = \frac{1}{{\sigma \sqrt {2\pi } }} \cdot e ^ \frac{-(x- \mu)^2}{{2\sigma ^2 }} $$ where $\mu$ is the mean of the distribution and $\sigma$ is the standard deviation.

The **standard normal distribution** is a special case of **normal distribution** where the values for $\mu = 0$ and $\sigma = 1$. Thus, the above equation for the Normal distribution simplifies to:

$$P(x) = \frac{1}{{\sqrt {2\pi } }} \cdot e ^ \frac{-x^2}{2} $$ Now for any $x$ we can easily solve this equation since $\pi$ and $e$ are known constants.

------------------------------------------------------------------------

#### Visualization

Let's generate three random normal distributions with different means and standard deviations and visualize them together

```{r, message=F}
## Use the function `dnorm()` to plot the density distribution
x = seq(-10, 30, by=.1)
d1 = dnorm(x, mean=0, sd=1)
d2 = dnorm(x, mean=10, sd=1)
d3 = dnorm(x, mean=20, sd=1)

# Compare with the histogram build from 1000 random number drawn from the standard normal distribution
r1 = rnorm(n=1000, mean=0, sd =1)  # random distributions of values
r2 = rnorm(n=1000, mean=10, sd=1)
r3 = rnorm(n=1000, mean=20, sd=1)

# Histogram visualization
# We will start by converting the rnorm outputs into one single df
data.frame(vals = c(r1, r2, r3), # column vals is actual numbers
           Specs = c(rep("mean=0, sd=1", 1000), # column Specs is mean/sd 
                     rep("mean=10, sd=1", 1000),
                     rep("mean=20, sd=1", 1000))
             ) %>%
  ggplot(aes(x = vals, 
             colour = Specs)) +
  geom_histogram(bins = 50) + 
  geom_freqpoly(bins = 50) # geom_freqpoly adds in the line on top of the histogram
```

> Play with the mean and sd parameters and visualize the distributions (plain lines) as well as the corresponding histograms. 

------------------------------------------------------------------------

#### Application on a real dataset

Now we will use the Normal distribution to make predictions about gene expression of TP53 in lung cancer. TP53 is the most commonly mutated gene across multiple cancer types especially in lung cancers. We will read a table (import) containing measurements of TP53 expression levels in 586 patients.

```{r}
tp53.exp = read.table("https://www.dropbox.com/s/rwopdr8ycmdg8bd/TP53_expression_LungAdeno.txt?dl=1", 
                      header=T, sep="\t")[,1:2]
summary(tp53.exp)
```

##### Data cleaning and central values

We will remove all the missing values and calculate the mean and standard deviation for the TP53 gene expression.

```{r}
tp53.exp = tp53.exp %>%
  na.omit() 

m.tp53 = mean(tp53.exp$TP53_expression) # mean
m.tp53

s.tp53 = sd(tp53.exp$TP53_expression) # standard deviation
s.tp53
```

##### Modeling using a normal distribution

Let's see how well a normal distribution with $\mu = 1380.822$ (m.tp53) and $\sigma = 719.5934$ (s.tp53) can approximate the **real** distribution of TP53 expression.  
We assume that the population mean and standard deviation is similar as calculated above since we cannot measure the expression of TP53 in each and every lung cancer patient in the world.

```{r}
# distribution of the measured data
ggplot(tp53.exp,
       aes(x = TP53_expression)) +
  geom_density() + 
  xlim(-1500, 6000)
```

Make a normal distribution with the above parameters
```{r}
x = seq(0,5000,by=5)
d.pred = dnorm(x, mean = m.tp53, sd = s.tp53)

# Now plot both, predicted and measured data
ggplot() +
  geom_density(aes(x = tp53.exp$TP53_expression)) +
  geom_line(aes(x = x, 
                y = d.pred),
                colour = "red") +
  geom_vline(xintercept = c(quantile(tp53.exp$TP53_expression, probs = c(0.1, 0.9)))) +
  geom_vline(xintercept = c(qnorm(p = 0.1, mean = m.tp53, sd = s.tp53), 
                            qnorm(p = 0.9, mean = m.tp53, sd = s.tp53)),
             colour = 'red', linetype = 'dotted') 
```

##### Data prediction using the normal distribution model

Using a normal distribution with $\mu = 1380.822$ (m.tp53) and $\sigma = 719.5934$ (s.tp53), we will ask the following questions -

-- **(Q1)** What is the probability of observing the expression of TP53 to be **less** than 1000?
```{r}
pnorm(q=1000, mean =m.tp53, sd = s.tp53) # returns the cumulative probability
```

-- **(Q2)** What is the probability of observing the expression of TP53 to be **greater** than 1000?

```{r}
1 - pnorm(q=1000, mean =m.tp53, sd = s.tp53) 
# is same as
pnorm(q=1000, mean =m.tp53, sd = s.tp53, lower.tail = FALSE) # or pnorm(q=1000, mean =m.tp53, sd = s.tp53, lower.tail = F)
```

##### Evaluating the quality of the predictions

Let's check how good these predictions are compared to **real** data.
-- **(Q1)**  What is the probability of observing the expression of TP53 to be **less** than 1000?

```{r}
sum(tp53.exp$TP53_expression < 1000)/nrow(tp53.exp)
```

-- **(Q2)** What is the probability of observing the expression of TP53 to be **greater** than 1000? 
```{r}
sum(tp53.exp$TP53_expression > 1000)/nrow(tp53.exp)
```

> I would say those predictions are pretty good !! Now, let's try to break this model. Re-execute the code above with different $q$ values `q=100, q=500, q=4000, q=4500` etc. At what values do you think the model would not perform well. HINT: Look at the tails of the distribution!

```{r}
# What is the probability of observing the expression of TP53 to be less than q?
q = c(100,500,1000,4000,4500)

# for loop used to calculate and store the predicted (a) and real (b) values
pred = numeric()
meas = numeric()
for (i in 1:length(q)){
  pred[i] = pnorm(q=q[i], mean = m.tp53, sd = s.tp53) 
  meas[i] = sum(tp53.exp$TP53_expression < q[i])/nrow(tp53.exp)
}

# Change into a dataframe 
model_mat = data.frame(pred, meas, q)
model_mat
```

Again, using a normal distribution with $\mu = 1380.822$ and $\sigma = 719.5934$, what if we ask what is the value of TP53 expression at the 10% and 90% quantiles:
```{r}
qnorm(p = 0.1, mean = m.tp53, sd = s.tp53)
qnorm(p = 0.9, mean = m.tp53, sd = s.tp53)
```

Let's check how good these predictions are compared to our real data.

```{r}
quantile(tp53.exp$TP53_expression, 
         probs = c(0.1, 0.9))
```

Again the predictions are pretty good!


##### Visualization

We can also visualize all of this on a simple graph:

```{r}
# Model prediction:
x = seq(0,5000,by=5)
d.pred = dnorm(x,mean = m.tp53, sd = s.tp53)

# Model and measured data and predicted versus measured 0.1 and 0.9 quantiles:
ggplot() +
  geom_line(aes(x = x, 
                y = d.pred),
            colour = "red") +
  geom_density(aes(x = tp53.exp$TP53_expression)) 

```

Compare the black and red vertical lines (real vs predicted).

> Re-execute the code above with `p=0.25, p=0.5, p=0.75 etc` and check how good the predictions are.

##### Visualization using a Q-Q plot

Now, let's plot the sample quantiles against theoretical quantiles to check the similarity between the two. This is called a quantile - quantile plot or a Q-Q plot, which you are familiar with (see Exercises Sheet 1).

```{r fig.height=5, fig.width=5, warning=FALSE}
q = seq(0,1,0.01) # Creating a vector of quantiles

# Find values corresponding to these quantiles in the real data
q.observed = quantile(tp53.exp$TP53_expression, probs = q)

# Find values corresponding to these quantiles in the theoretical normal distribution
q.theoretical = qnorm(p = q, mean = m.tp53, sd = s.tp53)

# # Correlate the above two values
ggplot(tp53.exp,
       aes(sample = TP53_expression)) +
  geom_qq() +
  geom_qq_line(colour = 'red')

## Would be the same as:
# ggplot() +
#   geom_point(aes(x = q.theoretical, 
#                  y = q.observed), size = 1) +
#   geom_abline(intercept = 0, slope = 1, 
#               size = 1, color = "red") 

```

------------------------------------------------------------------------

#### Binomial distribution

A binomial distribution can be defined as -

$$P(x) = \frac{n!}{x!(n-x)!}\cdot p^x \cdot (1-p)^{n-x}$$ 

Where $x$ is the number of successes out of $n$ experiments and $p$ is the probability of success.

-   $mean = n \cdot p$
-   $variance = np \cdot (1 - p)$
-   $sd = \sqrt{np \cdot (1 - p)}$

The design of the experiment is as follows -

-   The experiment is repeated and are independent of one another
-   Each experiment has just two outcomes
-   The probability of success is constant and does not change with experiments

We can for example compute the probability of having 7 heads in a series of 10 throws of a coin:

```{r}
# x = number of successes; size = number of trials; prob = probability of success on each trial
dbinom(x=7, size=10, prob = 0.5) 
```

Or we can compute what the probability is to get 7 or more heads using the function `pbinom()`. Remember that the parameter "lower.tail" is used to specify whether to calculate the probability of observing x or fewer successes (if `lower.tail = TRUE`) or the probability of observing more than x successes (`lower.tail = FALSE`):

```{r}
pbinom(6, size=10,
       prob=0.5,
       lower.tail=FALSE) 
```

Beware that this syntax means **strictly more than 6**, i.e. 7 or more!!

> How would you compute the probability to get less than 5? What would `qbinom(0.3,size=10,prob=0.5,lower.tail=FALSE)` represent?


------------------------------------------------------------------------

### Confidence interval

The **confidence interval** describes the interval containing the (unknown) expectation value of a distribution with 95% confidence. This means that out of 100 random realizations of this random variable, the true expectation value $\mu$ will indeed be in this interval.

Let us try a simulation: we consider a random variable distributed according to a Poisson distribution $$P(x) = \frac{{e^{ - \lambda } \lambda ^x }}{{x!}}$$ Here, *we know the true value of the expectation value*. We want to get an estimate for $\lambda$, and check if the confidence interval contains the true expectation value.

For example, a farmer expects to collect 75 eggs from his hens per hour.

```{r}
lambda = 75
```

He now collects during 100 days the eggs $N=8$ times a day (each time during one hour). We want to compute the mean $m_N$ over these $N=8$ realizations and determine the 95% confidence interval, and check, how often the expectation value $\mu$ is inside the confidence interval.

Remember that the 95% CI is given by $$[m_N-t_{95,N-1}\frac{\sigma}{\sqrt{N}},m_N+t_{95,N-1}\frac{\sigma}{\sqrt{N}}]$$ where $t_{95,N-1}$ is the critical value for the $t$-distribution with $n-1$ degrees of freedom.

Let's start by creating our samples:

```{r}
# size of the sample
N = 8

# we now draw 100 times samples of size N=8
## rpois is the function used to generate the Poisson distribution
X = lapply(1:100, function(i) {
  rpois(N, lambda = lambda)
  })
```

`lapply()` is a function in R that stands for "list apply". It is used to *apply* a function to each element of a list or vector and produces a list with the same length as an output.  

In this previous example, the input is a vector (`1:100`). 
Then, we `lapply(v, function(i))`:
+  v: The list or vector you want to apply the function to.
+  function: The function you want to apply where `i` is each element in X. In this case, we are applying `rpois` 100 times.

As an output, `lapply` returns a list where each element has had the specified function applied to it. 

> Run View(X) to see how this object looks like. Try using lapply to obtain the mean of all elements in each X using `lapply(X, function(i){mean(i)})` or `lapply(X, mean)`. 

Now, we calculate the mean and the standard deviation of the respective samples:

```{r}
# we compute the sample means
Xm = sapply(X,mean)
# and the sample standard deviations
Xsd = sapply(X,sd) 
```

Next, we determine the upper and lower bounds of the 95% CI. Remember that the confidence interval is based on a $t$-distribution. The degrees of freedom of this distribution is the sample size -1 ($N$-1=7 in this case)

```{r}
df = N-1
tc = qt(c(0.975),df) # this is the critical value for the t-distribution for df = N-1 degrees of freedom and 95% CI

Xl = Xm-tc*Xsd/sqrt(N) # upper bound of the 95% CI
Xh = Xm+tc*Xsd/sqrt(N) # lower bound of the 95% CI
```

Finally, we determine whether each sample mean is found within the 95% CI or not:

```{r, fig.width=8, fig.height=4}
## vector of TRUE/FALSE if the real expectation value lambda is inside the interval
i.ok =  as.factor(Xl < lambda & Xh > lambda)

plot_data <- data.frame(
  n = 1:100,
  Xm = Xm,
  Xl = Xl,
  Xh = Xh,
  i.ok = i.ok
) 

# Plot using ggplot2
ggplot(plot_data, 
       aes(x = n, y = Xm, 
           color = `i.ok`)) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = Xl, ymax = Xh, color = i.ok), width = 0.2) +
  geom_hline(yintercept = lambda, linetype = "dashed", color = "black") +
  scale_color_manual(values = c("grey", "orange")) +
  labs(y = "Mean values",
       title = paste("Mean values and confidence intervals, N=",N),
       color = "95% CI Status") 
```


Here, the orange/grey bars represent the confidence interval, the dot is the mean of the sample values, and the dotted line at `\lambda` represents the true expectation value. Whenever the true expectation value is within the CI, the bar is orange, if not, the bar is grey. How often is the true expectation value outside the CI? Count the grey bars!

It happens `r sum(!as.logical(i.ok))` times, which fits pretty well with the expected 5%.

> Repeat this simulation, but now with samples of $N=24$ (again 100 times)\
> What do you observe?\
> How often is the true expectation value outside the CI? Change to 90% CI and check if that works!


------------------------------------------------------------------------

### EXERCISES 

#### Exercise 1: Probability distributions

1.  What is the expression of TP53 observed at 10th percentile? What is the expression of TP53 observed at 90th percentile?

2.  (optional) Other distribution types can become very similar to the normal distribution under certain conditions. Plot the histogram of 1000 random numbers drawn from the Poisson distribution with lambda = 1, 10, 100, 1000. What do you observe?

#### Exercise 2: Confidence intervals

You are buying 10 packs of gummy bears. You particularly like the red ones and the green ones. A pack contains 6 different colors and you expect them to be equally distributed. There are 84 pieces per 200g pack.

1.  What is the expected amount of red or green gummy bears?

2.  You selected your 10 packs according to the colors you could see in the pack. At home, you counted the following bears per pack:

-   for the red ones: 12 16 17 12 16 13 11 18 13 19\
-   for the green ones: 11 10 15 16 12 14 13 10 13 17\
    Was your selection procedure a success? In other words, is the expected value bellow (congrats!), within or above (bad luck!) the 95% CI?


## Day 4: Hypothesis testing

On this section, we will go through hypothesis testing. You will start to see how to *formulate hypotheses* and how to *test* them.
In addition, we want to learn how to use and interpret hypothesis tests.

We will work again with the diabetes dataset that we used previously.

```{r}
dat = read.delim('https://tinyurl.com/y4fark9g')

# set the row names using the column id
rownames(dat) = dat$id
```

Load the required packages
```{r, message=F, warning=F}
library(dplyr)
library(ggplot2)
library(tibble)
```

Check out the content of the dataset using the summary function
```{r}
summary(dat)
```

How can we inspect the differences between the weight of men and women? We can start by ploting two histograms or density plots representing the weight by biological sex. 
```{r, warning=FALSE}
ggplot(dat,
       aes(x = weight, 
           fill = gender)) +
  geom_density(alpha = 0.5)
```

The distributions look different in shape, right? Where do you think the mean would be located in the plot? 
```{r}
# We can use "filter()" to filter the cholesterol values for men and women
dat.male = dat %>%
  filter(gender == 'male')

dat.female = dat %>%
  filter(gender == 'female')

# we will calculate the mean weight by sex.
mean.men <- mean(dat.male$weight, na.rm = TRUE)
mean.women <- mean(dat.female$weight, na.rm = TRUE)
```

And we can then add in the means to the plot as vertical lines. 

```{r, warning=FALSE}
ggplot(dat,
       aes(x = weight, 
           fill = gender)) +
  geom_density(alpha = 0.5) +
  geom_vline(xintercept = mean.men, colour = "cyan") +
  geom_vline(xintercept = mean.women, colour = "salmon")
```

> Do you think that the mean of the male weight is lower than 180?
> Do you think that the mean of the female weight is **really** different from the mean of the male weights?
> Do you think that the mean of the male weight is higher than that of the females?

------------------------------------------------------------------------

### Tests of mean


Now, we want to use a statistical test to check if,

-   males have a mean weight that is **significantly** lower than a specific value (one-sample, one-tailed test)
-   there is a **significant** difference in the mean of the weights between the two groups (two-sample, two-sided test)
-   male have a **significantly** higher mean weight than females (two-sample, one-sided test)

Note the use of the word **significant** in the previous statements!

This is exactly what **mean tests** such as the t-test (or the Wilcoxon test) are designed for!
We will perform here a **t-test**.

------------------------------------------------------------------------

#### One sample t-test

Using a one-sample t-test, we can check if values differ significantly from a target value.
For example, you could sample 10 chocolate bars, and test if they significantly differ from the expected weight of 100 g:

> Should we perform a one- or two-sided test?

```{r}
bars = c(103,103,97,102.5,100.5,103,101.3,99.5,101,104) # weights of 10 chocolate bars
chocbar.mean = 100 # expected weight
```

The function `t.test()` offers three **alternative** options: *two.sided*, *less* and *greater*.
Here, if we want to test whether the mean weight of the 10 chocolate bars is **different** from the expected weight of 100 g, we want to perform a *two sided* test and use the alternative *two.sided*.

It is essential to **clearly formulate the H0 and H1 hypothesis**.
There are two alternative but equivalent ways to do so.
Either:

-   H0: the expectation value of the random variable "Weight of a chocolate bar" is **equal** to 100 g.\
-   H1: the expectation value of the random variable "Weight of a chocolate bar" is **different** from 100 g.

or


-   H0: the mean weight of a chocolate bar is not significantly different from 100 g.\
-   H1: the mean weight of a chocolate bar is significantly different from 100 g.

Note the difference between these two formulations, and ask for help if you have questions about this!

```{r}
t.test(x = bars, mu = chocbar.mean, alternative = "two.sided")
```

> How would you interpret this result?
> Can you reject the H0 hypothesis?

Using $\alpha=0.05$, the H0 hypothesis can not be rejected as the p-value is 0.05244 (p-value \>= 0.05).
With $\alpha=0.05$, the mean weight of the chocolate bars is not significantly different from 100 g.

Using $\alpha=0.1$, the H0 hypothesis can be rejected (p-value \< 0.1).
With $\alpha=0.1$, the mean weight of the chocolate bars is significantly different from 100 g.

**BUT** ... It does not mean that $\alpha$ should be chosen with respect to the results of the t.test!!!

Before running a t.test, **formulate the null hypothesis H0 and the alternative hypothesis H1** and **decide about the alpha** value.
Remember, **alpha** represent the **false positive rate**: under the H0 hypothesis (test of two identical distributions), this is the proportion of tests that will detect a difference between the two groups (p-value \< $\alpha$).

**Beware not to get confused between one-/two-sample tests, and one-/two-sided tests!**

Regarding the mean weight of the males, we would like to check whether males have a mean weight that is significantly **lower** than 180.
Here as well, we will perform a **one-sample test**, with mu = 180.
However, we will perform a **one-sided** test using the alternative option **less**.

The hypotheses can be formulated as:

-   H0: the expectation value of the random variable "Weight of male patients" is **equal or greater** 180.\

-   H1: the expectation value of the random variable "Weight of male patients" is **less** than 180.

```{r}
t.test(dat.male$weight, mu = 180, alternative = "less")
```

> How would you interpret the result with $\alpha=0.05$?
> Can you reject the H0 hypothesis?

------------------------------------------------------------------------

#### Two-sample t-test (2-sided)

Now, we will compare the mean of the weights between males and females.\

According to the previous histogram, females have a different mean weight as males.
This can be tested using a *two-sample and two.tailed* t.test.

```{r, warning=FALSE}
ggplot(dat,
       aes(x = weight, 
           fill = gender)) +
  geom_density(alpha = 0.5) +
  geom_vline(xintercept = mean.men, colour = "cyan") +
  geom_vline(xintercept = mean.women, colour = "salmon")
```

The hypotheses can be formulated as:

-   H0: the expectation value of the random variable "Weight of male patients" is **equal** to the expectation value of the random variable "Weight of female patients".\

-   H1: the expectation value of the random variable "Weight of male patients" is **different** from the expectation value of the random variable "Weight of female patients".

```{r}
t.test(dat.male$weight, 
       dat.female$weight) # remember, alternative = "two.sided" per default.
```

> How would you interpret the result with $\alpha=0.05$?
> Can you reject the H0 hypothesis?

------------------------------------------------------------------------

#### Two-sample t-test (1-sided)

Looking at the histogram, other observers could in principle see a difference between the mean values of the weights and formulate the following hypotheses:

-   H0: the expectation value of the random variable "Weight of male patients" is **equal or lower** to the expectation value of the random variable "Weight of female patients".\

-   H1: the expectation value of the random variable "Weight of male patients" is **higher** than the expectation value of the random variable "Weight of female patients".

```{r}
t.test(dat.male$weight, 
       dat.female$weight, alternative = "greater")
```

> How would you interpret the result with $\alpha=0.05$?

> Can you explain why the p-value of the one-tailed t.test is lower than the p-value of the two-tailed t.test?
> What is the relation between these two values?

```{r}
# p-value of the two-sided t-test versus p-value of the one-sided t-test: 
t.test(dat.male$weight, 
       dat.female$weight, alternative = "two.sided")$p.value # two-tailed
t.test(dat.male$weight, 
       dat.female$weight, alternative = "greater")$p.value # one-tailed
```

This can be visualized using the t-distribution.
Here (see above result of the t.test), *t = 1.8453* and *df = 372.45*.

In the **one-tailed t.test**, the p-value is the area under the curve for t \> 1.8453 (alternative greater) **OR** for t \< -1.8453 (alternative less).

```{r, echo=F, fig.height=3, fig.width=5}
# No need to understand this code - just look at the graph and shaded areas.
x = seq(-5,5,by=0.01)
y = dt(x,df=372.45)
z = dt(seq(1.84,5,0.01),df=372.45)

ggplot() +
  geom_line(aes(x = x, y = y), 
            linewidth = 0.8) +
  geom_vline(xintercept = 1.8453, colour = "blue",
             linetype = 'dashed') +
  geom_ribbon(aes(x = ifelse(x >= 1.84, x, NA), 
                  ymin = 0, ymax = y), 
              alpha = 0.8, fill = 'pink')
```

In the **two-tailed t.test**, the p-value is the area under the curve for t \> 1.8453 (alternative greater) **AND** for t \< -1.8453 (alternative less).
It is two times the p-value of the one-sided t.test!

```{r, echo=FALSE, fig.height=3, fig.width=5}
# No need to understand this code - just look at the graph and shaded areas.
x = seq(-5,5,by=0.01)
y = dt(x,df=372.45)

ggplot() +
  geom_line(aes(x = x, y = y), 
            linewidth = 0.8) +
  geom_vline(xintercept = 1.8453, colour = "blue",
             linetype = 'dashed') +
  geom_vline(xintercept = -1.8453, colour = "blue",
             linetype = 'dashed') +
  geom_ribbon(aes(x = ifelse(x >= 1.84, x, NA), 
                  ymin = 0, ymax = y), 
              alpha = 0.8, fill = 'pink') +
  geom_ribbon(aes(x = ifelse(x <= -1.84, x, NA), 
                  ymin = 0, ymax = y), 
              alpha = 0.8, fill = 'pink')
```

IMPORTANTLY, a t-test can only be performed if the data is **normally distributed**!
If the data is not normally distributed, you will need to use a non-parametric test, like Wilcoxon test. 


------------------------------------------------------------------------

#### Wilcoxon test

What if the data is not normally distributed?
In that case, we are not supposed to use the t-test for testing differences between mean values!
Let us us see an example.

Let us start by loading the data.
```{r}
all.aml = read.delim('http://bioinfo.ipmb.uni-heidelberg.de/crg/datascience3fs/practicals/data/all.aml.cleaned.csv',
                     header=TRUE)
all.aml.anno = read.delim("http://bioinfo.ipmb.uni-heidelberg.de/crg/datascience3fs/practicals/data/all.aml.anno.cleaned.csv",
                          header=TRUE) %>%
  mutate(id = paste0("pat", Samples))
```

We check whether the distribution of the expression values for the gene *SOX4* corresponds to a normal distribution:

```{r fig.height=5, fig.width=5.5, warning=FALSE}
expression = all.aml %>%
  t() %>%
  as.data.frame() %>%
  na.omit()

# Plot histogram 
ggplot(expression,
       aes(x = SOX4)) +
  geom_histogram(bins = 20)

# Check if it's normally distributed using a QQ-plot 
ggplot(expression,
       aes(sample = SOX4)) +
  geom_qq() +
  geom_qq_line(colour = 'red')
```

This looks everything but normal!
In that case, we cannot apply the t.test, but need to apply a **non-parametric test** called the *Wilcoxon test*.
This test is performed not on the *values* (like the t-test) but on the *ranks* of these values (remember the difference between the Pearson's and the Spearman's correlations!)

```{r, warning=FALSE}
# divide the gene expression data in two groups according to ALL or AML patients:
# obtain the AML and rest
aml.patient.id = all.aml.anno %>%
  filter(ALL.AML == "AML")
other.id = all.aml.anno %>%
  filter(ALL.AML != "AML") # filters for those which are not labeled AML

gene.all = expression %>%
  rownames_to_column("id") %>%
  filter(id %in% other.id$id)

gene.aml = expression %>%
  rownames_to_column("id") %>%
  filter(id %in% aml.patient.id$id)
  
# test for a difference in the mean expression values using the Wilcoxon test:
wilcox.test(gene.aml$SOX4, gene.all$SOX4)
```

Compare the obtained p-value with the p-value obtained if we would have used the t-test:

```{r}
t.test(gene.aml$SOX4, gene.all$SOX4)
```

The p-values are very different!!
So is the difference of expression between ALL and AML patients for this gene significant or not taking $\alpha=0.05$?

Here, we **cannot** trust the t-test due to the non-normality of the data!
Hence, the correct p-value is the one from the Wilcoxon test.

------------------------------------------------------------------------

### Proportion tests

The t-test and Wilcoxon tests are **tests of the mean**, meaning that we are **comparing the means of two samples and looking for significant differences**.

But there are other hypothesis that one might want to test, related to the relationship between two **categorical** variables:

-   is the proportion of men **significantly** higher in the patients from Louisa compared to the ones from Buckingham?
-   is the proportion of smokers under 18 in Germany **significantly** higher than in other European countries?

The proportion test (**Fisher Exact Test** or **chi-squared test**) are used to investigate the relationship between 2 categorical variables, starting from a **contingency table**.
We will use a dataset with clinical information about breast cancer patients.

```{r}
dat.brca = read.delim('http://bioinfo.ipmb.uni-heidelberg.de/crg/datascience3fs/practicals/data/gbsg_ba_ca.dat', 
                      stringsAsFactors = FALSE)
```

> Check which variables in this dataset are categorical/ordinal/numerical.

We can now check if there is a significant relationship between some variables.
For example, we can verify if the choice of treatment with tamoxifen (variable `hormon`) is related to the pre-/post-menopausal status (variable `meno`)

First, we can build the contingency table for these 2 variables:

```{r}
## build contingency table
table(dat.brca$meno, dat.brca$hormon)
```

We can compute the **odds-ratio (OR)** for these two variables.
This metric measures the strength of the association between two events, which in this case would be treatment with tamoxifen and menopausal status.

Here's how to interpret the odds ratio (OR) value:

-   **OR = 1**: events are independent   
-   **OR > 1**: events are positively correlated   
-   **OR < 1**: events are negatively correlated   

```{r}
CT = table(dat.brca$meno,dat.brca$hormon)

OR = (CT[1,1]/CT[1,2])/(CT[2,1]/CT[2,2])
OR
```

> How would the odds-ratio look like if you would transpose the matrix?

Now we can run the one-sided **Fisher Exact Test** (FET).
The H0/H1 hypothesis are:

-   H0: the odds-ratio is not significantly larger than one
-   H1: the odds-ratio is significantly larger than one

```{r}
## build contingency table
tab = table(dat.brca$meno,dat.brca$hormon)
#
## run the FET
fisher.test(tab, alternative = 'greater')
```

Check if your computation of the odds-ratio is right!
Compute also the **two-sided** test:

> Formulate the H0/H1 hypothesis!

```{r}
fisher.test(tab)
```

We can also use the **chi-square test** to answer the same question.
The Chi-square test compares the **observed** number of occurrences in the contingency table to the **expected** number of occurrences if there was no relationship between the variables.

-   H0: the observed and expected occurrences are not significantly different
-   H1: the observed and expected occurrences are significantly different

```{r}
chisq.test(tab)
```

Now we want to verify the impact of age on the grade of the tumor.
We categorize the patients in under and over 40 year groups, and perform a chi-squared test:

```{r}
## contingency table
tab = table(dat.brca$age>40,
            dat.brca$grade)
tab # tumor grades 1,2,3; age under 40 (FALSE) and over 40 (TRUE)
##
chisq.test(tab)
```

We can determine the table of expected counts using the `apply()` function. 
This works similarly to the `lapply`, but it can be used to *apply* a function by rows (if 1), columns (if 2), or both:

```{r}
tot = apply(tab,2,sum) # this is the total number of occurrences in the 3 categories of "grade", independently of "age"
tot

age = apply(tab,1,sum) # this is the total number of persons above/below 40, independently of "grade"
age
```

On the other hand, `sapply` is commonly used for dataframe or other tabular formats: 

```{r}
tot.proportions = tot/sum(tot) # HO proportions (the proportions of occurrences in the three categories of grade, independently of age)

tab.exp = sapply(tot.proportions,function(x) {x*age}) # expected counts under H0
tab.exp
```

> How would you compute the chi-square test statistic using the `tab` and `tab.exp` tables?


------------------------------------------------------------------------

### EXERCISES   

#### Exercise 1: One-sided t-test

1. Consider the following graph, formulate the hypotheses H0 and H1 and perform a (one-sided) t-test.
Interpret the result using $\alpha=0.05$.

```{r, warning=FALSE, echo=F}
mean.height.f = mean(dat.female$height, na.rm = T)
mean.height.m = mean(dat.male$height, na.rm = T)

ggplot(dat,
       aes(x = height, 
           fill = gender)) +
  geom_histogram(alpha = 0.5, bins = 20,
                 colour = "black") +
  geom_vline(xintercept = mean.height.m, 
             colour = "cyan", linetype = 'dashed',
             linewidth = 1) +
  geom_vline(xintercept = mean.height.f, 
             colour = "salmon", linetype = 'dashed',
             linewidth = 1)
```

2.  Calculate the mean age of the men.
3.  Compare it to age = 50. Formulate the hypotheses H0 and H1 and perform a (one-sided) t-test. Interpret the result using $\alpha=0.05$.

#### Exercice 2: Two-sided t-test

Can you find interesting differences in the mean values of parameters of the dataset **dat** for the two groups defined by the **location**?
Follow these steps:

1.  Select the two groups according to the location.
To do so, check the result of `distinct(location)` and create two tibbles corresponding to each of possible the locations.

2.  Calculate the mean values of the numerical parameters for each group (ex: age, height, weight, waist, hip, ...).
*Hint*: create first a dataframe with numeric columns only (use `select(which(is.numerical))`).
Select the rows corresponding to the two groups and use an `summarise()` and `group_by` loop to calculate the mean values (grouped by location).

3.  Select one of these, formulate the H0 and H1 hypotheses and perform a (two-sided) t-test.
Interpret the result ($\alpha=0.05$).

#### Exercise 3: Mean and proportion testing
What test would you use for the following questions?

-   A lotion company has to figure out whether their last product is more likely to give acne to men rather than women.
-   The department of education wants to find out whether social science students have higher grades than science students.
-   A biologist needs to find out whether a specific gene is more likely to be silenced in lactose intolerant people.


#### Going further: Checking the normality of the distribution

In principle, t-tests require that the data is approximately normally distributed.
If not, we can use **non-parametric** tests (see next lecture).

In order to check whether the data is normally distributed or not, it is possible to perform a **Shapiro-Wilk** normality test (see lecture).
This statistical test is implemented in R in the function `shapiro.test()`. Try it out with any of the datasets we used before. 

A p-value inferior to the chosen $\alpha$ level here (we will use 0.05), means that the null hypothesis can be rejected. Therefore, the data distribution tested is not normal and you need to use a non-parametric test.


## Day 5: Multiple testing and regression

### Multiple testing  

In the previous examples we were only testing for one single hypothesis. However, it is common in biology that we have more than one hypothesis being tested.  
The table below is a reminder of which errors can be made when testing hypotheses:

```{r echo = F}
decision = c("Decision: Reject H0", "Decision: Do not reject H0")
h0_true = c("Type I Error", "Correct Decision")
h0_false = c("Correct Decision", "Type II Error")

knitr::kable(data.frame(decision,
                        h0_true,
                        h0_false), col.names = c("", "H0 is true", "H0 is false"))
```

Decision is associated with $\alpha$, so if the p-value is under 0.05, we reject the null hypothesis. However, when testing multiple hypothesis, we are prone to erroneously rejecting the 1st, 2nd or Nth null hypothesis. I.e., if we test 1000 hypotheses simultaneously, we expect to erroneously reject 50 (5%) just by chance!  

The **family wise error rate (FWER)** is the probability of making at least one Type I error and we can control this probability using a p-value correction. 

#### Example

Imagine that you are trying to test the effect of a treatment A on a cell line in the lab. You run a gene expression experiment to compare treatment with control (untreated) for a panel of 20 genes you pre-selected.  

The comparison between treatment and control for the panel gives you a set p-values of the 20 genes
```{r}
p.vals = c(0.1, 0.0001, 0.04, 1.2e-20, 0.002, 0.01, 0.5, 1, 0.02, 1.9e-5,
           1.1e-6, 0.03, 0.7, 0.06, 0.01, 0.3, 0.05, 1e-13, 0.032, 0.004)
```

But since you ran 20 different tests to generate these, there is a need to correct the p-values. You can do this on the p-values directly using the `p.adjust()` function. Run `?p.adjust()` to see which methods can be used. 

We will use the false discovery rate (FDR) correction on this example. 
```{r}
p.vals_FDR = p.adjust(p.vals, method="fdr")
p.vals_FDR
```

Check how the p-values change!

```{r, echo = F, out.width="50%", results='hold'}
## No need to understand this code;
results = data.frame(Genes = paste0("gene", 1:20),
                     Uncorr = p.vals, 
                     FDR = p.vals_FDR)
ggplot(results, 
       aes(y = Uncorr,
           x = Genes)) +
  geom_point(aes(colour = ifelse(Uncorr <= 0.05, "Significant", "N.S"))) +
  theme(axis.text.x = element_blank()) +
  scale_colour_manual(values = c("grey", "forestgreen")) +
  labs(y = "uncorrected p-value", title = "Uncorrected results",
       colour = "Outcome")

ggplot(results, 
       aes(y = FDR,
           x = Genes)) +
  geom_point(aes(colour = ifelse(FDR <= 0.05, "Significant", "N.S"))) +
  theme(axis.text.x = element_blank()) +
  scale_colour_manual(values = c("grey", "forestgreen")) +
  labs(title = "Corrected results",
       colour = "Outcome")
```

> How many genes change from being significant to non-significant? How does this impact your experiment?

### Regression

In this last part, we want to learn how to build a **regression model** in order to make predictions on certain quantitative variables using other quantitative variables. The important steps here are:

-   **learning** the model
-   **testing** the model to evaluate its performances, and also check that the assumptions of the linearity are given
-   **predict** values based on new data points

We will use again the diabetes data set, and build a simple linear regression models with a single explanatory variable.
We will focus on predicting the **cholesterol level**.


Load the data and perform some basic inspection on the data:

```{r}
tmp = read.table('https://www.dropbox.com/s/zviurze7c85quyw/diabetes_full.csv?dl=1',header=TRUE,sep="\t") 
```

We will limit the dataset to the numerical variables

```{r}
dat = tmp %>%
  select(
    where(is.numeric)
  )
head(dat)
```

First, we have to do some data cleaning and remove all patients with at least 1 "NA". Use `na.omit()`.
```{r}
# Select the patients without NAs
dat = dat %>%
  na.omit()
```


Then, we can make a heatmap and visualize the correlation values between each of the variables in the dataset.
```{r}
library(pheatmap)
cor.vals = cor(dat, method = "spearman")

pheatmap(cor.vals,
         cluster_cols = FALSE,
         cluster_rows = FALSE,
         display_numbers = TRUE)
```

> What are the strongest correlations? Do they make sense?

Apart from displaying the raw correlation values, we can test if any of those are **statistically significant**, i.e. if they are significantly positive (one-sided test), negative (one-sided test), or non zero (two-sided test). To do so, we will use `cor.test` function.

For example, there seems to be a positive correlation between *stabilized glucose* (stab.glu) and *hip circumference* (hip).

```{r}
## compute correlation
cor(dat$stab.glu,dat$hip)
##
## test for significance
cor.test(dat$stab.glu, dat$hip)
```

> Read carefully this output, and make sure you understand it. Check other pairs!

#### Univariate linear regression

Next, we will assess what is the most promising variable to predict cholesterol level. Go back to the correlation heatmap to see if there are variables highly correlated with it.

We will use glycosilated hemoglobin (`glyhb`) as a predictor of the cholesterol level and the function `lm()`.

```{r}
l.g = lm(chol ~ glyhb, data=dat)
summary(l.g)
```

For a simple linear regression (with only one explanatory variable), the p-value of the t-test for the slope is identical with the p-value of the F-test for the global model. *This will no longer be the case when including several explanatory variables!*

By the way, have we checked that a linear regression makes sense in this case? Remember that we have to check that:

-   the residuals are **normally distributed**
-   there is **no correlation** between the residuals and the explanatory variable

```{r, fig.height=4, fig.width=4, message=FALSE}
# normal distribution of residuals?
ggplot() +
  geom_histogram(aes(x = l.g$residuals)) 

ggplot() +
  geom_qq(aes(sample = l.g$residuals)) +
  geom_qq_line(aes(sample = l.g$residuals),
               colour = 'red')

## correlation residuals x-values?
corr.gly_resi = cor(dat$glyhb,l.g$residuals)

ggplot() +
  geom_point(aes(x = dat$glyhb, y = l.g$residuals))
```

> What is your overall opinion about the validity of the regression model here?

We can now use the model to predict the cholesterol values, and compare them to the real cholesterol values, since we have the information in this dataset.

```{r, fig.height=4, fig.width=4}
ggplot() +
  geom_point(aes(x = dat$chol, 
                 y = l.g$fitted.values),
             colour = "navy") +
  labs(x = 'Real values',
       y = 'Predicted values') +
  geom_abline(intercept = 0, slope = 1, 
              color = 'red')
```

Not really super convincing, right? Let's put more information into the model!

#### Multiple regression model

Let us include all the information to try to predict cholesterol level:

```{r}
l.all = lm(chol ~ .,data=dat)
summary(l.all)
```

> Do you note something unexpected in this report?

We can see that the inclusion of several explanatory variables improves the regression. Check the $R^2$ values for example!

We can see that the variables `weight`, `waist` and `hip` do not reach the significance level. Two explanations are possible

1.  either these variables are indeed non-informative regarding the prediction of the cholesterol level
2.  or the mutual correlation between these 3 variables interferes with the model.

We can remove waist and hip for example, and redo the regression

```{r}
l.less = lm(chol ~ stab.glu + hdl + glyhb + age + height + weight  + bp.1s + bp.1d,data=dat)
summary(l.less)
```

We can see how the `weight` variable seems indeed to contribute to the prediction of the cholesterol. The removal of the strongly correlated variables has increased its significance. It now almost reaches significance at the 5% level!

> Check the result of the F-test to compare both models!

We can now check if the prediction are better that with the univariate model

```{r, fig.height=4, fig.width=4}
ggplot() +
  geom_point(aes(x = dat$chol, 
                 y = l.less$fitted.values),
             colour = "navy") +
  labs(x = 'Real values',
       y = 'Predicted values') +
  geom_abline(intercept = 0, slope = 1, 
              color = 'red')
```

Better? I would say so... To determine the accuracy, we can compute the so called **root mean squared error (RMSE)**:

$$
RMSE = \sqrt{\frac{1}{n}\sum_{i=1}^n (x_i-\hat{x_i})^2}
$$

```{r}
n = nrow(dat)
rmse = sqrt(1/n*sum(l.less$residuals^2))
rmse
```

Of course, this is cheating, right? We are predicting the values on **exactly** the same data we used to learn the model. In real machine learning, we need to perform **cross-validation**, i.e. learn the model on one part of the data (*training set*) and validate it on another set (*test set*).

Let us split the dataset in a test and training set randomly:

```{r}
set.seed(1234)
## take 200 random patients to form the training set
i.train = sample(1:nrow(dat),100)
##
dat.train = dat[i.train,]
dat.test = dat[-i.train,]
```



We now learn a new model on the train dataset:

```{r}
l.train = lm(chol ~ stab.glu + hdl + glyhb + age + height + weight  + bp.1s + bp.1d,data=dat.train)
summary(l.train)
```

We can compute the RMSE for the training dataset

```{r}
n.train = nrow(dat.train)
rmse.train = sqrt(1/n.train*sum(l.train$residuals^2))
rmse.train
```

Let use use that model to predict cholesterol values for the left out test set

```{r}
pred = predict(l.train, newdata = dat.test)
```

and compute the rmse:

```{r}
n.test = nrow(dat.test)
residuals = dat.test$chol - pred
rmse.test = sqrt(1/n.test*sum(residuals^2))
rmse.test
```

Of course, the RMSE is higher on the test dataset, since this does not include the data used for the establishment of the regression model; however, this is a more realistic estimation of the validity of the model, as it indicates how well the model could be extended to novel, independent data!

An important topic here is **feature selection**, i.e. finding the optimal and minimal set of explanatory variables that allow to predict well the output variable.


We now repeat the train/test split 10 times with each time a different random split; plot the 10 `rmse.train` and `rmse.test` values!
```{r}
set.seed(345)
RMSE <- sapply(1:10, function(x) {
  i.train = sample (1:nrow(dat),100)
  ##
  dat.train = dat[i.train,]
  dat.test = dat[-i.train,]
  ##
  l.train = lm(chol ~ stab.glu + hdl + glyhb + age + height + weight  + bp.1s + bp.1d,data=dat.train)
  ##
  n.train = nrow(dat.train)
  rmse.train = sqrt(1/n.train*sum(l.train$residuals^2))
  ##
  pred = predict(l.train,newdata = dat.test)
  ##
  n.test = nrow(dat.test)
  residuals = dat.test$chol - pred
  rmse.test = sqrt(1/n.test*sum(residuals^2))
  RMSE <- c(rmse.train,rmse.test)
  RMSE
})
#

ggplot() +
  geom_point(aes(y = RMSE[1,],
                 x = 1:10, colour = "gold")) +
  geom_point(aes(y = RMSE[2,],
                 x = 1:10, colour = "steelblue")) +
  scale_x_continuous(breaks=c(2,4,6,8,10)) +
  geom_hline(yintercept = mean(RMSE[1,]),
             colour = "gold") +
  ylim(min(RMSE), max(RMSE +2)) +
  geom_hline(yintercept = mean(RMSE[2,]),
             colour = "steelblue") +
  scale_colour_manual(name = "RMSE", guide = 'legend',
                      values =c('gold'='gold','steelblue'='steelblue'), 
                      labels = c('train','test')) +
  labs(x = "Iteration", y = "RMSE values")
```








