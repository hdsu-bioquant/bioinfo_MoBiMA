---
title: Bioinformatics Introductory Course - Master Molecular Biotechnology
author: "Biomedical Genomics group, IPMB"
site: bookdown::bookdown_site
output: bookdown::bs4_book

documentclass: book
bibliography: [book.bib, packages.bib]
# url: your book url like https://bookdown.org/yihui/bookdown
# cover-image: path to the social sharing image like images/cover.jpg
description: |
  RMarkdowns for the Data Analysis introduction course (SoSe2024), MoBi MA 
link-citations: yes
github-repo: rstudio/bookdown-demo
editor_options: 
  markdown: 
    wrap: sentence
---


![](./logos_IPMB_final_202309.jpg)  


# Introduction {.unnumbered}

R is a powerful programming language for the analysis of data.
R is very versatile, it is free, and very easy to understand.
That is because, unlike other programming languages, R is quite “human-friendly” and dynamic, thus it does not require compiling to be “machine-readable”.
All you have to do is type R commands in the console or script.

Through this course, you will learn:

> -   Basic R programming for data analysis (*before we start*, go though the sections present in this Introduction)
> -   How to use R to work on different datasets and apply statistics (*Week One*)
> -   How to annotate, analyse, obtain, pre-process, and visualize genomic data (*Week Two*)
> -   How to perform RNA-seq analysis (*Week Three*)

## Where to start {.unnumbered}


### Log into the server {.unnumbered}

We will use an **RStudio Server** running on a virtual machine.
Here are the instructions to connect:

You can find the list of students [here](https://docs.google.com/spreadsheets/d/1eMCr4b6UcOPIG8tJkfoEkMoZzJycWJPMYN5nMZWiU48/edit?usp=sharing).
Check your name, your user name on which you will connect.

#### if you are running machine with Linux or MacOS {.unnumbered}

-   open a terminal and type the following commant: `ssh <USER>\@194.94.113.18 -p 30210 -L 8787:127.0.0.1:8787`; replace `<USER>` with your user name from the Google Sheet
-   in a browser, go to [this url](http://127.0.0.1:8787)
-   log in with your user name and the password that will be provided to you.

You can now start working in RStudio!

#### if you are running machine with Windows {.unnumbered}

Here, the user should install [Putty](https://www.putty.org/).
In the GUI, the following settings are needed:

-   In the main panel:
    -   Host name (or IP address): <USER>\@194.94.113.18
    -   Port: 30210 (check the right port in the Google Sheet)
    -   Connection type: SSH
-   In the panel "Connection / SSH / Tunnel":
    -   Source port: 8787
    -   Destination port: 127.0.0.1:8787
    -   Check options "Local" and "Auto"
-   Then click "Add"
-   Now click "Open"
-   in a browser, go to [this url](http://127.0.0.1:8787)
-   log in with your user name and the password that will be provided to you.

You can now start working in RStudio!

### Installing R and RStudio {.unnumbered}

Alternatively, you can also choose to install both R and RStudio.
Rstudio is the most commonly used IDE (or Integrated Development Environment) for R, and it will **use the R version installed on your computer**.
RStudio is meant to make R programming quite visual and way easier for you as a beginner.

Start by installing [R](https://cran.r-project.org/) and then [RStudio](https://posit.co/download/rstudio-desktop/).

RStudio is composed of 4 main windows/panels.\
The **Editor**, the **Environment**, the **Console**, and the **Files** window/panel.
Order can vary.

![In **Editor (1)**, you can find the area where you usually write most code, like a Rscript or a Rmarkdown. **Environment (2)** shows you your variables, history, among others. Here, you can see any variable you define in your session. You can use this to do a very basic inspection of objects you create or import. **Console (3)** is where all your code gets processed when you run it. The console remembers (which is why when you click arrow up or down, it shows you commands you ran before), but it does not keep! Therefore, ALWAYS write the meaningful code lines in a script/markdown (1). Lastly, **4** can be used for browsing documents (*Files*), see the plots you produce (*Plots*), searching/managing packages (*Packages*), or find help on functions (*Help*).](figures/RstudioLayout.png)

### Courses {.unnumbered}

It is recommended that you take an introductory class for R to get to know this language before you start coding here.
Start by going through [this Chapter on Getting Started with Data in R](https://moderndive.netlify.app/1-getting-started).
We will cover other basics on the next sections.

You can find more resources here:

> -   R Tutorial for Beginners at [guru99](https://www.guru99.com/r-tutorial.html)
> -   R courses at [Babraham Bioinformatics](https://www.bioinformatics.babraham.ac.uk/training.html#advancedrtidy)
> -   R for [Data Science](https://r4ds.had.co.nz)
> -   R markdown at [RStudio](https://rmarkdown.rstudio.com/lesson-1.html) and in this [cheatsheet](https://www.markdownguide.org/cheat-sheet/)
> -   [Youtube tutorial](https://www.youtube.com/watch?v=h29g21z0a68) on ggplot2 with one of its developers

### Installing and configuring Cyberduck {.unnumbered}

In order to access remote files (for example opening a pdf file), you will need to install an additional tool, called cyberduck (https://cyberduck.io/).

-   Install Cyberduck according to your operating system
-   Once installed, open Cyberduck
-   On the top left, click on Neue Verbindung(or new connection)

In the new window, make the following changes:

-   select sftp in the dropdown menu
-   in the server box, type 194.94.113.18
-   in the port box, type 30210
-   in Benutzername (or Username), type your username (like user1)
-   password box: gkBio%!

Click on Verbinden (or Connect)

## Good practice {.unnumbered}

Two notes of advice about good practice now that your journey is about to start:

-   **Commenting**: Comment your code as you go with `#`! This way you avoid forgetting the meaning of a given line. This is also important if you want to share your code with others. Trust me, you will not remember what you are writing today in this class one week from now, so it will be best to have a reminder.\
-   **Clean and tidy code**: See that weird line in the middle of your script/markdown? This is meant to be a guideline about code length. If your code is so long that it crosses the line, please find a way to make it shorter. In addition, avoid making your chunks too long and make separate markdowns or scripts for each class or topic, it will keep everything more organised and easy to track.

## R Markdown {.unnumbered}

R Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents.
You will be working with R Markdown during this course.
For more details on using R Markdown see <http://rmarkdown.rstudio.com> or <https://www.markdownguide.org/basic-syntax/>.

R instructions in a markdown file are written in a "chunk", the one below.\
Chunks can be added using the `+C` icon at the top right corner of this panel/editor.\
There is also a keyboard shortcut for it.

```{r Print Hello World!}
# Sentences written after "#" are comments. Comments are ignored during the execution of the code.
# print "Hello world!" on the screen.
print("Hello world!")
```

> What happens in the editor if you remove the second parenthesis?
> Do you see the red cross appearing?
> You can click on it and read the comment.\
> And what happens if you press "enter" while the second parenthesis is missing?\

R is very smart and it will usually let know know when there is a simple syntax mistake like this one on your code.
R will also try to guess variable and function names from your input, all you have to do is *click TAB for auto-complete*.

## R Basics {.unnumbered}

### Variable assignment in R {.unnumbered}

You can start by testing one of the most basic tasks - variable assignment.
Variable assignments in R can be done using either the `<-` symbol, or the `=` symbol.

```{r Assigments, results='hide'}
a <- "Hello world!"
# or
hello = "Hello world!"
```

You can now run `print(a)` and `print(hello)`.
Note that after assignment both variables (a and hello) are listed in the "Environment" window of RStudio.

> R is case sensitive.
> `hello` is a known variable, but `Hello` is not!

You can remove variables too...

```{r rmvar, results='hide'}
rm(a)
```

### Classes {.unnumbered}

In the same way that a orange is a fruit or Malta is a country, any R variable corresponds to a class and holds a given structure.
Classes Let us say you generate a new variable (`name`).
How can you know more about it?

```{r}
name = "Marie"
```

The function `str()` gives you the **structure** of the variable.

```{r}
str(name)
```

where *chr* stands for character, the class of this variable.

The function `class()` gives you the **class** of the variable.

```{r}
class(name)
```

### Logical operations {.unnumbered}

We can perform tests on simple variables using the `==`, `>`,`<` operators:

```{r}
name == "Harry"
```

The output of this test will be either FALSE/TRUE.
You can test these using numbers too:

```{r}
33 > 23
```

### Vectors {.unnumbered}

We can make any vector using `c()`.
Try it with some numbers.

```{r}
var1 = c(3,7,12,2)
```

If you want to identify the specific number of this vector by position, you can use square brackets.
Like this:

```{r, results='hide'}
var1[1]     # outputs the first element
var1[2:3]   # outputs the 2nd to 3rd elements
var1[-3]    # outputs all except the 3rd element
```

### Matrixes {.unnumbered}

A matrix is a two-dimensional **array** of numbers.
They are defined using the `matrix()` function as follows: Take note of the difference between `byrow=FALSE` and `byrow=TRUE`.

```{r Defining matrices , results='hide'}
x <- c(1, 2, 3, 4, 5, 6)

X <- matrix(data = x, nrow = 2, ncol = 3, byrow = TRUE)
X <- matrix(data = x, nrow = 2, ncol = 3, byrow = FALSE)
```

> Do you understand all parameters ("nrow", "ncol", "byrow") of the `matrix()` function?
> What happens if you do not specify "byrow" as TRUE or FALSE?

You can ask for the **dimensions** of a matrix (and see later also for data frames) using the function `dim()`.
Do it for X.

```{r dim_matrix, results='hide'}
dim(X)
```

To access the elements of a matrix, it is similar to the vector but with 2 indexes:

```{r Accessing Matrices, results='hide'}
X[1, 2]	                    # [1] 3  -> element in the first row and second column
X[1:2, 1]	                  # [1] 1 2	
X[2, ]	                    # [1] 2 4 6	 -> all elements of the second row
length(X[1:2, 1])	          # [1] 2  -> X[1:2, 1] is a vector of length 2
```

### Dataframes {.unnumbered}

Data frames are like matrices, but they can contain **multiple types** of values mixed.\
We can create a data frame using the `data.frame()` function.
We can also convert a matrix into a data frame with the `as.data.frame()` function.

We can prepare three vectors of the same length (for example 4 elements) and create a data frame:

```{r student_gen}
Name <- c("Leah", "Alice", "Jonas", "Paula")
Age <- c(21, 22, 20, 22)
Course <- c("Mathematics", "Physics", "Medicine", "Biology")
Place_of_birth <- c("USA", "Germany", "Germany", "France")

Students <- data.frame(First_Name = Name, 
                       Age = Age, 
                       Course = Course,
                       Place_of_birth = Place_of_birth)
Students
```

We can again access specific elements in a similar manner to the matrix before.
Try some of these examples:

```{r get_elements_df, results='hide'}
# Columns
Students$Age 
Students[["Age"]]

# Rows or columns
Students[, 3]
Students[3,]

# Elements
Students$Course[3]    # access element 3 of the column "Course"
Students[2:3, 3]      # select elements 2 and 3 of column 3 
```

### Apply, sapply, lapply {.unnumbered}

`Apply()` is used to repeat the same operation over all columns/rows of a **matrix** or **data frame**.
Therefore, we need to specify three parameters:\
1.
The matrix on which the operation should be performed\
2.
Whether to repeat the operation over rows, which is defined by a "1", or columns (use a "2" instead)\
3.
The operation that should be performed.\

Let's have a look at an example to make this more clear: We want to determine the minimum for each row of the matrix "Mat".
Therefore, we can use the function `min()`.

```{r apply for matrices}
#First, we build the matrix 
m <- rnorm(30)   # generate 30 random numbers for the matrix
Mat <- matrix(data = m, nrow = 6)
Mat

#Next, we want to determine the minimum value of each row: 
apply(Mat, 1, min)
```

> What happens if you change the "1" in the apply function to "2"?
> Try it!

> Remember that you can use `help(apply)` or `?apply` to get help on this function!

We can perform a loop over all elements of a **vector** or **list** using the `sapply()` function.\
`sapply()` needs two information:\
1.
which vector do we consider?\
2.
which function do we want to apply to each element of this vector?\
`sapply()` will return a **vector** containing the results.

For instance, we can calculate the square root of every element of a vector using the function `sqrt()`:

```{r sapply for vectors}
x <- c(1:5)
sapply(x, sqrt)
```

Instead of using built-in functions, we can also write our own little function and combine it with `sapply()`, `apply()` or `lapply()` (see below):

```{r}
# sapply()
z <- c(1:5)
sapply(z, function(x) {x*2}) 

# apply()
Mat 
apply(Mat, 1, function(x) {sum(x*2)}) #the operation is performed on every row of "Mat"; every element is multiplied by two and the sum of the row is calculated
```

Besides, we can use `sapply()` for a **list**, for example to calculate the length of every element using `length()`:

```{r lapply for lists}
List1 <- list(color = c("blue", "red"), size = 5, state = c(TRUE, FALSE, TRUE, TRUE))

sapply(List1, length)
```

As you can see, this returns a vector with the length of every list element.

However, sometimes it can be useful to keep the results stored in a list.
Therefore, we can use `lapply()` instead.
Let's have a look at the difference between `sapply()` and `lapply()` using the example from above:

```{r lapply}
lapply(List1, length)
```

> Do you understand the difference?

### For Loops {.unnumbered}

Besides the apply-family, we can use **for loops** for iterating over a sequence:

```{r For loops}
x <- c("a", "b", "c")

for (i in x) {
  print(i)
}

#or 
for (i in 1:4) {
  print(i*3)
}
```

### If Statements {.unnumbered}

You can use an **if statement** to execute a block of code only, if a condition is TRUE.

```{r If statements}
a <- 5
b <- 10

if (a < b) {
  print("a is smaller than b")
}
```

> Modify "a" or "b" and see how the output changes!

We can also add **else if** to this statement.
In that case, if the **if** condition is FALSE, the **else if** condition will be tried:

```{r else if}
a <- 10
b <- 10

if (a < b) {
  print("a is smaller than b")
} else if (a == b) {
  print("a is equal to b")
}
```

Besides, we can add an **else** statement to be executed when all previous conditions are not TRUE.

```{r else}
a <- 10
b <- 5

if (a < b) {
  print("a is smaller than b")
} else if (a == b) {
  print("a is equal to b")
} else {
  print("a is greater than b")
}

```

> Do you understand how this works?
> What would happen if a = 10 and b = 10?
> Try it!

## Data wrangling with tidyverse {.unnumbered}

### Introducing tidyverse {.unnumbered}

**Tidyverse** is a set of packages often used to manipulate data.
You can think of tidyverse as a variation of the original R, which is meant to be a grammar specific to data manipulation.

You can install these through:

```{r, results='hide', eval=F}
install.packages("tidyverse")
```

And load it using

```{r, results='hide', warning=FALSE}
library(tidyverse)
```

You can find R packages in CRAN.
CRAN is a large repository containing almost 20 thousand packages!
For this course, we will just use CRAN packages.
To install them, you will use `install.packages()`.

We will learn a couple of functions from this framework which will make the next tasks a lot easier.
Tidyverse works with the *tibble* class, which is very similar to a the dataframe class, only faster and tidier.

Start by loading a default tibble including sleeping patterns from different mammals.
It also includes information on the animal order, genus, diet, or body weight.
To do so, run the following:

```{r}
data(msleep)
```

We can start by inspecting `msleep`.

```{r, results='hide'}
glimpse(msleep)
```

We can print the first rows of a dataframe using `head()`.
Try it out.

```{r}
head(msleep)
```

### Filter based on conditions {.unnumbered}

Now that we know how this dataset looks like, we can start doing more with it.
Let us see the information for the domestic pig.\
We can do it using `filter()` on the **genus** column:

```{r}
msleep %>%
  filter(genus == "Sus")
```

We also use the `%>%` operator here.
This is a **pipe operator** and you will use it very often.

To understand piping, let's imagine you are making a three layer cake in a factory.
Instead of manually carrying the cake from one station to another to add layers and frosting, you have a conveyor belt transporting the cake through the stations automatically to make the process much more efficient.
Piping avoids that you have to create multiple intermediate objects to achieve your final result when performing multiple functions.

> Try filtering other columns based on numeric conditions.
> Use `filter()` to see which 7 animals in the datasets have a body weight superior to 200 (kg).

### Select specific columns {.unnumbered}

Now, let's say you want to *select* only specific columns.
Use the `select()` function to select the columns `name`, `order`, and `bodywt` (body weight).

```{r}
body_weights = msleep %>%
  select(name, order, bodywt)

head(body_weights)
```

### Operations on groups {.unnumbered}

We can assess the average (`mean()`) body weight by animal order present.
To do so, we will use `summarise()` and `group_by()` functions on the *body_weights* we generated previously.

-   `group_by()`: Assigns a group to a given set of rows (observations).\
-   `summarise()`: Makes calculations for a given group based on the row values assigned to it.

```{r}
body_weights %>% 
  group_by(order) %>%
  summarise(mean_bweight = mean(bodywt, na.rm = T)) # na.rm = T will exclude missing values (NA)
```

### Reshaping data {.unnumbered}

On tidyverse, he objects you are working on are sometimes not in a tidy format.
There are three rules which make a dataset tidy:

-   Each variable has its own column.
-   Each observation has its own row.
-   Each value has its own cell.

![Here is the comparison between object types. Tidy objects (right) require an extra step of processing.](figures/tidyexp.png) [Image source](https://r4ds.hadley.nz/data-tidy.html)

How can we convert them then?
The mammalian sleep dataset we used earlier is already in a tidy format, so we will generate an example.

You are testing a new diet on 7 cats, and thus registering their weight before and after 1 month on the diet.

```{r}
cat.weights <- data.frame(CatName=c('Muffin', 'Lily', 'Mittens', 'Oreo', 'Loki', 'Fluffy', 'Honey'),
                          month0=c(4.3, 4.5, 5, 4.4, 3.8, 5.5, 4.5),
                          month1=c(4.4, 4.5, 4.9, 4.3, 3.9, 5.4, 4.6))

cat.weights
```

In this example, you can see that we have 2 different variables for the same unit (weight), these are our *gatherable* columns.
We can then make this dataframe tidy using the `gather()` from the `tidyr` package:

```{r, warning=FALSE}
library(tidyr)

cat.weights %>% 
  gather(key = Time, # column name given to the gathered column names
         value = Weight, # column name will be given to the values
         month0:month1) # columns to gather
```

Do you understand the differences?
Note that now each name shows up twice.

## Plotting using ggplot2 {.unnumbered}

One of the many advantages of tidyverse is how compatible it is with `ggplot2`.
Thanks to the wonders of the `%>%` operator, you can generate a basic plot instantly from a gathered/tidy tibble without creating intermediate objects.

**The structure of any `ggplot()` is always based on two essential elements: the aesthetics (`aes()`) and the plot layers (named as `geom_something()`. `aes()` can found within the `ggplot()` function or on the `geom_something()`.** There are multiple options for `geom_X()` depending on the information you are plotting.
You can find all possibilities [here](https://ggplot2.tidyverse.org/reference/).

The `aes()` function can harbor information on axis (x, y) and other aesthetics.
Let us see an example within the scatter plot next.

In this course we will work with `ggplot()`, but plots can also be generated by another framework.
The alternative to `ggplot()` is `plot()`.
Beware when looking for solutions for your data visualization problems.

### Scatter plot {.unnumbered}

For example, if we want to see the relationship between the sleep time (total, in hours) and the body weights, we use this formula:

```{r, fig.height=4, fig.width=4}
ggplot(msleep,                    # start by picking the dataframe to plot
       aes(x = bodywt,            # column to use for x-axis
           y = sleep_total)) +    # column to use for y-axis
  geom_point()                    # plot layer for points
```

> The x-axis looks a little squished, so it will be hard to view many of the trends.
> We can then log-transform this scale using the `log()` function.
> Try it out!

We can try to play with the other aesthetics of the plot.
For example, we can use the colour easthetic to colour the dots based on the *vore*/diet.

```{r, fig.height=4, fig.width=5}
ggplot(msleep,                    
       aes(x = log(bodywt),            # column to use for x-axis
           y = sleep_total,            # column to use for y-axis
           colour = vore)) +           # column used for colouring dots
  geom_point() 
```

### Histogram {.unnumbered}

Let us start by making an histogram, as we will work with distributions over this course.
If we want to see the global distribution of sleep time (total, in hours) for all animals in the dataset, we will use `geom_histogram()`.
Like this:

```{r, message=FALSE, fig.height=4, fig.width=5}
ggplot(msleep,                    # start by picking the dataframe to plot
       aes(x = sleep_total)) +    # column to use for y-axis
  geom_histogram()                # plot layer for histogram
```

In some cases, you will not have the data on a tidy tibble and rather on a vector for example.
For such cases, the structure would be slightly different.
Check how you can plot the distribution of a random set of numbers.

```{r}
rand.numbers = rnorm(100) # we will learn this function later on

ggplot() +   
  geom_histogram(aes(x = rand.numbers),  # aes goes in the plot layer for histogram
                 bins = 50)
```

> Try changing the option `bins`.
> What do you see?

### Boxplot {.unnumbered}

In this next example, we are creating a boxplot with the sleep time (total, in hours) for each dietary group (`vore`).

```{r, fig.height=3, fig.width=4}
ggplot(msleep, # we use na.omit here to hide NAs from the plot
       aes(x = vore,
           y = sleep_total)) +
  geom_boxplot()
```

See the NA on the x-axis?
In the course (week one) we will learn how to clean the data to avoid missing values like these ones.

> Try some of the other geom\_ layers available.
> As an example, `geom_violin()` can be used in a similar way to `geom_boxplot()`.

## Common errors {.unnumbered}

`Error in library(viridis) : there is no package called ‘viridis’`:\
**Solution:** install the required package using `install.packages("package")` and load it using `library(package)`.

`Error in gather : could not find function "gather"`:\
**Solution:** load the package the function belongs to.
You can see it by running `??gather`

`Error in fortify(): ! data must be a <data.frame>, or an object coercible by fortify(), not a <uneval> object.`:\
**Solution:** `aes()` is within `ggplot()` when it should be on `geom_...()`.

## Introductory exercises {.unnumbered}

### Exercise 1: Vectors {.unnumbered}

1.  Create a vector named "test_scores" containing the test scores 75, 90, 65, 68 and 83 of students.

2.  Create a vector "passing" that contains TRUE for test scores above 70 and FALSE for scores equal or below 70.
    *Hint*: Remember that you can perform tests on vector elements.
    Use `sum()` to check how many students passed the test.

3.  *(expert)* Can you create a vector "high_scores" that contains only scores above 70, using the results obtained in 1.
    and 2.?

### Exercise 2: Matrices {.unnumbered}

1.  Create a matrix that looks like this:

```{r}
#      [,1] [,2] [,3]
# [1,]    2    4    6
# [2,]    8    10   12
# [3,]    3    5    7
# [4,]    2    9    11
```

2.  Select different parts of the matrix, for example:

    -   the element of the second row and first column
    -   the second and third element of the second column
    -   all elements of the last column
    -   Try something else!

3.  Add a new column to the matrix using `cbind()`.

4.  Can you name the columns of the matrix as "A", "B", "C" and "D" using `colnames()`?

### Exercise 3: Data Frames {.unnumbered}

1.  Create a data frame from these vectors, with three columns and five rows:

```{r Exercise 4 Data frames}
Product_name <- c("orange", "strawberry", "broccoli", "blueberry", "cucumber")
Fruit <- c(TRUE, TRUE, FALSE, TRUE, FALSE)
Color <- c("orange", "red", "green", "blue", "green")
```

2.  Select the column "Fruit".
    How many Fruits are there?
    *Hint*: Remember the function `sum()`.

3.  Add a column named "Berry" containing TRUE (for products that are berries) or FALSE to the data frame.

<!--chapter:end:index.Rmd-->

# Lecture files

* Day 1 - data types/graphs/correlation [slides](./slides/GKBioinfo_week1_day1.pdf)


<!--chapter:end:00-slides.Rmd-->

# Week 1 : data analysis {.unnumbered}

## Day 1: Descriptive statistics and data types {.unnumbered}

Today, you will learn how to perform basic tasks on a dataframe/tibble, descriptive statistics, perform data cleaning, and plotting.

### Data features and where to find them

#### Load the data

The diabetes dataset, which we will be using in this practical class will be downloaded from an online repository.
We will load that into R and have a sneak peek into how it looks like with the console.
In the following you will see several functions that give us information about our dataset.

```{r}
dat = as_tibble(read.delim('https://tinyurl.com/y4fark9g')) # Load the dataset
head(dat, 10) # Look at the first 10 lines of the table
```


#### Dimensions and naming

**1.** What is the dimension of our dataset (i.e. how many rows/columns are there in our data)

```{r}
# Dimension
dim(dat)
```

```{r, results='hide'}
# Number of columns
ncol(dat)
# Number of rows
nrow(dat)
```

**2.** What are the column names of our dataset

```{r}
colnames(dat) # Similarly rownames() for rows
```

Probably you are confused about what these column names mean.
For more description on these values [look here](https://biostat.app.vumc.org/wiki/pub/Main/DataSets/Cdiabetes.html)

#### Numerical features 

**3.** How do we extract the minimum and maximum age of patients in our dataset?

```{r, results='hide'}
min(dat$age)
max(dat$age)
range(dat$age)
```

> Can you find out the same for height and weight?

**4.** How does the overall summary of our entire dataset look like?

```{r, results='hide'}
summary(dat)
```

> Can you explain what you see after you run the `summary()` function?


Feel free to play around with this syntax until you feel comfortable with it.
You can open a window with `View(dat)` to compare your results.

------------------------------------------------------------------------

### Data cleaning

Very often the first thing one needs to do before any data science project is to clean up the raw data and transform it into a format that is readily understood and easy to use for all downstream analysis.
This process usually involves: --

-   Removing empty value rows/columns
-   Removing unused or unnecessary rows/columns
-   Reordering the data matrix
-   Keeping columns uniformly numeric (age, weight etc) or string (names, places etc) or logical (TRUE/FALSE, 1/0)
-   Handling strange caveats which are data specific like replacing `,` or `.`, or `;` from numbers etc

Lets do some clean up of our own diabetes data

1.  We will make the `id` column the row names for the dataset;
2.  We will remove the `bp.2s` and `bp.2d` columns as it has mostly missing values (see summary above);
3.  We will also remove the column `time.ppn` which will not be required in our analysis;
4.  We will reorder the columns of the data such that all the qualitative and quantitative values are separated.

To perform this cleanup, we need a couple of important functions, that we will first discuss:

-   `filter`
-   `is.na`
-   `mutate`
-   `across`
-   `%in%`

#### filter()

`filter()` is used on a dataset to filter rows satisfying a condition you specify like we saw previously (Introduction).
Let's look at an example. We are only filtering for senior individuals in our dataset. 

```{r}
dat_seniors = dat %>%
  filter(age <= 65)
```

We can also filter based on other conditions, like location, sex, among others.
In some cases, we can also use `which()` to filter values. The syntax is different...

```{r, eval=FALSE}
dat[dat$age <= 65,]
```

... but it works for vectors and other classes. Let's see the next example.

```{r, eval = TRUE}
# number of animals you have
number = c(2,3,4,5,1,2,5)
# Let's create a different vector (of the same length)
animals = c("cat", "dog", "cow", "parrot", "zebra", "sparrow", "lizard")
# Let's use the "which()" function now
animals[which(number > 2)]
```

We selected all animals from the "animals" vector that correspond to more than three individuals in the "number" vector.

#### is.na()

`is.na()` is used to determine if NA values are present in a given object. We can try a simple example with one variable being assigned as NA.
```{r}
x = 2
is.na(x)

y = NA
is.na(y)
```

We can do this with vectors obtained from `dat`. What class is the output in? 

```{r, results='hide'}
is.na(dat$glyhb)
```

#### mutate()

`mutate()` is often used to create a new column based on another column of the dataframe. Let us use this function to *mutate* two new columns including the weight in kilograms and the height in centimeters. **The conversion from pounds to kilograms can be done by multiplying weight in pounds by 0.454. To covert height to centimeters we only need to multiply height (inches) by 2.54**.  

```{r}
dat %>%
  mutate(weight.kg = weight * 0.454,        # you can generate both columns using the same mutate!
         height.cm = height * 2.54) %>%     # we do not need to save this output
  select(id, weight.kg, height.cm)
```

#### across()

`across()` is very often used together with `mutate()` and another helper function, like `everywhere()`, `starts_with()`, `ends_with()`, or `contains()`.
Later, we will use `across()` together with the other functions we learned previously to remove NAs like this:

```{r, results='hide', eval=FALSE}
dat %>%
  rowwise() %>%
  mutate(na_count = sum(is.na(across(everything()))))
```

There is much to unpack here:

-  `rowwise()` ensures that the next operations are applied by row.  
-  `mutate()` adds a new column called na_count to the dataframe.  
-  `across(everything())` selects all columns in the current row.  
-  `sum(is.na(...))` calculates the sum of missing values for each row.  

> Try to run the previous example without `rowwise()`. What does it look like?



#### %in%

This is an operator to check which elements of a first vector are inside a second vector.

```{r}
c('Frodo','Sam') %in% c('Pippin','Sam','Frodo','Merry')
```

#### Ready for the cleaning!

The first column of the dataframe is the column with the name "id".
The rows are just numbered, without names.
We are going to rename the rows using the column "id". The function `column_to_rownames()` allows us to do this efficiently.  

```{r, results='hide'}
# set the row names using the column id
dat = dat %>%
  column_to_rownames(var = 'id')
```

> Keep in mind that rownames must be unique!

The *na_count* column will then include the number of NAs per row. Do you understand how it works? 
We finally apply `filter` again to keep only rows with less than or 2 NAs.

```{r}
dat = dat %>%
  rowwise() %>%
  mutate(na_count = sum(is.na(across(everything())))) %>%
  filter(na_count <= 2)
```

We will also remove the *na_count* and some problematic columns (*bp.2s*, *bp.2d* and *time.ppn*) by **selecting the ones which are not these**. We can do this using `!`, as this character can be used to invert results. Let us try it with `select()`. 

```{r}
dat = dat %>%
  select(!c(na_count, time.ppn, bp.2d, bp.2s))
```


Next, we can re-order the remaining columns, in order to put the categorical columns first, and numerical columns after. We can use `select` to order columns too, but we need to combine it with `where()` and functions which verify the class of the columns, like `is.character()` or `is.numeric()`.  

Here is a simple example:
```{r, results='hide'}
# Create a character and numeric 
name = c("Antonia")
age = c(23)

# Verify if the previous object are from the character/numeric classes
is.character(name)
is.character(age)
is.numeric(age)
```

And here we can apply the same principle to the re-ordering:
```{r}
dat <- dat %>%
  select(
    # Select categorical columns
    where(is.character), 
    # Select numerical columns
    where(is.numeric)
  )

# OR you can use the indexes too, but if you more than 10-20 columns, that is not ideal
# dat = dat[,c(8,6,11,9,10,14,15,2,5,1,3,4,12,13)]
```

Now lets look at our cleaned data:

```{r}
summary(dat)
```

Hold up, the ordering and selection of columns looks right, but it seems that there are certain rows that have missing values still (like `glyhb` column has 3 `NA` values still).
Lets remove all rows with any missing value using `na.omit()`.
Remember, 1 row = 1 patient.  

```{r}
dat = dat %>%
  na.omit()
```

> How many patients were removed because they were associated with missing values?

Now our cleaned data has no missing values, columns are cleanly ordered and each column is in the right format

```{r}
summary(dat)
```

> Can you identify which types of data (continuous, discrete etc) each column above represents and why?

------------------------------------------------------------------------

### Visualizing data distribution

In this section you will also learn the essential functions to plot data in an intuitive and useful way using the `ggplot2` package, just like in the introductory section to tidyverse. 

#### Histograms

We can plot the column "stab.glu" as a histogram using the `hist()` function:

```{r, message=F}
ggplot(dat,
       aes(x = stab.glu)) +
  geom_histogram() +
  labs(x = "Stabilized Glucose concentration in blood",  # add labels to the x-axis
       title = "Glucose concentration")                  # add title
```

> Add the parameter `bins = 50` in the above lines of code (inside `geom_histogram`) and see what happens.
> Try different values for `bins` like `10, 20, 75, 100`. Can you interpret the differences?
> Is this a good or bad thing about histograms?

#### Density plots

For density plots, we use the `geom_density()` function to estimate the probability density function for a given variable.

```{r eval = TRUE}
ggplot(dat,
       aes(x = stab.glu)) +
  geom_density() +
  labs(x = "Stabilized Glucose concentration in blood",  # add labels to the x-axis
       title = "Glucose concentration")                  # add title
```

#### Boxplots

The `boxplot()` function produces a boxplot for a given variable:

```{r fig.width=4, fig.height=2}
ggplot(dat,
       aes(x = stab.glu)) +
  geom_boxplot() +
  labs(x ="Stabilized Glucose concentration in blood")
```

> Can you explain all features of this graph, such as upper/lower whisker, 25% quantile, ...? 

#### QQ-plots

We can use **QQ-plots** to either (1) compare two distributions, or (2) compare a distribution with a theoretical distribution (typically the normal distribution).

We can for example compare the distribution of the blood pressure values to check if they are normally distributed

```{r eval=TRUE}
## Let's first make a histogram
ggplot(dat,
       aes(x = bp.1s)) +
  geom_histogram(bins = 50)
```

Now we can use the function `geom_qq()` to generate the **QQ-plot** of this distribution against the standard normal distribution:

```{r eval = TRUE, fig.width=3, fig.height=3}
ggplot(dat,
       aes(sample = bp.1s)) +     # we use sample= inside aes for the QQ-plot
  geom_qq()                       # creates the QQ-plot
```

Using the additional command `geom_qq_line()`, we can add a straight line that goes through the first and third quartile:

```{r fig.width=3, fig.height=3}
ggplot(dat,
       aes(sample = bp.1s)) +     # we use sample= inside aes for the QQ-plot
  geom_qq() +
  geom_qq_line(colour = 'red')                  # adds in the QQ-line on top
```

> So, is the distribution normal??

Now let's compare the quantiles of the cholesterol values by biological sex.
**Notes on `ggplot()` here:** Rather than `ggplot(dataset, aes(...))` we use `ggplot() + geom_xx(aes(...))` for situations where the data we wish to plot is not in a dataframe.

```{r fig.width=3, fig.height=3}
# We can use "filter()" to filter the cholesterol values for men and women
dat.male = dat %>%
  filter(gender == 'male')

dat.female = dat %>%
  filter(gender == 'female')

# Compute the quantiles (note the "na.rm" option to ignore missing NA values!)
q.male = quantile(dat.male$bp.1s, 
                  probs=seq(0,1,by=0.05), 
                  na.rm=TRUE)
q.female = quantile(dat.female$bp.1s, 
                    probs=seq(0,1,by=0.05),
                    na.rm=TRUE)

# Now plot against each other!
ggplot() +
  geom_point(aes(x = q.male, y = q.female)) +
  labs(title = "Quantiles", x = "Male quantiles", y = "Female quantiles")
```

------------------------------------------------------------------------

### Correlation  

#### Measuring the centrality in data

Before you begin, think back to the mean, median and quantiles we saw on the boxplot. Do you remember what these terms mean? How does an asymmetrical distribution influence mean and median? 
We have already seen that the `summary()` and `quantile()` functions in R can compute the mean, median and quantiles of any given data. 

```{r, eval=F, results='hide'}
mean(dat$stab.glu) 
median(dat$stab.glu) 
quantile(dat$stab.glu) 
```
> Calculate the mean and median of other continuous numeric data in the diabetes dataset and measure the difference between them. (a) Why is there a difference between the mean and median? (b) Why do you think there are larger differences for some and almost no difference for others?  


#### Association between variables

Often a common step during any data analysis project is to find associations between variables present in the dataset. Such associations helps us to decipher the underlying structure in the data.  

For instance, in our diabetes dataset we would expect a high correlation between free blood glucose levels and glycosylated blood levels or between waist and hip sizes. One of the most common ways of measuring associations is *correlations*. 

Let us start by producing a **scatter plot** between a pair of variables:

```{r}
ggplot(dat,
       aes(x = stab.glu, y = glyhb)) +
  geom_point() +
  labs(x='Stabilized glucose', y='Glycosylated hemoglobin')
```

> Do you suspect that the two variables have a relationship? Do the scatter plot for other pairs of numerical variables!

We now can compute the correlation of the two variables. We can compute the **Pearson correlation** or the **Spearman correlation**:

```{r}
## compute the Pearson correlation
cor(dat$stab.glu, dat$glyhb, method='pearson')

## compute the Spearman correlation
cor(dat$stab.glu, dat$glyhb, method='spearman')
```

The Spearman correlation seems much lower, right? To understand why, we can do a scatter plot between the **ranks** of the two variables:

```{r}
ggplot(dat,
       aes(x = rank(stab.glu), y = rank(glyhb))) +
  geom_point() +
  labs(x='Rank - Stabilized glucose', y='Rank - Glycosylated hemoglobin')
```

Do you understand the usage of ranks here? Run `rank()`` on a vector like *c(3,5,10,1,23)* to see how the output looks like.

> Associations are among the simplest forms of structure in the data! It is important to remember that *Association does not imply correlation* and *Correlation does not imply causation*. Take a look at this page to view few common logical fallacies. [see here](https://en.wikipedia.org/wiki/Fallacy)

------------------------------------------------------------------------

### EXERCISES

#### Exercise 1: Data features

1.  Try to obtain the same result of the `head()` function by using slicing on the dataset "dat" using row indexes.

2.  Print out the last element in the last column of "dat" using the `dim()` function instead of using numerals.

#### Exercise 2: Visualization and correlation

1.  Visualize the cholesterol levels of all patients with a histogram using the `geom_histogram()` function.

2.  Visualize the cholesterol levels of all **male** patients with a histogram using `geom_histogram()`.
    *(expert):* Mark the median, first and third quartile with vertical lines using `geom_vline()`. The values are defined using `xintercept` (as in which value of x should the vertical line intercept).
    Then mark median, first and third quantile for **female** patients in the same graph with a different color.
    What can you tell from the differences in these values?

3.  Is there an association between "hip" and "waist" on the data frame "dat"? Use the `geom_point()` function to do a scatter plot of the **values** and of the **ranks** (as determined by the `rank()` function). Compute both the **Pearson** and **Spearman** correlation values.
    
#### Going further *(expert)*

1.  Select only the numerical columns (using `select(where(is.numeric))`), and apply the function `cor()` **directly**: `%>% cor()` . What happens? What is the output? Store the result of this command in a new variable named "all.cor". Plot a heatmap of these results using the `pheatmap()` function from the "pheatmap" package. Remember that you first have to install and then activate this package using `library("pheatmap")`.

2.  Find the **highest** and **lowest** Pearson correlation value of the result from exercise 2.2. To which pair of variables do they correspond? Plot the corresponding scatter plots using `geom_point()`! *Hint*: Before finding the highest Pearson correlation value, use the `diag()` function to set all diagonal values (=1) of "all.cor" to NA.


## Day 2: Dimensionality reduction and unsupervised learning

### Preparing the data

**Unsupervised clustering** is one of the most basic data analysis techniques. It allows to identify groups (or clusters) or observations (here: patients) or variables. *Unsupervised* means that you are not using any prior knowledge about groups of variables or associations. **K-means clustering** is a good example of unsupervised learning because the method categorizes sample based uniquely on the data.

In this part, we will use a dataset of gene expression data from the TCGA (The Cancer Genome Atlas) project. This project has sequenced several thousand samples from cancer patients of more than 30 cancer types. We will use a subset of this data, containing 200 samples (=patients, as columns) , for which the expression of 300 genes (= rows) has been measured.

#### Load data

We will start by reading the gene expression data. The columns are the samples and the rows are the genes. This is matrix, which allows some numerical operations to be conducted directly. 

```{r}
brca.exp = readRDS(url('https://www.dropbox.com/s/qububmfvtv443mq/brca.exp.rds?dl=1'))
dim(brca.exp)
```

**WARNING**: If you have problem loading the data, please download [this file](https://www.dropbox.com/s/qububmfvtv443mq/brca.exp.rds?dl=1), store it on your disk, and open it with the following command:

```{r}
#brca.exp = readRDS("xxxx") # xxxx should be replaced with the path to the downloaded file in your device
```

Next we will load the clinical annotation file for this gene expression data and explore it

```{r}
brca.anno = readRDS(url('https://www.dropbox.com/s/9xlivejqkj77llc/brca.anno.rds?dl=1'))
head(brca.anno)
```

Same here: if you have issues running the previous `readRDS` command, download [this file](https://www.dropbox.com/s/z6bzwzgzdhky1qz/brca.anno.rds?dl=1), save it on your disk and load it with

```{r}
### brca.anno = readRDS(xxx)
```

You can check the number of samples for each tumor type using the `table()` function, applied to a specific column (here, there is only one column...)

```{r}
table(brca.anno$HER2_status)
```

#### Data transformation

You will see that the distribution of the data is extremely squeezed due to **outliers** with very high or low values. We will need to make the data more homogeneous, so that our downstream analysis is not affected by these very large magnitude numbers.

We will carry out the following data processing steps. Some of these steps use rather arbitrary values, which come from visually inspecting the data!

1.  **Thresholding:** cap the values to the 95th percentile

2.  **Homogenization:** base-2 logarithmic transformation of the entire dataset

3.  **Scaling:** standardize the data so that across genes the mean = 0 and variance = 1.

**Before we start modifying the data, we will store the original data frame into a variable, so that in case of problems we can revert back to the initial data!!**

```{r}
brca.exp.original = brca.exp # keeps the original as matrix
```

**Thresholding**

```{r}
## what is the value of the 95th percent percentile?
q95 = quantile(brca.exp,probs=0.95)

## set all values above to this value
brca.exp[brca.exp>q95] = q95
```

**Homogenization and Scaling**

We will perform this step by log-transforming the data. We are able to use this operation because the data is still in a matrix.
```{r}
brca.exp = log2(brca.exp+1)
```

> Why do we add +1 ?

Next, we will scale the data and plot its distribution. To do this efficient, we need to convert the data to tibble first, make it tidy, and then plot. 

Conversion to tibble can be done using `as_tibble(brca.exp, rownames = NA)`, where `rownames = NA` is meant to keep the original rownames (in this case, gene names) in the new tibble, although they are invisible. 

Check this out:
```{r}
rownames(as_tibble(brca.exp, rownames = NA))[1:5]
```

In addition, `gather(key = "sample", value = "expression")` converts the tibble to a long format, where "sample" represents the original column names, and "expression" represents the values present in the initial matrix.

```{r, message=F}
## scaling
brca.exp = scale(brca.exp)

## plotting the density
as_tibble(brca.exp, rownames = NA) %>%
  gather(key = "sample", 
         value = "expression") %>%
  ggplot(aes(x = expression)) +
  geom_histogram() +
  labs(title = "Transformed data")
```

Compare to the density plot before these pre-processing steps using the same strategy.

```{r, message=F}
as_tibble(brca.exp.original, rownames = NA) %>%
  gather(key = "sample", value = "expression") %>%
  ggplot(aes(x = expression)) +
  geom_histogram() +
  labs(title = "Untransformed data")
```

------------------------------------------------------------------------

### k-means clustering

Another widely used method for grouping observations is the k-means clustering. Now we will cluster our dataset using k-means and explore the underlying structure of the data. In this dataset, different clusters could represent different batches, different tumour subtypes, among other features.

#### Performing k-means

We use the function `kmeans()` in R on our matrix. You can check the options and usage in the help panel on the right. The parameter `centers` indicates how many clusters are requested.

```{r}
km = kmeans(x=t(brca.exp), 
            centers = 2, 
            nstart = 10)
```

> Just type `km` in your console and check all results generated. Play around with the `centers` parameter. See cluster assignments by typing `table(km$cluster)`

#### Quality of the clustering

We can judge the quality of the clustering by computing the **intra**-cluster distances, i.e. the sum (squared) of all distances between pairs of objects belonging to the same cluster. This is called the **within sum of squares (WSS)**. The better the clustering, the smaller WSS should be. However, it also automatically decreases with increasing k.

> What would be WSS if we request a number of clusters equal to the number of data points? You can check what the WSS is for a particular clustering by typing

```{r}
km$tot.withinss
```

> run k-means for k=2 to k=7 clusters, and for each k check the WSS value. How does WSS evolve with increasing k?

We can also run a little loop to test different k. Loops are very important structures in any programming language. 
We can test a simple scenario before. Check how the output of this simple loop looks like.

```{r}
k_to_test = c(2:7)

for (i in 1:length(k_to_test)) {
  print(i) # We can print the indexes
}

for (i in 1:length(k_to_test)) {
  print(k_to_test[i]) # or the actual elements
}
```

> Do you understand the difference between the 2 previous for loops? Try to make your own for loop. 

Now we can make one to test k from 2 to 7:
```{r}
km_wws = numeric() # we start by creating an empty vector

# To write in the position 1, we use i
# to find the 1st element, we use k_to_test[i]
for (i in 1:length(k_to_test)) {
  km_wws[i] = kmeans(x=t(brca.exp), 
                     centers = k_to_test[i])$tot.withinss
}

# We can plot the k against WSS using geom_line
ggplot() +
  geom_line(aes(x = k_to_test, y = km_wws)) +
  labs(x="Number of clusters K",
       y="Total within-clusters sum of squares")
```

> Do you see an obvious "elbow" or "kink" in the curve?? Another criteria for the quality of the clustering is the **silhouette** method.

To run the silhouette method, we need to compute the pairwise distances between all objects (i.e. patients) in the data matrix. This is done with the `dist` function, which can take different metrics (euclidean, ...)

```{r}
## compute the patient-patient distance matrix (this is why we transpose using the `t()` function)
D = dist(t(brca.exp))
```

We now compute the silhouette for a specific k-means clustering:

```{r, warning=FALSE}
library(cluster)
km = kmeans(x=t(brca.exp), centers = 3, nstart = 10)
s = silhouette(km$cluster,D)

# Let us use the basic R function plot() to see the results
plot(s)
```

------------------------------------------------------------------------

### Hierarchical clustering

Clustering is a method by which we group together similar observations while separating out the dissimilar ones. We will cluster our samples from the cancer dataset to see which samples cluster together or separately. Hierarchical clustering does not generate discrete clusters of datapoints, but rather creates a dendrogram that indicates the magnitude of similitude between samples. Once again is up to the Data Scientist to decide the right amount of clusters.

#### Determine the most variable genes

When performing clustering, we usually reduce the number of genes used, as some of them are not informative. For example, genes that show a mostly constant expression across all samples will not be useful to distinguish the samples, right? One simple method is to select genes that show a **high variance** across all samples.

```{r}
brca.exp.tibble = as_tibble(brca.exp, rownames=NA) %>%
  rownames_to_column("gene")

## create a new column with the variance for all genes across all samples
brca.exp.var = brca.exp.tibble %>%
  rowwise() %>%
  mutate(variance = var(c_across(starts_with("TCGA")))) 
# only includes the columns starting with TCGA
```

We now want to find the top 25% with the highest variance

```{r}
## what is the 75% quantile of the variance?
q75 = quantile(brca.exp.var$variance, probs = 0.75)
q75
```

So let us select all rows (genes) with a variance higher than or equal to `q75`:

```{r}
## only select the genes with a variance in the top 25%
topVariantGenes <- brca.exp.var %>%
  filter(variance >= q75)

print(topVariantGenes$gene)
```

#### Computing the correlation between all patients

Let us start by filtering for only the highly variable genes. Then we can directly calculate Spearman correlation.
```{r}
brca.exp.highvar.cor = brca.exp.tibble %>%
  filter(gene %in% topVariantGenes$gene) %>%        # from the whole list, select only high variable
  select(where(is.numeric)) %>%                     # get only numerical columns
  cor(method="spearman")                            # create correlation-based distance matrix
```

If we want to display the correlation matrix as a heatmap, we can use the `pheatmap` function as before:

```{r}
library(pheatmap)
pheatmap(brca.exp.highvar.cor, 
         show_rownames = FALSE, 
         show_colnames = FALSE)
```

Each cell of this heatmap represents the correlation value between the sample in the row and the sample in the column. The correlation of a sample to itself is always 1 (red diagonal).

The function automatically determines the clustering trees of the rows and columns (which are identical, since the correlation matrix is symmetrical!)

#### Including clinical annotations in the heatmap

This is a nice representation, but in order to interpret this clustering, we need to add some additional (clinical) information to interpret the clustering structure. To do this, we use an annotation data frame containing as columns a number of clinical features.

The clinical annotation is stored in the `brca.anno` data frame.
We can now plot again the heatmap, using the annotation dataframe to add additional information

```{r, fig.width=8, fig.height=7}
pheatmap(brca.exp.highvar.cor,
         annotation_row = brca.anno,
         show_rownames = FALSE, 
         show_colnames = FALSE)
```

> How would you interpret this dendrogram? Do the clusters you observe make any sense? What are the parameters by which the samples cluster together? How many meaningful clusters can you observe? Do you see any relation between the distribution of the data and your clusters ?

> The function `pheatmap` accepts a parameter clustering_method to indicate alternative linkage methods; try other linkage methods (check which are available with the pheatmap help page, which can be accessed by typing `?pheatmap` in the console!)


------------------------------------------------------------------------

### Principal component analysis

We will now use principal component analysis to explore the same dataset, and identify directions (i.e. principal components) with maximal variance. Principal components analysis finds n-dimensional vectors (Principal Components) in the direction of the largest variance, thereby allowing you to describe an n-dimensional dataset with just a few dimensions.

```{r}
pca = topVariantGenes %>%
  select(where(is.numeric)) %>%
  t() %>% # do not forget to transpose the data!
  prcomp(center = FALSE, scale = FALSE) # We set these as false as we have already scaled our data
summary(pca)
```

> How many principal components do you obtain? Compare this to the dimension of the matrix using the `dim()` function!

> What would happen if you would not transpose the matrix with t(...) in the prcomp function?

Principal components are ranked by the amount of variance that they explain. This can be visualized using a **scree plot**, indicating how much variance each PC explains: the **standard deviation** explained by each principal component is contained in the `pca$sdev` vector:

```{r}
pca$sdev
```

We see that the standard deviation is indeed going down! Let us now plot the proportion of total variance explained by each PC

```{r}
variance = (pca$sdev)^2
prop.variance = variance/sum(variance)
names(prop.variance) = 1:length(prop.variance)

# We make a data.frame from the prop.variance and the PC it corresponds to 
# we can obtain the PCs using names()
data.frame(proportion = prop.variance, 
           PCs = as.numeric(names(prop.variance))) %>%
  ggplot(aes(x = PCs, y = proportion)) +
  geom_col() +                               # to make the barplot
  labs(y='Proportion of variance')           # we only plot the first 20 PCs
```

Principal component analysis represents each data point (here: patient) in a new space in which the coordinates are principal components. Check the following output:

```{r, results='hide'}
head(pca$x)
```

We can now display the data points (i.e. patients) in the first two principal components. In addition, we can color the dots according to certain clinical parameters:

```{r, fig.width=6, fig.height=4}
# We start by creating a dataframe and combining it with the annotation
pca_with_annot = as.data.frame(pca$x) %>%
  merge(brca.anno, by = 0) # by = 0 makes use of the rownames as common information

## Now the object is in a ggplot2 friendly format
ggplot(pca_with_annot,
       aes(x = PC1, y = PC2, colour = ER_status)) +
  geom_point() +
  scale_colour_manual(values = c("grey", "red", "navy")) # scale_colour_manual can be used to change colours
```

> Choose different PCs for this plot. Can you still observe the two clusters corresponding to the ER_status of the patients? 

------------------------------------------------------------------------

### EXERCISES

#### Exercise 1: Variance

*For this exercise set it is given that all the data cleanup steps have been taken, you don't need to put them in the results.*

1.  Make a heatmap of the reduced matrix "topVariantGenes" using the `pheatmap()` function of the `pheatmap` library (do not forget to select only for numerical columns). Check for parameters that might change the style of the heatmap (column names, row names, etc..). How is this heatmap different from the heatmap in section 2?

2.  Repeat the selection of top variable genes (apply the same quantile used to generate "topVariantGenes"), but using the median absolute deviation (or MAD) using the `mad()` function instead of the `sd()` function, and store into as `brca.topMAD`

3.  Extract the gene names of `topVariantGenes` and `brca.topMAD` and check how many overlap using the `intersect()` function.

#### Exercise 2: Hierarchical clustering

In section 2, we have computed a correlation matrix, and used this matrix to build a clustering tree.

Try different linkage methods using the `clustering_method` parameter to see if the topology of the dendrogram changes!

2.  Try building a distance matrix which would lead to different topologies of the dendrogram, depending on which linkage method is used! Show the dendrograms built with different linkage methods!

#### Exercise 3: PCA

1.  Display the patients in the first two principal components (available in `pca_with_annot`) using `geom_point()`. Color the patients in the PCA plot according to `HER2_status`.

2.  (optional) Color the patients in the PCA plot according to `Classification`; you will probably need to define some more colors... You can check available colors [here](http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf)

#### Going further *(expert)*

Instead of performing the k-means on the whole gene expression matrix, we can run k-means on the space in which each patient is represented by the first k principal components.

1.  Run k-means with different numbers of clusters (1-10) on the patients using the first 2, 4, 6,... principal components (i.e. the first columns of `pca_with_annot`). Use the elbow method to evaluate how the within sum of squares (WSS) evolves. What is the optimal number of clusters? 

2.  Represent the patients in the PCA plot as previously, but color them according to the cluster they belong to! Run kmeans with two clusters for this and merge the k-means results.


## Day 3: Probability distributions

### Probability distributions

In the previous Exercise Sheet we have learnt more about unsupervised learning, like hierarchical clustering and especially PCA. These are among the fundamental methods of Data Analysis. Today we will learn more about another milestone of statistics: **probability distributions**.

![Various kinds of distributions and the relations among them.](figures/distrib.png)

[Figure source](https://medium.com/@srowen/common-probability-distributions-347e6b945ce4)

Distributions represented by sparse lines represent outcomes which will be discrete (for example the _roll of dice_ will always have discrete integer values from 1 to 6). Distributions represented by dense lines represent outcomes which can be continuous i.e real numbers (for example the height of people living in Heidelberg).

R has in-built functions for almost all probability distributions

```{r, eval=FALSE}
# Get the list of all probability distribution related functions
help(distributions)
```

All of these functions for probability distributions follow the same common scheme, the **root name** of the function prefixed by either of **p**, **d**, **q** and **r**. For example for the Normal distribution we have the four variants of function available - **pnorm**, **dnorm**, **qnorm** and **rnorm**.

Here, you can find some specific help on these functions:

```{r, eval=FALSE}
?rnorm
# or
?rpois
# etc ...
```

In the respective help documentation, you will find details on each of the functions.\
Probably, the most difficult to distinguish or to remember are **pnorm()** and **qnorm()** (respectively **ppois()** and **qpois()**, etc ...).\
We are going to look at them more deeply in the following.

------------------------------------------------------------------------

#### Getting to know the various functions

Let us get a grasp of what these functions actually do. You should be familiar with the cumulative distribution function, let's take a look at that and its inverse first. We will work with a Normal distribution. We calculate the **cumulative probability** for the values 1,2,3,4 in three different distributions, using one of the functions described previously.  
Short hint: **p** like **cumulative P-robability**. Which function are you going to use?

```{r echo=TRUE}
# Distribution 1: Mean = 2, Standard Deviation = 1
pnorm(1:4,mean=2,sd=1)

# Distribution 2: Mean = 2, Standard Deviation = 2
pnorm(1:4,mean=2,sd=2)

# Distribution 2: Mean = 4, Standard Deviation = 1
pnorm(1:4,mean=4,sd=1)
```

> Do you understand why the cumulative distribution functions change the way they do? 

Now, on the same distributions, we calculate the **inverse cdf** (inverse cumulative distribution function) for the cumulative probabilities of 25%, 50% and 75%. We use the `qnorm()` function for this (**q- like quantile**): 

```{r}
# Distribution 1: Mean = 2, Standard Deviation = 1
qnorm(c(0.25,0.5,0.75),mean=2,sd=1)

# Distribution 2: Mean = 2, Standard Deviation = 2
qnorm(c(0.25,0.5,0.75),mean=2,sd=2)

# Distribution 3: Mean = 4, Standard Deviation = 1
qnorm(c(0.25,0.5,0.75),mean=4,sd=1)
```

> Try with 100% on any of the distributions. Can you explain this result? Do you expect the result to be different in the other ones?


Now that you know the output of the p- and q- functions, let's look at **d- like density probability** functions. For any continuous distribution, the value of the function at any specific value is 0. This is why this probability function is used in discrete distribution such as the binomial distribution (function **dbinom()**). We first calculate the probability of getting 5 events out of 5 in a binomial distribution with a probability of 0.5. Then, we calculate the odds of **not** getting 5 out of 5.

```{r}
# probability of 5 out of 5
dbinom(5, size=5, prob=0.5) # size = number of trials; prob = probability of success on each trial

# probability of NOT getting 5 out of 5
1-dbinom(5, size=5, prob=0.5) 
# or
pbinom(4, size=5, prob=0.5) # Remember, pbinom() returns the "cumulative probability" of ...
```

> What is the probability of getting 5 out of 10? And NOT getting 5 out of 10? 


Suppose that the distribution of body height is described by a **normal distribution** with **mean = 1.75 m** and standard deviation **sd = 0.15**. What is the probability that someone is taller than 1.9 m? We can use the parameter **lower.tail** of the function `pnorm()` for this.  
What is the probability that someone is smaller than 1.60m?

```{r}
## taller than 1.9
pnorm(1.9, mean=1.75, sd=0.15, lower.tail=FALSE)
# or
1-pnorm(1.9, mean=1.75, sd=0.15)

## smaller than 1.6
pnorm(1.6, mean=1.75, sd=0.15, lower.tail=TRUE)
# equivalent to 
pnorm(1.6, mean=1.75, sd=0.15) # lower.tail is set to TRUE by default (check the help page using `?pnorm`)
```

Let's have a look at this distribution using the **dnorm()** function:

```{r}
x = seq(1, 2.5, by=0.01)
y = dnorm(x, mean=1.75, sd=0.15)

ggplot() +
  geom_line(aes(x = x, y = y),
            colour = "red", linewidth = 0.8) +
  geom_vline(xintercept = c(1.6, 1.9),
             linetype = 'dashed')
```

Let's finally look at **r- like random** functions. These, unlike the others, don't return single probabilities or values but rather generate a random distribution of values. We can use this to generate a normal distribution with mean = 10, sd = 5.

```{r fig.width=6, fig.height=4}
## normal distribution
x = rnorm(n=1000,mean=10,sd=5)

ggplot() +
  geom_histogram(aes(x=x), bins = 20,
                 fill = "orange") +
  labs(y = "Frequency") 
```

> Can you generate a Poisson distribution and a binomial distribution?

------------------------------------------------------------------------

#### Normal/Gaussian distribution

The normal or the Gaussian distribution is given as:

$$P(x) = \frac{1}{{\sigma \sqrt {2\pi } }} \cdot e ^ \frac{-(x- \mu)^2}{{2\sigma ^2 }} $$ where $\mu$ is the mean of the distribution and $\sigma$ is the standard deviation.

The **standard normal distribution** is a special case of **normal distribution** where the values for $\mu = 0$ and $\sigma = 1$. Thus, the above equation for the Normal distribution simplifies to:

$$P(x) = \frac{1}{{\sqrt {2\pi } }} \cdot e ^ \frac{-x^2}{2} $$ Now for any $x$ we can easily solve this equation since $\pi$ and $e$ are known constants.

------------------------------------------------------------------------

#### Visualization

Let's generate three random normal distributions with different means and standard deviations and visualize them together

```{r, message=F}
## Use the function `dnorm()` to plot the density distribution
x = seq(-10, 30, by=.1)
d1 = dnorm(x, mean=0, sd=1)
d2 = dnorm(x, mean=10, sd=1)
d3 = dnorm(x, mean=20, sd=1)

# Compare with the histogram build from 1000 random number drawn from the standard normal distribution
r1 = rnorm(n=1000, mean=0, sd =1)  # random distributions of values
r2 = rnorm(n=1000, mean=10, sd=1)
r3 = rnorm(n=1000, mean=20, sd=1)

# Histogram visualization
# We will start by converting the rnorm outputs into one single df
data.frame(vals = c(r1, r2, r3), # column vals is actual numbers
           Specs = c(rep("mean=0, sd=1", 1000), # column Specs is mean/sd 
                     rep("mean=10, sd=1", 1000),
                     rep("mean=20, sd=1", 1000))
             ) %>%
  ggplot(aes(x = vals, 
             colour = Specs)) +
  geom_histogram(bins = 50) + 
  geom_freqpoly(bins = 50) # geom_freqpoly adds in the line on top of the histogram
```

> Play with the mean and sd parameters and visualize the distributions (plain lines) as well as the corresponding histograms. 

------------------------------------------------------------------------

#### Application on a real dataset

Now we will use the Normal distribution to make predictions about gene expression of TP53 in lung cancer. TP53 is the most commonly mutated gene across multiple cancer types especially in lung cancers. We will read a table (import) containing measurements of TP53 expression levels in 586 patients.

```{r}
tp53.exp = read.table("https://www.dropbox.com/s/rwopdr8ycmdg8bd/TP53_expression_LungAdeno.txt?dl=1", 
                      header=T, sep="\t")[,1:2]
summary(tp53.exp)
```

##### Data cleaning and central values

We will remove all the missing values and calculate the mean and standard deviation for the TP53 gene expression.

```{r}
tp53.exp = tp53.exp %>%
  na.omit() 

m.tp53 = mean(tp53.exp$TP53_expression) # mean
m.tp53

s.tp53 = sd(tp53.exp$TP53_expression) # standard deviation
s.tp53
```

##### Modeling using a normal distribution

Let's see how well a normal distribution with $\mu = 1380.822$ (m.tp53) and $\sigma = 719.5934$ (s.tp53) can approximate the **real** distribution of TP53 expression.  
We assume that the population mean and standard deviation is similar as calculated above since we cannot measure the expression of TP53 in each and every lung cancer patient in the world.

```{r}
# distribution of the measured data
ggplot(tp53.exp,
       aes(x = TP53_expression)) +
  geom_density() + 
  xlim(-1500, 6000)
```

Make a normal distribution with the above parameters
```{r}
x = seq(0,5000,by=5)
d.pred = dnorm(x, mean = m.tp53, sd = s.tp53)

# Now plot both, predicted and measured data
ggplot() +
  geom_density(aes(x = tp53.exp$TP53_expression)) +
  geom_line(aes(x = x, 
                y = d.pred),
                colour = "red") +
  geom_vline(xintercept = c(quantile(tp53.exp$TP53_expression, probs = c(0.1, 0.9)))) +
  geom_vline(xintercept = c(qnorm(p = 0.1, mean = m.tp53, sd = s.tp53), 
                            qnorm(p = 0.9, mean = m.tp53, sd = s.tp53)),
             colour = 'red', linetype = 'dotted') 
```

##### Data prediction using the normal distribution model

Using a normal distribution with $\mu = 1380.822$ (m.tp53) and $\sigma = 719.5934$ (s.tp53), we will ask the following questions -

-- **(Q1)** What is the probability of observing the expression of TP53 to be **less** than 1000?
```{r}
pnorm(q=1000, mean =m.tp53, sd = s.tp53) # returns the cumulative probability
```

-- **(Q2)** What is the probability of observing the expression of TP53 to be **greater** than 1000?

```{r}
1 - pnorm(q=1000, mean =m.tp53, sd = s.tp53) 
# is same as
pnorm(q=1000, mean =m.tp53, sd = s.tp53, lower.tail = FALSE) # or pnorm(q=1000, mean =m.tp53, sd = s.tp53, lower.tail = F)
```

##### Evaluating the quality of the predictions

Let's check how good these predictions are compared to **real** data.
-- **(Q1)**  What is the probability of observing the expression of TP53 to be **less** than 1000?

```{r}
sum(tp53.exp$TP53_expression < 1000)/nrow(tp53.exp)
```

-- **(Q2)** What is the probability of observing the expression of TP53 to be **greater** than 1000? 
```{r}
sum(tp53.exp$TP53_expression > 1000)/nrow(tp53.exp)
```

> I would say those predictions are pretty good !! Now, let's try to break this model. Re-execute the code above with different $q$ values `q=100, q=500, q=4000, q=4500` etc. At what values do you think the model would not perform well. HINT: Look at the tails of the distribution!

```{r}
# What is the probability of observing the expression of TP53 to be less than q?
q = c(100,500,1000,4000,4500)

# for loop used to calculate and store the predicted (a) and real (b) values
pred = numeric()
meas = numeric()
for (i in 1:length(q)){
  pred[i] = pnorm(q=q[i], mean = m.tp53, sd = s.tp53) 
  meas[i] = sum(tp53.exp$TP53_expression < q[i])/nrow(tp53.exp)
}

# Change into a dataframe 
model_mat = data.frame(pred, meas, q)
model_mat
```

Again, using a normal distribution with $\mu = 1380.822$ and $\sigma = 719.5934$, what if we ask what is the value of TP53 expression at the 10% and 90% quantiles:
```{r}
qnorm(p = 0.1, mean = m.tp53, sd = s.tp53)
qnorm(p = 0.9, mean = m.tp53, sd = s.tp53)
```

Let's check how good these predictions are compared to our real data.

```{r}
quantile(tp53.exp$TP53_expression, 
         probs = c(0.1, 0.9))
```

Again the predictions are pretty good!


##### Visualization

We can also visualize all of this on a simple graph:

```{r}
# Model prediction:
x = seq(0,5000,by=5)
d.pred = dnorm(x,mean = m.tp53, sd = s.tp53)

# Model and measured data and predicted versus measured 0.1 and 0.9 quantiles:
ggplot() +
  geom_line(aes(x = x, 
                y = d.pred),
            colour = "red") +
  geom_density(aes(x = tp53.exp$TP53_expression)) 

```

Compare the black and red vertical lines (real vs predicted).

> Re-execute the code above with `p=0.25, p=0.5, p=0.75 etc` and check how good the predictions are.

##### Visualization using a Q-Q plot

Now, let's plot the sample quantiles against theoretical quantiles to check the similarity between the two. This is called a quantile - quantile plot or a Q-Q plot, which you are familiar with (see Exercises Sheet 1).

```{r fig.height=5, fig.width=5, warning=FALSE}
q = seq(0,1,0.01) # Creating a vector of quantiles

# Find values corresponding to these quantiles in the real data
q.observed = quantile(tp53.exp$TP53_expression, probs = q)

# Find values corresponding to these quantiles in the theoretical normal distribution
q.theoretical = qnorm(p = q, mean = m.tp53, sd = s.tp53)

# # Correlate the above two values
ggplot(tp53.exp,
       aes(sample = TP53_expression)) +
  geom_qq() +
  geom_qq_line(colour = 'red')

## Would be the same as:
# ggplot() +
#   geom_point(aes(x = q.theoretical, 
#                  y = q.observed), size = 1) +
#   geom_abline(intercept = 0, slope = 1, 
#               size = 1, color = "red") 

```

------------------------------------------------------------------------

#### Binomial distribution

A binomial distribution can be defined as -

$$P(x) = \frac{n!}{x!(n-x)!}\cdot p^x \cdot (1-p)^{n-x}$$ 

Where $x$ is the number of successes out of $n$ experiments and $p$ is the probability of success.

-   $mean = n \cdot p$
-   $variance = np \cdot (1 - p)$
-   $sd = \sqrt{np \cdot (1 - p)}$

The design of the experiment is as follows -

-   The experiment is repeated and are independent of one another
-   Each experiment has just two outcomes
-   The probability of success is constant and does not change with experiments

We can for example compute the probability of having 7 heads in a series of 10 throws of a coin:

```{r}
# x = number of successes; size = number of trials; prob = probability of success on each trial
dbinom(x=7, size=10, prob = 0.5) 
```

Or we can compute what the probability is to get 7 or more heads using the function `pbinom()`. Remember that the parameter "lower.tail" is used to specify whether to calculate the probability of observing x or fewer successes (if `lower.tail = TRUE`) or the probability of observing more than x successes (`lower.tail = FALSE`):

```{r}
pbinom(6, size=10,
       prob=0.5,
       lower.tail=FALSE) 
```

Beware that this syntax means **strictly more than 6**, i.e. 7 or more!!

> How would you compute the probability to get less than 5? What would `qbinom(0.3,size=10,prob=0.5,lower.tail=FALSE)` represent?


------------------------------------------------------------------------

### Confidence interval

The **confidence interval** describes the interval containing the (unknown) expectation value of a distribution with 95% confidence. This means that out of 100 random realizations of this random variable, the true expectation value $\mu$ will indeed be in this interval.

Let us try a simulation: we consider a random variable distributed according to a Poisson distribution $$P(x) = \frac{{e^{ - \lambda } \lambda ^x }}{{x!}}$$ Here, *we know the true value of the expectation value*. We want to get an estimate for $\lambda$, and check if the confidence interval contains the true expectation value.

For example, a farmer expects to collect 75 eggs from his hens per hour.

```{r}
lambda = 75
```

He now collects during 100 days the eggs $N=8$ times a day (each time during one hour). We want to compute the mean $m_N$ over these $N=8$ realizations and determine the 95% confidence interval, and check, how often the expectation value $\mu$ is inside the confidence interval.

Remember that the 95% CI is given by $$[m_N-t_{95,N-1}\frac{\sigma}{\sqrt{N}},m_N+t_{95,N-1}\frac{\sigma}{\sqrt{N}}]$$ where $t_{95,N-1}$ is the critical value for the $t$-distribution with $n-1$ degrees of freedom.

Let's start by creating our samples:

```{r}
# size of the sample
N = 8

# we now draw 100 times samples of size N=8
## rpois is the function used to generate the Poisson distribution
X = lapply(1:100, function(i) {
  rpois(N, lambda = lambda)
  })
```

`lapply()` is a function in R that stands for "list apply". It is used to *apply* a function to each element of a list or vector and produces a list with the same length as an output.  

In this previous example, the input is a vector (`1:100`). 
Then, we `lapply(v, function(i))`:
+  v: The list or vector you want to apply the function to.
+  function: The function you want to apply where `i` is each element in X. In this case, we are applying `rpois` 100 times.

As an output, `lapply` returns a list where each element has had the specified function applied to it. 

> Run View(X) to see how this object looks like. Try using lapply to obtain the mean of all elements in each X using `lapply(X, function(i){mean(i)})` or `lapply(X, mean)`. 

Now, we calculate the mean and the standard deviation of the respective samples:

```{r}
# we compute the sample means
Xm = sapply(X,mean)
# and the sample standard deviations
Xsd = sapply(X,sd) 
```

Next, we determine the upper and lower bounds of the 95% CI. Remember that the confidence interval is based on a $t$-distribution. The degrees of freedom of this distribution is the sample size -1 ($N$-1=7 in this case)

```{r}
df = N-1
tc = qt(c(0.975),df) # this is the critical value for the t-distribution for df = N-1 degrees of freedom and 95% CI

Xl = Xm-tc*Xsd/sqrt(N) # upper bound of the 95% CI
Xh = Xm+tc*Xsd/sqrt(N) # lower bound of the 95% CI
```

Finally, we determine whether each sample mean is found within the 95% CI or not:

```{r, fig.width=8, fig.height=4}
## vector of TRUE/FALSE if the real expectation value lambda is inside the interval
i.ok =  as.factor(Xl < lambda & Xh > lambda)

plot_data <- data.frame(
  n = 1:100,
  Xm = Xm,
  Xl = Xl,
  Xh = Xh,
  i.ok = i.ok
) 

# Plot using ggplot2
ggplot(plot_data, 
       aes(x = n, y = Xm, 
           color = `i.ok`)) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = Xl, ymax = Xh, color = i.ok), width = 0.2) +
  geom_hline(yintercept = lambda, linetype = "dashed", color = "black") +
  scale_color_manual(values = c("grey", "orange")) +
  labs(y = "Mean values",
       title = paste("Mean values and confidence intervals, N=",N),
       color = "95% CI Status") 
```


Here, the orange/grey bars represent the confidence interval, the dot is the mean of the sample values, and the dotted line at `\lambda` represents the true expectation value. Whenever the true expectation value is within the CI, the bar is orange, if not, the bar is grey. How often is the true expectation value outside the CI? Count the grey bars!

It happens `r sum(!as.logical(i.ok))` times, which fits pretty well with the expected 5%.

> Repeat this simulation, but now with samples of $N=24$ (again 100 times)\
> What do you observe?\
> How often is the true expectation value outside the CI? Change to 90% CI and check if that works!


------------------------------------------------------------------------

### EXERCISES 

#### Exercise 1: Probability distributions

1.  What is the expression of TP53 observed at 10th percentile? What is the expression of TP53 observed at 90th percentile?

2.  (optional) Other distribution types can become very similar to the normal distribution under certain conditions. Plot the histogram of 1000 random numbers drawn from the Poisson distribution with lambda = 1, 10, 100, 1000. What do you observe?

#### Exercise 2: Confidence intervals

You are buying 10 packs of gummy bears. You particularly like the red ones and the green ones. A pack contains 6 different colors and you expect them to be equally distributed. There are 84 pieces per 200g pack.

1.  What is the expected amount of red or green gummy bears?

2.  You selected your 10 packs according to the colors you could see in the pack. At home, you counted the following bears per pack:

-   for the red ones: 12 16 17 12 16 13 11 18 13 19\
-   for the green ones: 11 10 15 16 12 14 13 10 13 17\
    Was your selection procedure a success? In other words, is the expected value bellow (congrats!), within or above (bad luck!) the 95% CI?


## Day 4: Hypothesis testing

On this section, we will go through hypothesis testing. You will start to see how to *formulate hypotheses* and how to *test* them.
In addition, we want to learn how to use and interpret hypothesis tests.

We will work again with the diabetes dataset that we used previously.

```{r}
dat = read.delim('https://tinyurl.com/y4fark9g')

# set the row names using the column id
rownames(dat) = dat$id
```

Load the required packages
```{r, message=F, warning=F}
library(dplyr)
library(ggplot2)
library(tibble)
```

Check out the content of the dataset using the summary function
```{r}
summary(dat)
```

How can we inspect the differences between the weight of men and women? We can start by ploting two histograms or density plots representing the weight by biological sex. 
```{r, warning=FALSE}
ggplot(dat,
       aes(x = weight, 
           fill = gender)) +
  geom_density(alpha = 0.5)
```

The distributions look different in shape, right? Where do you think the mean would be located in the plot? 
```{r}
# We can use "filter()" to filter the cholesterol values for men and women
dat.male = dat %>%
  filter(gender == 'male')

dat.female = dat %>%
  filter(gender == 'female')

# we will calculate the mean weight by sex.
mean.men <- mean(dat.male$weight, na.rm = TRUE)
mean.women <- mean(dat.female$weight, na.rm = TRUE)
```

And we can then add in the means to the plot as vertical lines. 

```{r, warning=FALSE}
ggplot(dat,
       aes(x = weight, 
           fill = gender)) +
  geom_density(alpha = 0.5) +
  geom_vline(xintercept = mean.men, colour = "cyan") +
  geom_vline(xintercept = mean.women, colour = "salmon")
```

> Do you think that the mean of the male weight is lower than 180?
> Do you think that the mean of the female weight is **really** different from the mean of the male weights?
> Do you think that the mean of the male weight is higher than that of the females?

------------------------------------------------------------------------

### Tests of mean


Now, we want to use a statistical test to check if,

-   males have a mean weight that is **significantly** lower than a specific value (one-sample, one-tailed test)
-   there is a **significant** difference in the mean of the weights between the two groups (two-sample, two-sided test)
-   male have a **significantly** higher mean weight than females (two-sample, one-sided test)

Note the use of the word **significant** in the previous statements!

This is exactly what **mean tests** such as the t-test (or the Wilcoxon test) are designed for!
We will perform here a **t-test**.

------------------------------------------------------------------------

#### One sample t-test

Using a one-sample t-test, we can check if values differ significantly from a target value.
For example, you could sample 10 chocolate bars, and test if they significantly differ from the expected weight of 100 g:

> Should we perform a one- or two-sided test?

```{r}
bars = c(103,103,97,102.5,100.5,103,101.3,99.5,101,104) # weights of 10 chocolate bars
chocbar.mean = 100 # expected weight
```

The function `t.test()` offers three **alternative** options: *two.sided*, *less* and *greater*.
Here, if we want to test whether the mean weight of the 10 chocolate bars is **different** from the expected weight of 100 g, we want to perform a *two sided* test and use the alternative *two.sided*.

It is essential to **clearly formulate the H0 and H1 hypothesis**.
There are two alternative but equivalent ways to do so.
Either:

-   H0: the expectation value of the random variable "Weight of a chocolate bar" is **equal** to 100 g.\
-   H1: the expectation value of the random variable "Weight of a chocolate bar" is **different** from 100 g.

or


-   H0: the mean weight of a chocolate bar is not significantly different from 100 g.\
-   H1: the mean weight of a chocolate bar is significantly different from 100 g.

Note the difference between these two formulations, and ask for help if you have questions about this!

```{r}
t.test(x = bars, mu = chocbar.mean, alternative = "two.sided")
```

> How would you interpret this result?
> Can you reject the H0 hypothesis?

Using $\alpha=0.05$, the H0 hypothesis can not be rejected as the p-value is 0.05244 (p-value \>= 0.05).
With $\alpha=0.05$, the mean weight of the chocolate bars is not significantly different from 100 g.

Using $\alpha=0.1$, the H0 hypothesis can be rejected (p-value \< 0.1).
With $\alpha=0.1$, the mean weight of the chocolate bars is significantly different from 100 g.

**BUT** ... It does not mean that $\alpha$ should be chosen with respect to the results of the t.test!!!

Before running a t.test, **formulate the null hypothesis H0 and the alternative hypothesis H1** and **decide about the alpha** value.
Remember, **alpha** represent the **false positive rate**: under the H0 hypothesis (test of two identical distributions), this is the proportion of tests that will detect a difference between the two groups (p-value \< $\alpha$).

**Beware not to get confused between one-/two-sample tests, and one-/two-sided tests!**

Regarding the mean weight of the males, we would like to check whether males have a mean weight that is significantly **lower** than 180.
Here as well, we will perform a **one-sample test**, with mu = 180.
However, we will perform a **one-sided** test using the alternative option **less**.

The hypotheses can be formulated as:

-   H0: the expectation value of the random variable "Weight of male patients" is **equal or greater** 180.\

-   H1: the expectation value of the random variable "Weight of male patients" is **less** than 180.

```{r}
t.test(dat.male$weight, mu = 180, alternative = "less")
```

> How would you interpret the result with $\alpha=0.05$?
> Can you reject the H0 hypothesis?

------------------------------------------------------------------------

#### Two-sample t-test (2-sided)

Now, we will compare the mean of the weights between males and females.\

According to the previous histogram, females have a different mean weight as males.
This can be tested using a *two-sample and two.tailed* t.test.

```{r, warning=FALSE}
ggplot(dat,
       aes(x = weight, 
           fill = gender)) +
  geom_density(alpha = 0.5) +
  geom_vline(xintercept = mean.men, colour = "cyan") +
  geom_vline(xintercept = mean.women, colour = "salmon")
```

The hypotheses can be formulated as:

-   H0: the expectation value of the random variable "Weight of male patients" is **equal** to the expectation value of the random variable "Weight of female patients".\

-   H1: the expectation value of the random variable "Weight of male patients" is **different** from the expectation value of the random variable "Weight of female patients".

```{r}
t.test(dat.male$weight, 
       dat.female$weight) # remember, alternative = "two.sided" per default.
```

> How would you interpret the result with $\alpha=0.05$?
> Can you reject the H0 hypothesis?

------------------------------------------------------------------------

#### Two-sample t-test (1-sided)

Looking at the histogram, other observers could in principle see a difference between the mean values of the weights and formulate the following hypotheses:

-   H0: the expectation value of the random variable "Weight of male patients" is **equal or lower** to the expectation value of the random variable "Weight of female patients".\

-   H1: the expectation value of the random variable "Weight of male patients" is **higher** than the expectation value of the random variable "Weight of female patients".

```{r}
t.test(dat.male$weight, 
       dat.female$weight, alternative = "greater")
```

> How would you interpret the result with $\alpha=0.05$?

> Can you explain why the p-value of the one-tailed t.test is lower than the p-value of the two-tailed t.test?
> What is the relation between these two values?

```{r}
# p-value of the two-sided t-test versus p-value of the one-sided t-test: 
t.test(dat.male$weight, 
       dat.female$weight, alternative = "two.sided")$p.value # two-tailed
t.test(dat.male$weight, 
       dat.female$weight, alternative = "greater")$p.value # one-tailed
```

This can be visualized using the t-distribution.
Here (see above result of the t.test), *t = 1.8453* and *df = 372.45*.

In the **one-tailed t.test**, the p-value is the area under the curve for t \> 1.8453 (alternative greater) **OR** for t \< -1.8453 (alternative less).

```{r, echo=F, fig.height=3, fig.width=5}
# No need to understand this code - just look at the graph and shaded areas.
x = seq(-5,5,by=0.01)
y = dt(x,df=372.45)
z = dt(seq(1.84,5,0.01),df=372.45)

ggplot() +
  geom_line(aes(x = x, y = y), 
            linewidth = 0.8) +
  geom_vline(xintercept = 1.8453, colour = "blue",
             linetype = 'dashed') +
  geom_ribbon(aes(x = ifelse(x >= 1.84, x, NA), 
                  ymin = 0, ymax = y), 
              alpha = 0.8, fill = 'pink')
```

In the **two-tailed t.test**, the p-value is the area under the curve for t \> 1.8453 (alternative greater) **AND** for t \< -1.8453 (alternative less).
It is two times the p-value of the one-sided t.test!

```{r, echo=FALSE, fig.height=3, fig.width=5}
# No need to understand this code - just look at the graph and shaded areas.
x = seq(-5,5,by=0.01)
y = dt(x,df=372.45)

ggplot() +
  geom_line(aes(x = x, y = y), 
            linewidth = 0.8) +
  geom_vline(xintercept = 1.8453, colour = "blue",
             linetype = 'dashed') +
  geom_vline(xintercept = -1.8453, colour = "blue",
             linetype = 'dashed') +
  geom_ribbon(aes(x = ifelse(x >= 1.84, x, NA), 
                  ymin = 0, ymax = y), 
              alpha = 0.8, fill = 'pink') +
  geom_ribbon(aes(x = ifelse(x <= -1.84, x, NA), 
                  ymin = 0, ymax = y), 
              alpha = 0.8, fill = 'pink')
```

IMPORTANTLY, a t-test can only be performed if the data is **normally distributed**!
If the data is not normally distributed, you will need to use a non-parametric test, like Wilcoxon test. 


------------------------------------------------------------------------

#### Wilcoxon test

What if the data is not normally distributed?
In that case, we are not supposed to use the t-test for testing differences between mean values!
Let us us see an example.

Let us start by loading the data.
```{r}
all.aml = read.delim('http://bioinfo.ipmb.uni-heidelberg.de/crg/datascience3fs/practicals/data/all.aml.cleaned.csv',
                     header=TRUE)
all.aml.anno = read.delim("http://bioinfo.ipmb.uni-heidelberg.de/crg/datascience3fs/practicals/data/all.aml.anno.cleaned.csv",
                          header=TRUE) %>%
  mutate(id = paste0("pat", Samples))
```

We check whether the distribution of the expression values for the gene *SOX4* corresponds to a normal distribution:

```{r fig.height=5, fig.width=5.5, warning=FALSE}
expression = all.aml %>%
  t() %>%
  as.data.frame() %>%
  na.omit()

# Plot histogram 
ggplot(expression,
       aes(x = SOX4)) +
  geom_histogram(bins = 20)

# Check if it's normally distributed using a QQ-plot 
ggplot(expression,
       aes(sample = SOX4)) +
  geom_qq() +
  geom_qq_line(colour = 'red')
```

This looks everything but normal!
In that case, we cannot apply the t.test, but need to apply a **non-parametric test** called the *Wilcoxon test*.
This test is performed not on the *values* (like the t-test) but on the *ranks* of these values (remember the difference between the Pearson's and the Spearman's correlations!)

```{r, warning=FALSE}
# divide the gene expression data in two groups according to ALL or AML patients:
# obtain the AML and rest
aml.patient.id = all.aml.anno %>%
  filter(ALL.AML == "AML")
other.id = all.aml.anno %>%
  filter(ALL.AML != "AML") # filters for those which are not labeled AML

gene.all = expression %>%
  rownames_to_column("id") %>%
  filter(id %in% other.id$id)

gene.aml = expression %>%
  rownames_to_column("id") %>%
  filter(id %in% aml.patient.id$id)
  
# test for a difference in the mean expression values using the Wilcoxon test:
wilcox.test(gene.aml$SOX4, gene.all$SOX4)
```

Compare the obtained p-value with the p-value obtained if we would have used the t-test:

```{r}
t.test(gene.aml$SOX4, gene.all$SOX4)
```

The p-values are very different!!
So is the difference of expression between ALL and AML patients for this gene significant or not taking $\alpha=0.05$?

Here, we **cannot** trust the t-test due to the non-normality of the data!
Hence, the correct p-value is the one from the Wilcoxon test.

------------------------------------------------------------------------

### Proportion tests

The t-test and Wilcoxon tests are **tests of the mean**, meaning that we are **comparing the means of two samples and looking for significant differences**.

But there are other hypothesis that one might want to test, related to the relationship between two **categorical** variables:

-   is the proportion of men **significantly** higher in the patients from Louisa compared to the ones from Buckingham?
-   is the proportion of smokers under 18 in Germany **significantly** higher than in other European countries?

The proportion test (**Fisher Exact Test** or **chi-squared test**) are used to investigate the relationship between 2 categorical variables, starting from a **contingency table**.
We will use a dataset with clinical information about breast cancer patients.

```{r}
dat.brca = read.delim('http://bioinfo.ipmb.uni-heidelberg.de/crg/datascience3fs/practicals/data/gbsg_ba_ca.dat', 
                      stringsAsFactors = FALSE)
```

> Check which variables in this dataset are categorical/ordinal/numerical.

We can now check if there is a significant relationship between some variables.
For example, we can verify if the choice of treatment with tamoxifen (variable `hormon`) is related to the pre-/post-menopausal status (variable `meno`)

First, we can build the contingency table for these 2 variables:

```{r}
## build contingency table
table(dat.brca$meno, dat.brca$hormon)
```

We can compute the **odds-ratio (OR)** for these two variables.
This metric measures the strength of the association between two events, which in this case would be treatment with tamoxifen and menopausal status.

Here's how to interpret the odds ratio (OR) value:

-   **OR = 1**: events are independent   
-   **OR > 1**: events are positively correlated   
-   **OR < 1**: events are negatively correlated   

```{r}
CT = table(dat.brca$meno,dat.brca$hormon)

OR = (CT[1,1]/CT[1,2])/(CT[2,1]/CT[2,2])
OR
```

> How would the odds-ratio look like if you would transpose the matrix?

Now we can run the one-sided **Fisher Exact Test** (FET).
The H0/H1 hypothesis are:

-   H0: the odds-ratio is not significantly larger than one
-   H1: the odds-ratio is significantly larger than one

```{r}
## build contingency table
tab = table(dat.brca$meno,dat.brca$hormon)
#
## run the FET
fisher.test(tab, alternative = 'greater')
```

Check if your computation of the odds-ratio is right!
Compute also the **two-sided** test:

> Formulate the H0/H1 hypothesis!

```{r}
fisher.test(tab)
```

We can also use the **chi-square test** to answer the same question.
The Chi-square test compares the **observed** number of occurrences in the contingency table to the **expected** number of occurrences if there was no relationship between the variables.

-   H0: the observed and expected occurrences are not significantly different
-   H1: the observed and expected occurrences are significantly different

```{r}
chisq.test(tab)
```

Now we want to verify the impact of age on the grade of the tumor.
We categorize the patients in under and over 40 year groups, and perform a chi-squared test:

```{r}
## contingency table
tab = table(dat.brca$age>40,
            dat.brca$grade)
tab # tumor grades 1,2,3; age under 40 (FALSE) and over 40 (TRUE)
##
chisq.test(tab)
```

We can determine the table of expected counts using the `apply()` function. 
This works similarly to the `lapply`, but it can be used to *apply* a function by rows (if 1), columns (if 2), or both:

```{r}
tot = apply(tab,2,sum) # this is the total number of occurrences in the 3 categories of "grade", independently of "age"
tot

age = apply(tab,1,sum) # this is the total number of persons above/below 40, independently of "grade"
age
```

On the other hand, `sapply` is commonly used for dataframe or other tabular formats: 

```{r}
tot.proportions = tot/sum(tot) # HO proportions (the proportions of occurrences in the three categories of grade, independently of age)

tab.exp = sapply(tot.proportions,function(x) {x*age}) # expected counts under H0
tab.exp
```

> How would you compute the chi-square test statistic using the `tab` and `tab.exp` tables?


------------------------------------------------------------------------

### EXERCISES   

#### Exercise 1: One-sided t-test

1. Consider the following graph, formulate the hypotheses H0 and H1 and perform a (one-sided) t-test.
Interpret the result using $\alpha=0.05$.

```{r, warning=FALSE, echo=F}
mean.height.f = mean(dat.female$height, na.rm = T)
mean.height.m = mean(dat.male$height, na.rm = T)

ggplot(dat,
       aes(x = height, 
           fill = gender)) +
  geom_histogram(alpha = 0.5, bins = 20,
                 colour = "black") +
  geom_vline(xintercept = mean.height.m, 
             colour = "cyan", linetype = 'dashed',
             linewidth = 1) +
  geom_vline(xintercept = mean.height.f, 
             colour = "salmon", linetype = 'dashed',
             linewidth = 1)
```

2.  Calculate the mean age of the men.
3.  Compare it to age = 50. Formulate the hypotheses H0 and H1 and perform a (one-sided) t-test. Interpret the result using $\alpha=0.05$.

#### Exercice 2: Two-sided t-test

Can you find interesting differences in the mean values of parameters of the dataset **dat** for the two groups defined by the **location**?
Follow these steps:

1.  Select the two groups according to the location.
To do so, check the result of `distinct(location)` and create two tibbles corresponding to each of possible the locations.

2.  Calculate the mean values of the numerical parameters for each group (ex: age, height, weight, waist, hip, ...).
*Hint*: create first a dataframe with numeric columns only (use `select(which(is.numerical))`).
Select the rows corresponding to the two groups and use an `summarise()` and `group_by` loop to calculate the mean values (grouped by location).

3.  Select one of these, formulate the H0 and H1 hypotheses and perform a (two-sided) t-test.
Interpret the result ($\alpha=0.05$).

#### Exercise 3: Mean and proportion testing
What test would you use for the following questions?

-   A lotion company has to figure out whether their last product is more likely to give acne to men rather than women.
-   The department of education wants to find out whether social science students have higher grades than science students.
-   A biologist needs to find out whether a specific gene is more likely to be silenced in lactose intolerant people.


#### Going further: Checking the normality of the distribution

In principle, t-tests require that the data is approximately normally distributed.
If not, we can use **non-parametric** tests (see next lecture).

In order to check whether the data is normally distributed or not, it is possible to perform a **Shapiro-Wilk** normality test (see lecture).
This statistical test is implemented in R in the function `shapiro.test()`. Try it out with any of the datasets we used before. 

A p-value inferior to the chosen $\alpha$ level here (we will use 0.05), means that the null hypothesis can be rejected. Therefore, the data distribution tested is not normal and you need to use a non-parametric test.


## Day 5: Multiple testing and regression

### Multiple testing  

In the previous examples we were only testing for one single hypothesis. However, it is common in biology that we have more than one hypothesis being tested.  
The table below is a reminder of which errors can be made when testing hypotheses:

```{r echo = F}
decision = c("Decision: Reject H0", "Decision: Do not reject H0")
h0_true = c("Type I Error", "Correct Decision")
h0_false = c("Correct Decision", "Type II Error")

knitr::kable(data.frame(decision,
                        h0_true,
                        h0_false), col.names = c("", "H0 is true", "H0 is false"))
```

Decision is associated with $\alpha$, so if the p-value is under 0.05, we reject the null hypothesis. However, when testing multiple hypothesis, we are prone to erroneously rejecting the 1st, 2nd… or Nth null hypothesis. I.e., if we test 1000 hypotheses simultaneously, we expect to erroneously reject 50 (5%) just by chance!  

The **family wise error rate (FWER)** is the probability of making at least one Type I error and we can control this probability using a p-value correction. 

#### Example

Imagine that you are trying to test the effect of a treatment A on a cell line in the lab. You run a gene expression experiment to compare treatment with control (untreated) for a panel of 20 genes you pre-selected.  

The comparison between treatment and control for the panel gives you a set p-values of the 20 genes
```{r}
p.vals = c(0.1, 0.0001, 0.04, 1.2e-20, 0.002, 0.01, 0.5, 1, 0.02, 1.9e-5,
           1.1e-6, 0.03, 0.7, 0.06, 0.01, 0.3, 0.05, 1e-13, 0.032, 0.004)
```

But since you ran 20 different tests to generate these, there is a need to correct the p-values. You can do this on the p-values directly using the `p.adjust()` function. Run `?p.adjust()` to see which methods can be used. 

We will use the false discovery rate (FDR) correction on this example. 
```{r}
p.vals_FDR = p.adjust(p.vals, method="fdr")
p.vals_FDR
```

Check how the p-values change!

```{r, echo = F, out.width="50%", results='hold'}
## No need to understand this code;
results = data.frame(Genes = paste0("gene", 1:20),
                     Uncorr = p.vals, 
                     FDR = p.vals_FDR)
ggplot(results, 
       aes(y = Uncorr,
           x = Genes)) +
  geom_point(aes(colour = ifelse(Uncorr <= 0.05, "Significant", "N.S"))) +
  theme(axis.text.x = element_blank()) +
  scale_colour_manual(values = c("grey", "forestgreen")) +
  labs(y = "uncorrected p-value", title = "Uncorrected results",
       colour = "Outcome")

ggplot(results, 
       aes(y = FDR,
           x = Genes)) +
  geom_point(aes(colour = ifelse(FDR <= 0.05, "Significant", "N.S"))) +
  theme(axis.text.x = element_blank()) +
  scale_colour_manual(values = c("grey", "forestgreen")) +
  labs(title = "Corrected results",
       colour = "Outcome")
```

> How many genes change from being significant to non-significant? How does this impact your experiment?

### Regression

In this last part, we want to learn how to build a **regression model** in order to make predictions on certain quantitative variables using other quantitative variables. The important steps here are:

-   **learning** the model
-   **testing** the model to evaluate its performances, and also check that the assumptions of the linearity are given
-   **predict** values based on new data points

We will use again the diabetes data set, and build a simple linear regression models with a single explanatory variable.
We will focus on predicting the **cholesterol level**.


Load the data and perform some basic inspection on the data:

```{r}
tmp = read.table('https://www.dropbox.com/s/zviurze7c85quyw/diabetes_full.csv?dl=1',header=TRUE,sep="\t") 
```

We will limit the dataset to the numerical variables

```{r}
dat = tmp %>%
  select(
    where(is.numeric)
  )
head(dat)
```

First, we have to do some data cleaning and remove all patients with at least 1 "NA". Use `na.omit()`.
```{r}
# Select the patients without NAs
dat = dat %>%
  na.omit()
```


Then, we can make a heatmap and visualize the correlation values between each of the variables in the dataset.
```{r}
library(pheatmap)
cor.vals = cor(dat, method = "spearman")

pheatmap(cor.vals,
         cluster_cols = FALSE,
         cluster_rows = FALSE,
         display_numbers = TRUE)
```

> What are the strongest correlations? Do they make sense?

Apart from displaying the raw correlation values, we can test if any of those are **statistically significant**, i.e. if they are significantly positive (one-sided test), negative (one-sided test), or non zero (two-sided test). To do so, we will use `cor.test` function.

For example, there seems to be a positive correlation between *stabilized glucose* (stab.glu) and *hip circumference* (hip).

```{r}
## compute correlation
cor(dat$stab.glu,dat$hip)
##
## test for significance
cor.test(dat$stab.glu, dat$hip)
```

> Read carefully this output, and make sure you understand it. Check other pairs!

#### Univariate linear regression

Next, we will assess what is the most promising variable to predict cholesterol level. Go back to the correlation heatmap to see if there are variables highly correlated with it.

We will use glycosilated hemoglobin (`glyhb`) as a predictor of the cholesterol level and the function `lm()`.

```{r}
l.g = lm(chol ~ glyhb, data=dat)
summary(l.g)
```

For a simple linear regression (with only one explanatory variable), the p-value of the t-test for the slope is identical with the p-value of the F-test for the global model. *This will no longer be the case when including several explanatory variables!*

By the way, have we checked that a linear regression makes sense in this case? Remember that we have to check that:

-   the residuals are **normally distributed**
-   there is **no correlation** between the residuals and the explanatory variable

```{r, fig.height=4, fig.width=4, message=FALSE}
# normal distribution of residuals?
ggplot() +
  geom_histogram(aes(x = l.g$residuals)) 

ggplot() +
  geom_qq(aes(sample = l.g$residuals)) +
  geom_qq_line(aes(sample = l.g$residuals),
               colour = 'red')

## correlation residuals x-values?
corr.gly_resi = cor(dat$glyhb,l.g$residuals)

ggplot() +
  geom_point(aes(x = dat$glyhb, y = l.g$residuals))
```

> What is your overall opinion about the validity of the regression model here?

We can now use the model to predict the cholesterol values, and compare them to the real cholesterol values, since we have the information in this dataset.

```{r, fig.height=4, fig.width=4}
ggplot() +
  geom_point(aes(x = dat$chol, 
                 y = l.g$fitted.values),
             colour = "navy") +
  labs(x = 'Real values',
       y = 'Predicted values') +
  geom_abline(intercept = 0, slope = 1, 
              color = 'red')
```

Not really super convincing, right? Let's put more information into the model!

#### Multiple regression model

Let us include all the information to try to predict cholesterol level:

```{r}
l.all = lm(chol ~ .,data=dat)
summary(l.all)
```

> Do you note something unexpected in this report?

We can see that the inclusion of several explanatory variables improves the regression. Check the $R^2$ values for example!

We can see that the variables `weight`, `waist` and `hip` do not reach the significance level. Two explanations are possible

1.  either these variables are indeed non-informative regarding the prediction of the cholesterol level
2.  or the mutual correlation between these 3 variables interferes with the model.

We can remove waist and hip for example, and redo the regression

```{r}
l.less = lm(chol ~ stab.glu + hdl + glyhb + age + height + weight  + bp.1s + bp.1d,data=dat)
summary(l.less)
```

We can see how the `weight` variable seems indeed to contribute to the prediction of the cholesterol. The removal of the strongly correlated variables has increased its significance. It now almost reaches significance at the 5% level!

> Check the result of the F-test to compare both models!

We can now check if the prediction are better that with the univariate model

```{r, fig.height=4, fig.width=4}
ggplot() +
  geom_point(aes(x = dat$chol, 
                 y = l.less$fitted.values),
             colour = "navy") +
  labs(x = 'Real values',
       y = 'Predicted values') +
  geom_abline(intercept = 0, slope = 1, 
              color = 'red')
```

Better? I would say so... To determine the accuracy, we can compute the so called **root mean squared error (RMSE)**:

$$
RMSE = \sqrt{\frac{1}{n}\sum_{i=1}^n (x_i-\hat{x_i})^2}
$$

```{r}
n = nrow(dat)
rmse = sqrt(1/n*sum(l.less$residuals^2))
rmse
```

Of course, this is cheating, right? We are predicting the values on **exactly** the same data we used to learn the model. In real machine learning, we need to perform **cross-validation**, i.e. learn the model on one part of the data (*training set*) and validate it on another set (*test set*).

Let us split the dataset in a test and training set randomly:

```{r}
set.seed(1234)
## take 200 random patients to form the training set
i.train = sample(1:nrow(dat),100)
##
dat.train = dat[i.train,]
dat.test = dat[-i.train,]
```



We now learn a new model on the train dataset:

```{r}
l.train = lm(chol ~ stab.glu + hdl + glyhb + age + height + weight  + bp.1s + bp.1d,data=dat.train)
summary(l.train)
```

We can compute the RMSE for the training dataset

```{r}
n.train = nrow(dat.train)
rmse.train = sqrt(1/n.train*sum(l.train$residuals^2))
rmse.train
```

Let use use that model to predict cholesterol values for the left out test set

```{r}
pred = predict(l.train, newdata = dat.test)
```

and compute the rmse:

```{r}
n.test = nrow(dat.test)
residuals = dat.test$chol - pred
rmse.test = sqrt(1/n.test*sum(residuals^2))
rmse.test
```

Of course, the RMSE is higher on the test dataset, since this does not include the data used for the establishment of the regression model; however, this is a more realistic estimation of the validity of the model, as it indicates how well the model could be extended to novel, independent data!

An important topic here is **feature selection**, i.e. finding the optimal and minimal set of explanatory variables that allow to predict well the output variable.


We now repeat the train/test split 10 times with each time a different random split; plot the 10 `rmse.train` and `rmse.test` values!
```{r}
set.seed(345)
RMSE <- sapply(1:10, function(x) {
  i.train = sample (1:nrow(dat),100)
  ##
  dat.train = dat[i.train,]
  dat.test = dat[-i.train,]
  ##
  l.train = lm(chol ~ stab.glu + hdl + glyhb + age + height + weight  + bp.1s + bp.1d,data=dat.train)
  ##
  n.train = nrow(dat.train)
  rmse.train = sqrt(1/n.train*sum(l.train$residuals^2))
  ##
  pred = predict(l.train,newdata = dat.test)
  ##
  n.test = nrow(dat.test)
  residuals = dat.test$chol - pred
  rmse.test = sqrt(1/n.test*sum(residuals^2))
  RMSE <- c(rmse.train,rmse.test)
  RMSE
})
#

ggplot() +
  geom_point(aes(y = RMSE[1,],
                 x = 1:10, colour = "gold")) +
  geom_point(aes(y = RMSE[2,],
                 x = 1:10, colour = "steelblue")) +
  scale_x_continuous(breaks=c(2,4,6,8,10)) +
  geom_hline(yintercept = mean(RMSE[1,]),
             colour = "gold") +
  ylim(min(RMSE), max(RMSE +2)) +
  geom_hline(yintercept = mean(RMSE[2,]),
             colour = "steelblue") +
  scale_colour_manual(name = "RMSE", guide = 'legend',
                      values =c('gold'='gold','steelblue'='steelblue'), 
                      labels = c('train','test')) +
  labs(x = "Iteration", y = "RMSE values")
```









<!--chapter:end:01-week.Rmd-->

# Week 2 : sequence analysis {.unnumbered}

## Day 1: Annotathon! Identifying ORFs

The Annotathon project aims to give the students the opportunity to learn how to annotate unidentified sequences while helping the scientific community. In this course, the results you will produce will help progress on the discovery of new species or advance knowledge on known ones.

Each group will be affected an unknown sequence of DNA. Your mission, if you accept it, is to analyse this sequence and to gather the maximum of information you can on it.

Which means that there is no already known answer for the questions you will have. Each of you will follow the same bioinformatic process but the results will be uniq to the sequence.

### Getting ready

#### Creating an account

To create you account on https://annotathon.org/, clic on the "*New account*" tab
![](figures/Annotathon/Annot_create_account.png)
Follow the instructions to open a new account; make sure you select the appropriate Team

![](figures/Annotathon/Annot_select_affiliation.png)
And the correct "**Team code**"

**GKBIOINFO2024**

You are required to enter at least one firstname/lastname pair, and one email address in order to receive Annotathon specific notifications. Your email address is secure and will under no circumstance be made public or passed on to any third party. Only low traffic messages specific to your course duration will be mailed to this address; no further messages will be sent after the course is completed.

Finally a clic on "**Open account**" should be followed by the message "**Account 'XYZ' has been created**". Use your 'username' and 'password' and clic "**Connect**" in the form at top of page to open an Annotathon session. You will be reminded that your email address is not validated until you have followed the special link included in an email automatically sent to you at account creation.

#### Reading the doc

To guide you for the Annotathon, you have the document you are reading right now that contains the main informations and guidelines. To go further in details, you can access a complete "**Rule Book**" directly on the website.

![](figures/Annotathon/Annot_access_rulebook.png)

### Getting started

#### Getting the sequence

The first step is to get your sequence. To do that, go to "**Cart**", select the sample and clic "**Add a new sequence to your cart**".

![](figures/Annotathon/Annot_add_sequence_to_cart.png)

You now have your sequence and should be on a page with a lot of informations about your sequence. (This is the new look of your "**Cart**" page)

![](figures/Annotathon/Annot_your_sequence.png)


- In this example, GOS_11742010.1 is the name of the sequence i am analysing.
- In the "**Actions**" section, the eye is to view the state of your annotation, the notebook is to change the annotation
- the "**version**" is to go back to a previous save of your annotation
- the "**Message**" section is to send messages to me in case you have questions
- "**Genomic Sequence**" is to get your actual sequence in a FASTA format
- ...

#### And now what ??

Well, now it is time to dive into the world of sequence annotation. You will follow a protocol described in the next chapter. For each step you will have to fill a report directly on the website. To start editing the report, clic on the notebook.

![](figures/Annotathon/Annot_edit_report.png)

Every step of the analysis will be written following the same pattern :

Protocol :
Describe what tool is used, how it is used ...

-----------------------------------------
Results Analysis:
your observations

-----------------------------------------
Raw results:
output of the tool used

### Finding ORFs

Steps to find the Open Reading Frame (ORF)

#### copy your fasta sequence

![](figures/Annotathon/Annot_copy_sequence.png)
go to : https://www.ncbi.nlm.nih.gov/orffinder/ 

Paste your sequence and choose the parameters (do not forget to note the selected parameters to your report).

For this study, you will only consider ORFs that verify the following criteria:

1. do not contain any **STOP codons** (basic ORF definition...)
2. contains at least **60 codons**
3. can be on either direct or reverse **strands**
4. can be in **frames 1, 2 or 3** on each strand
5. can be **complete or incomplete** at the 5' or 3' ends, or both!

![](figures/Annotathon/Annot_ORF_input.png)
Then clic on submit and you will have your first output for your sequence, congratulations !!

![](figures/Annotathon/Annot_ORF_output.png)
Now you can gather all of your ORFs sequences into your report (FASTA format) on the Annotathon website (only the ones that are more than 60 aa).


This should be your first entry to the annotathon report page, so it is time to talk about one really important thing here to avoid frustration : **REMEMBER TO SAVE YOUR WORK !!!!!!**
To do that, you clic on the "**save your annotations**" button on the top of the page.


Gather informations about all the ORFs in a table like this one :

![](figures/Annotathon/Annot_table1_ORF.png)

then, make a representation of the ORFs on a schematic figure :

![](figures/Annotathon/Annot_figure1_ORF.png)

If you do have multiple ORFs, just arbitrary select the biggest one. If you want more details on this part, you can refer to the rule book, but here we will skip the classification of each ORF part.

You now have one ORF that you will annotate deeply, do remember to keep the sequence of the ORF on your report and to **save your work**.

### Conserved protein domains

Your new mission is now to find if your ORF contains any known domains (comparison to databases)
go to https://www.ebi.ac.uk/jdispatcher/pfa/iprscan5

![](figures/Annotathon/Annot_protein_domain.png)
Copy paste your sequence, keep all the databases selected, put a name for the run and clic on submit. After 2 - 5 minutes you will obtain your results. (you need to use the protein sequence here)

Then copy the raw output to your report and make a table with the informations as described here:

![](figures/Annotathon/Annot_table2_prot_domain.png)

then analyse your results following the next guidelines :

**1. Selected domains** (if present)

    - Which domains predictions do you select to annotate the ORF? Specifiy their sizes, E-value! Justify your selection!
    - Refer clearly to Table 2 for your detailled analysis.

**2. Rejected domains** (if present)

    - Why are some functional domains rejected? (high E-value?  No IPR domains?)

Here you have to make choices based on the output of interproscan and other ressources you could find on the internet. Do remember to cite any exterior tool and also to **save your work**.

## Day 2: Annotathon! Alignments

### BLAST homolog search

BLAST is a well known tool to identify homologs of sequences. To use it, first go here :
https://blast.ncbi.nlm.nih.gov/Blast.cgi?PROGRAM=blastp&PAGE_TYPE=BlastSearch&LINK_LOC=blasthome
Here, since we already have the protein sequence we will use the BLASTp (protein BLAST).

The BLAST will be done twice, once against the NR database

![](figures/Annotathon/Annot_BLASTp_NR.png)
and once against the swissprot database (just change the database in the yellow part). Remember to copy the raw results for both and to note the informations to create this table :

![](figures/Annotathon/Annot_table3_BLASTp.png)
Then, using the raw results and this tool : https://annotathon.org/outils/blast_definition_list.php
create a definition list.

![](figures/Annotathon/Annot_BLAST_hit_list.png)
look at your list and merge the similar categories into a table in your results with this format :

![](figures/Annotathon/Annot_table4_BLAST.png)
Then, take time to discuss your analysis following those guidelines:

**RESULTS ANALYSIS**
Do not continue the analysis of the fragment if:

- No homologs (or very small number of homologs in NR database, ie <100)
- Your gene is already present in the NR biological database (nucleotide BLASTn with **ID > 95%**)

_**Proposed structure of your analysis section:**_

**1. Overview of the alignments**

- Synthetic description of your alignment results (number, known functions, quality of the alignments, ...)
- Give details on E-value range, % identity/similarity range, what about indels, alignment coverage, ....)

**2. Identification of protein homologs**

   - Justify E-value thresholds (NR & SP) (Evalue cutoffs, changes in putative homolog functions), refer to tables 3 & 4.

**3. Function of homologs from SWISSPROT analysis**

   - From SP entries, give details on closest homolog functions, specific role of important amino acids involved in catalytic function, cross your analysis with protein domain analysis. In all cases, cite your sources with web links.
   
### BLAST Taxonomic report

Using the output of your BLAST analysis you will now be able to construct a taxonomic report. To do that, go to this website https://annotathon.org/outils/blast_tax_report2.php , and copy your BLAST hits.

![](figures/Annotathon/Annot_Taxonomy_from_BLAST.png)
Hit the **Taxonomy Report** button. Copy the output to your report. Then, use this second tool : https://annotathon.org/outils/blast_taxonomy_list.php to produce a summary table.

![](figures/Annotathon/Annot_taxonomy_summary_tool.png)
hit the **Taxa table** button and you will get a summary table like this :

![](figures/Annotathon/Annot_taxonomy_summary.png)

you can change the classification level parameter to have a more precise prediction

### Selecting sequences for multiple alignment

Now, using the previous results, you will create two groups of sequences :

- The study group (20 - 30 sequences) is composed with sequences of the same taxonomic group than your ORF and with a good E-value (the lowest possible).

- The external group (10 - 15 sequences) with the closest homologs outside of the taxonomic group of your ORF (Lowest e-value here too)

keep the amino acide sequences in a FASTA format.


## Day 3: Annotathon! Phylogenetic trees and conclusion

### Multiple alignments

Now that you have your list of sequences you can go to this website to do a multiple sequence alignment : http://www.phylogeny.fr/simple_phylogeny.cgi?workflow_id=b02e40313c3ca8c0e3d4dbc97db10078&tab_index=2

Copy paste your sequences here :
![](figures/Annotathon/Anno_Multiple_sequence_alignment.png)
and clic submit.

The limitation in the number of sequences to align is simply due to computation time of multiple alignment programs, as well as subsequent phylogenetic tree reconstruction. Computation time is reasonable up to around thirty or fifty sequences of a few hundred residues.

Copy & paste the **"ClustalW" formated** multiple alignment in the '**Multiple Alignement**' Annotathon field.

![](figures/Annotathon/Annot_Clustal_format.png)
**RESULTS ANALYSIS:**
**1. Quality of the multiple alignment**

   - Can you confirm that that sequences are really homologs? Similar lengths? How many identical positions? How many conservative substitutions positions? Number of indels? can you find that the conservation of sequences within alignment reflects the subgroups (In and out groups)? 
  - After curation with GBLOCKS, what is the number of conserved homolog positions (informative sites) for phylogenetic reconstruction? It is enough? 

**2. Identification of conserved blocks**

  - You can annotate well conserved blocks in your alignment with codes (such as A, B, C etc.) and refer to them in your analysis. 
  - Are there any conserved amino acids that are known as actives sites for this protein family? If yes, position in alignment, function, activity? 

**3. N and C-termini of the studied ORF**

   - Analysis of the N-ter/C-term of the alignment (complete? start codon? potentially missing number of amino acids in N and C-termini?)
   
### Phylogeny

To construct the phylogenic tree :
Use the previous multiple alignment to infer a phylogenetic tree using two distinct tree reconstruction approaches:

- 'distance' method (e.g. '**neighbor-joining (NJ)**', '**BioNJ**' or '**Phylip protdist/neighbor**')
- 'maximun likelyhood' method (e.g. '**PhyML)**')

Go to this place : http://www.phylogeny.fr/alacarte.cgi

![](figures/Annotathon/Annot_phylogeny_alacarte.png)
Once you have the two versions for the report, you can play with the different parameters to see how the results are impacted.

Then use this tool : https://annotathon.org/outils/nw_utils.php
to create a tree to the format expected for your report. (copy the NEWICK format of your tree and paste it in the box)

![](figures/Annotathon/Annot_newick_to_annot_tree.png)

Your result should look like this :
![](figures/Annotathon/Annot_newick_example.png)

Copy & paste the **textual** tree representation in the '**Tree**' Annotathon field. Remember to include a protocol line in the '**Tree**' field that includes the program name and run parameters (ex 'Phylip / Protdist+neighbor / Randomized input - Random number seed = 11 / rooted on: Coccidioides immitis (ascomycetes)').

**Suggested plan for the analysis section:

**1. Tree topologies**

    - Describe the topology of each tree. What are the monophyletic groups?
    - Do the two independant trees describe the same evolutionary history? the same topology? Similar or different clades? 
    - Identify the commonalities as well as the potential incoherencies.

**2. Coherence with reference trees**

	- Are the in- and out-groups correctly separated?
	- Are your gene trees coherent with the reference species trees ("tree of life")? 
	- Identify each discrepancy with the reference species tree, and suggest some explanations (HGTs, gene duplications...).

**3. Predict the most likely taxonomic origin of the metagenomic ORF**

    - In which monophyletic clade does the the metagenomic sequence seem to emerge?
    - Propose a hypothetical taxonomic classification for the metagenomic ORF!
    - Provide detailed justification of your hypothesis, do not under/over interpret the infered phylogenetic trees!


After you have analysed the phylogenetic tree produced, specify the most likely taxonomic group (e.g. "Alphaproteobacteria") to which belongs the organism carrying your DNA fragment. To specify this group in the '**Taxonomy**' Annotathon field you have two options:

- specify in the '**NCBI numerical identifier**' box the taxonomic group code (for instance 204455 for _Rhodobacterales_, these codes are found in GENBANK records in the feature table, such as /db_xref="taxon:204455" or can be found by querying the NCBI taxonomy database using the link in the Help tab)
- specify the exact scientific name for this group in the '**Scientific name**' box (e.g. _Rhodobacterales_)


### Conclusion

Write up your interpretations and hypotheses based on the observations you have made in the preceeding "RESULTS ANALYSES". Imagine you are trying to convince a very sceptical colleague: use rigorous argumentation, cite precise evidence and numerical values when ever possible, highlight important findings, cross information from independent sources. Remember that in silico analyses generally do not constitute final proof, only suggestions. Terms such as "putative", "suggests" or "probably" can show understanding of the limitations of computational biology results.
Make sure you have at least covered:

-   arguments supporting your coding versus non-coding hypothesis; make sure you discuss the start position of your ORF (refer to the FAQ for all the subtlities and pitfalls)!
-   functionnal predictions for the protein, both at the biochemical level (e.g. "ubiquitin conjugation enzyme"), and the biological role at the organism level (e.g. "implicated in the control of the cell cycle"). Make careful use of annotations available for sequence homologs or conserved domains
-   your taxonomic classification hypothesis for the organism carrying this DNA fragment.

Some common pitfalls to avoid at all cost:

-   explain the theoretical aims and methodology of the tools used (please consider the readers knowledgable in these matters)
-   describe on which button you clicked (please consider that readers are highly familiar with running BLASTs)
-   write in telegraphic style
-   dilute, inflate, digress in the hope that evaluation is proportional to word volume
-   copy raw results in extenso in the discussion when the reader has direct access to them in the appropriate fields
-   write linearly without any structure, or purely chronologically
-   insulate the analyses from each other (you can, and certainly should, make reference to the multiple alignment while discussing the ORF boundaries)
-   conclude without references to the results and observations
-   propose hypotheses without providing the supporting evidence
-   make approximate statements, such as citing BLAST homologs or conserved domains without citing their respective E-values

Concentrate on producing a scientific, structured, synthetic and rigorous argumentation that will hold up to peer scrutiny!


## Day 4: Data formats and where to find them

Today we will go through commonly used tools and databases applied in genomic research. You will learn how to:

-   Identify common data formats and know how they are produced;\
-   Search and obtain genomic data from commonly used databases - GEO;\
-   Handle different data formats used for transcriptomics and epigenomics;\
-   Visualize genomic data in IGV.

### Meet IGV

Alongside the next sections, you will find some examples of files in IGV Viewer and you will use it in the exercises. Access it [here](https://igv.org/app/). We will work only with the Web app, but IGV provides a local software that is quite versatile too. 

![Menu overview in IGV Viewer (web app). Along this course, you will load local files and ENCODE tracks into IGV Viewer. This can be done in the *Tracks* menu (pink arrow). Genome assembly needs to be adjusted according to the local file you are importing (orange arrow). We will mostly work with hg38 and mm10, the most commonly applied assemblies for human and mouse respectively. In addition, we will import a session from a local file too (green arrow).](figures/IGVintro.png)

Below this menu, you can select genome locations (in the format *chrN:start-end*) and search for genes. 

#### Example 1

Try IGV out using [this session](IGV_sessions/heart_Histones.json). 
Do you recognize the differences between the different histone modifications? Blue represents active histone modifications (linked to active transcription). H3K27ac is a classical marker of active gene promoters and other open chromatin regions. H3K36me3 is linked with gene bodies.  

Conversely, red represents repressive histone modifications. H3K27me3 and H3K9me3 are both associated with repression/heterochromatin. However, H3K9me3 is meant to be much more permanent, while H3K27me3 is often associated with regions that can be activated in development.


> What genome assembly is being used?

> Inspect the HOXD gene cluster (Find it in *chr2:175,992,177-176,263,537*). Based on these signal tracks, do you think this cluster of genes is being expressed in heart?

> Check different genes (eg. BDP1, NPLOC4, PIK3C3) and observed the shape of the H3K27ac (activation) signal at their promoters. 


### Identifying formats

Genomic data will generally fit into one of four classes:

-   Nucleotide sequences (like FASTQ);
-   Read alignments (like BAM files);
-   Annotations (as in GTF/GFF files);
-   Quantitative outputs (like count tables, peaks, among others).

Different formats and/or classes will be processed and visualised in different ways because they have distinct features.

Let us start with an example of a **FASTQ file**:

FASTQ files are text-based (as many other sequence classes) and are the **most common format for storage of sequences and quality scores from high-throughput sequencing technologies**. These are large files, so they are generally compressed (extension `.fastq.gz`). When opening a FASTQ file (using `zless` or `less` in the terminal), this what an entry will generally look like:

```         
@@A00665:126:HTNC2DRXX:1:2101:1136:1047 1:N:0:AGGCAGAA+NTAAGGAG
GNCCTTACTAGACCAATGGGACTTAAACCCACAAACACTTAGTTAACAGCT
+
F#:FFFFFFFFF:FFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFF:FF
```

One FASTQ file contains millions of these entries. Each entry (starting with `@`) is composed of a sequence identifier (in this case, `@A00665:126:HTNC2DRXX ...`), the sequence itself, a separator (in this case, `+`), and the quality scores (Phred +33 encoded) for each base called. Sequence identifiers include information which allows us to know the name of the instrument which produced it, run identifiers, flowcell identifiers, pair number, index sequence, among others.

This example is from paired-end sequencing, meaning that there are two FASTQ files per sample and entries match between them. I.e., `@A00665:126:HTNC2DRXX` will also exist in FASTQ file 2. When the sequencing is single-end, only one FASTQ file is produced per sample.

Upon alignment to the reference genome, the FASTQ generates a BAM file. BAM files are compressed formats of SAM files (stands for **S**equence **A**lignment/**M**ap format), and they represent read alignments to a genomic location. SAM files are tabular-based and include header (which start with `@`) and alignment lines.

![SAM format and the meaning of elements in the alignment lines. Source: <https://www.samformat.info/sam-format-flag>](https://www.samformat.info/images/sam_format_annotated_example.5108a0cd.jpg)

Alignment lines, corresponding to reads, will include different elements as shown above. One of the most important is the **flag**, which includes information on the alignment of a given read. The flag (in red) is just a number but it represents different combinations of read properties as you can see here [here](https://broadinstitute.github.io/picard/explain-flags.html). Flags can be used to remove unmapped read pairs or failing reads from further downstream processing.

> Check in the link above what would the flag 99, 512, and 13 mean in a SAM file.

Because these are large files, it is much more common to see read alignments in BAM format. However, BAM files are not human-readable as FASTQ is. You will visualize a BAM file later on in IGV. This is what it looks like:

![Example of BAM file for a genomic region within the NPLOC4 gene as present in IGV example session. Grey rectangules represent reads.](figures/bamexample.png)

While FASTQ and read alignment formats are generally common between different kinds of assays, the output after alignments depends on the assay. Let us explore different formats and to which assay they are associated to.

#### Where do different formats come from?

![File outputs after alignment for common experiments.](figures/fileformatorigin.png)

Assays meant to assess chromatin accessibility, transcription factors, or histone modifications are typically based in peaks, which are saved in a BED-like format. A BED file is structured as:

```
chr1	9922	10469	
chr1	564407	565441	
chr1	565595	567010	
chr1	2583259	2588018	
chr1	2627935	2629661	
```

In this example, each line is a detected peak at a given location in the genome (indicated as chromosome  start  end). Additional fields (like p-value, log fold change, length, among others) are often included but depend on the software that generated them. 


Chromatin accessibility, transcription factors, or histone modifications can also be represented in BigWig/bedGraph format. This formats, which look like summits, represent *signal*. Some assays require a control to distinguish this signal from noise (in ChIP-seq, this is usually refered to as Input). 

Let us see an example of BigWigs and peaks.

#### Example 2

Load the session [here](IGV_sessions/ExampleATAC_ChIP_CTCF_hg38.json) to see an example of how ChIP-seq and ATAC-seq correlate with each other. 

Observe how peaks (rectangles below the coverage tracks) are not always identified for all the "bumps" in the BigWig.

### A practical guide to GEO

[GEO](https://www.ncbi.nlm.nih.gov/gds/), standing for Gene Expression Omnibus is one of the most used repositories for microarray, next-generation sequencing, and other genomic datasets. When publishing a manuscript, authors must upload the sequencing data they produced into a public database to make it accessible to others, and GEO is often the chosen one. For this reason, GEO includes thousands of original submitter-supplied records that can be accessed from any place by anyone. 

We will learn how to search in GEO and how to read a GEO record to obtain specific information. 
Datasets in GEO can be found directly through a valid GEO accession number. 

#### Using filters and querying

Searching for datasets is not always easy because although some aspects of GEO are common, others are not. Submitters have freedom to adapt descriptions fitting to their specific submission and experiment, but that can make things harder when filtering for datasets.

You can see the global aspect of GEO search here:

![GEO search: On the top, you can write your own query. Left menus indicate filters that are then added to your query. Try it out with different organisms and study types. Attribute types can also be applied. You can see suggested organisms on the right (yellow). Additional filters can also be seen (eg. *Supplementary file* to see only records including BED, WIG, ...).](figures/GEOfilters.png)

The study section is one of the most important, as it indicates what kind of assay was performed and for which goal. For example, you would apply *Expression profiling ...* when looking for transcription data and *Genome binding/occupancy profiling ... * when looking for chromatin accessibility, histone modifications, or transcription factor data. In addition, you can also select the type of assay (*high throughput sequencing*, *array*, *Mass Spec*, ...). 

After this, you are ready to dive into a GEO record. Feel free to open one to see information inside. 

#### GEO records

![This is a GEO record. Appearance and amount of information can widely vary, particularly in summary and overall design (red). Inside each record, you will find multiple samples (green). These include information on any experimental or phenotypic differences (knock-out, wild type, treatments, among others). You will also find information on the processing of the dataset here (features like assembly, software, and other parameters). ](figures/GEOrecord.png)

Oftentimes it can be challenging to find the data you want in the format you need. While uploading data to GEO is a somewhat standardized procedure, looking for processed formats can be tricky. 

At the bottom of each GEO record, you can find the **Supplementary files**. This typically includes a collection (zipped) of all supplementary files (which you can access individually by selecting a single sample). 


##### Example 3

In this example, we will explore a GEO record and obtain information about data processing from it. 
Search for the dataset identified as **GSE205807**. 

> What cell line was this dataset generated from?

> What species and genome assembly is used for alignment?

> What kind of processed files are present in each sample?  


### EXERCISES

#### Exercise 1. Finding transcription factor targets using ChIP-seq

ChIP-seq can be used to identify genes regulated by targeted transcription factors. In the example, we will explore a dataset obtained in the human liver cell line HepG2.

1.  Download the set of peaks for the FOS transcription factor from the GEO dataset GSE104247.

1.1. What genome assembly is this dataset aligned to? Apply the necessary changes to see it in IGV. 

1.2. Load the dataset in the IGV web app. Give a couple of examples of genes that are likely regulated by FOS in this cell line.

1.3. Pick a set of peaks obtained in your favorite transcription factor from the same GEO dataset! How is the set of peaks you observed before comparable? 


##### Exercise 2. ATAC-seq and DNase-seq comparison

2.1. Load a track in BED and BigWig format from ATAC-seq and DNase-seq obtained in the same cell line using the *Tracks* section from *ENCODE Other* (BED) and *ENCODE Signals - Other* (Tip: There is typically plenty of material included for the A549 cell line). Compare the called peaks and other features from each experiment (peak size, noise, coverage) and discuss the advantages one has over the other.  

2.2 (Optional) Load a *4DN* track on insulation score for your cell line. This is from the 4D Nucleome Data Portal and it is a way to quantify the number of regions interacting within a window. At chromosomal scale, do you see how this correlates (or not) with the ATAC-seq track? What are the regions that present a high insulation score?

##### Exercise 3. Using GEO to look for DNA methylation datasets

3.1. What filters would you apply to look for a dataset of DNA methylation array which includes rhesus macaque (*Macaca mulatta*) and gorilla (*Gorilla gorilla*)? 

3.2. (optional) You want to find multiple transcriptomics datasets from *human tissues* infected with malaria parasites. What filters would you apply? 


##### Exercise 4. Going further

Let us see a different type of data - interaction data from ChIA-PET. Re-open the session [here](IGV_sessions/ExampleATAC_ChIP_CTCF_hg38.json) we used for example 2. Then, load a ChIA-PET *loop* track from A549 (genome-wide chromatin interactions mediated by *CTCF*) using **Tracks > ENCODE Other**. 

CTCF is an important element in genome organisation and ChIA-PET is an assay used to measure chromatin interactions mediated by specific factors.

Observe interactions at chromosomal and gene level. What can you conclude from the comparison between ATAC-seq, CTCF ChIP-seq, and ChIA-PET? 

> Is an interaction mediated by CTCF always associated with a CTCF peak? Why? 


## Day 5: RNA-seq - from FASTQ to count matrix

Today we will analyze the RNA sequencing data. 

We will use the RNA-seq data from this paper: Nature volume 571, pages505–509 (2019). The raw sequence data is stored at: https://www.ebi.ac.uk/biostudies/arrayexpress/studies/E-MTAB-6798/sdrf

We will take two samples from there. One from the 2 week post-conception (2WPC) mouse brain and the other from 4WPC brain.

You can find the raw data here: 
```{Raw data}
/vol/data/raw/mouse_brain_2wpb.fastq.gz


/vol/data/raw/mouse_brain_4wpb.fastq.gz
```

We will start with these `fastq` files, align them to the reference genome and quantify the gene expression. You can find the workflow below.

![Workflow for the RNA-seq analysis](figures/workflow.png)



### Quality control

We retrieve the raw data and employ a software tool named `fastqc` to examine the quality of the sequence files.


We next examine the quality of the samples with `fastqc`. 

```{fastqc}
mkdir path/to/your_fastqc_folder

fastqc /vol/data/raw/mouse_brain_4wpb.fastq.gz -o path/to/your_fastqc_folder/

fastqc /vol/data/raw/mouse_brain_2wpb.fastq.gz -o path/to/your_fastqc_folder/

```

The start of the FastQC quality report provides basic statistics and an evaluation of various quality parameters.

![](figures/fastqc_statistics.png)


> Question: What information do you get from the QC report?


In particular, we will look at the (1) per base quality, and (2) per sequence quality.


![Per base quality from the fastqc report. (Top) The result from the 2 week post-conception (2WPC) mouse brain. (Bottom) The result from the 4WPC mouse brain](figures/per_base_quality.png)

![Per sequence quality from the fastqc report. (Top)The result from the 2WPC mouse brain. (Bottom) The result from the 4WPC mouse brain](figures/per_sequence_quality.png)



> Question: How does FastQC quantify the sequence quality?



> Question: What do you find when you compare the two sequences?


### Alignment

Alignment involves the process of mapping sequences to a reference genome. 

We use `Bowtie2` to implement seuqence alignment. Common aligners include `BWA`, `TOPHAT`, and `STAR`. They differ in their accuracy, speed, and memory usage due to different computational algorithms. 


```{}

# Create a new directory

mkdir path/to/your_bam_folder


# Alignment (It will take for a while)
bowtie2 -p 2 --no-unal -x /vol/data/raw/chr6 -U /vol/data/raw/mouse_brain_2wpb.fastq.gz | samtools view -bS - > path/to/your_bam_folder/mouse_brain_2wpb_chr6.bam 


```
> Question: How to view the BAM files? What do you find in your BAM files?
Ref: https://bookdown.org/content/24942ad6-9ed7-44e9-b214-1ea8ba9f0224/learning-the-bam-format.html
Hint: Use `samtools view` to view your BAM files.



We then use `samtools flagstat` to evaluate the quality of the aligned bam file.

```{flagstat}
samtools flagstat path/to/your_bam_folder/mouse_brain_2wpb_chr6.bam  > path/to/your_bam_folder/mouse_brain_flagstat_2wpb_chr6.txt

```


![Quality of the aligned bam file.](figures/flagstat.png)


### Quantification

Now, we utilise the aligned BAM files to quantify the gene expression in our samples. We implement this with a tool named `htseq-count`. There are various methods exist for quantifying gene expression in `htseq-count`. For further illustration, please refer to the tutorial (https://htseq.readthedocs.io/en/release_0.11.1/count.html).

```{}

# Create a new directory
mkdir path/to/your_count_folder
# Keep the high quality reads

samtools view -bq 40 path/to/your_bam_folder/mouse_brain_2wpb_chr6.bam > path/to/your_bam_folder/mouse_brain_2wpb_chr6_Q40.bam

htseq-count -s no -m intersection-nonempty --nonunique all --format bam path/to/your_bam_folder/mouse_brain_2wpb_chr6_Q40.bam /vol/data/raw/mm39.ncbiRefSeq.chr6.gtf > path/to/your_count_folder/mouse_brain_2wpb_chr6.txt

```


![Gene expression profile in 2WPC mouse brain. The first column shows the gene name. The second column shows the count. Speical counts are highlighted in red](figures/count_matrix.png)

### Visualization


```{Visualization}

library(tidyverse)
setwd("path/to/your_count_folder")


tb_2wpb <- read_tsv("mouse_brain_2wpb_chr6.txt", col_names = FALSE) %>% 
  rename(Gene = X1, Count = X2) %>% 
  mutate(Sample = "2wpb")

tb_4wpb <- read_tsv("mouse_brain_4wpb_chr6.txt", col_names = FALSE) %>% 
  rename(Gene = X1, Count = X2) %>%
  mutate(Sample = "4wpb")

tb_merge <- bind_rows(tb_2wpb, tb_4wpb)

tb_merge %>% ggplot(mapping = aes(x = log10(Count + 1), fill = Sample)) +
  geom_density(alpha = 0.2)


```
![](figures/count_distribution.png)



### Excercises

> 1. Please explore your bam files with IGV. What do you find?

> 2. Please try to run the whole workflow that you learnt today on mouse chromosome 8. The reference genome sequence can be found at /vol/data/raw. However, you are encouraged to retrieve the reference genome from the databases by yourself.  

> 3. Please explore your count matrix of these two samples. Do you find any genes that express differentially between the samples? Also, think about that, if you are a project leader, how would you design your experiment and what hypothesis testing strategy would you opt for? (Note: Formal analysis will be revealed next week)

<!--chapter:end:02-week.Rmd-->

# Project report guide {.unnumbered}

At the end of this course, you will have to produce a report in RMarkdown. RMarkdown is an inherantly ready-to publish format. Meaning that if there are no mistakes on your text, it should simply produce a PDF in one go. Let's see how.

## Knitting

Create a new RMarkdown document and start trying it out. You can already call this document *Report* if you would like to. You can also use a layout we have made available [here](https://github.com/hdsu-bioquant/bioinfo_MoBiMA/tree/main/report_example).

By clicking on the little blue yarn ball on top, you can *Knit* the document into a PDF (to PDF option can be found on the right drop arrow).

Tips to ensure you catch errors early on:

-   Knit your document every so often! If you have too much code/text that you have not verified if it's running or not, this might cause issues later on.
-   Read your errors when you have them: R will usually tell you where the error is located. Most times it's a character error as some characters will have meanings in RMarkdown and that is not always clear at first.
-   Check for missing dependencies like `dplyr`, `tidyr`, or `ggplot2` if you are using these packages.
-   Comment your code!
-   Keep things short, simplify, and avoid having a wall of text/code that is hardly decipherable.

## Formatting

New chapter titles and subtitles are defined from `#` (larger title as in 1.) to `####` (smaller title as in 1.1.1.1). Further formatting can be found [here](https://bookdown.org/yihui/rmarkdown-cookbook/formatting.html).

Numerical equations and special characters have their own formatting. For example: `$\sqrt{13}$` is square-root of 13 `$\alpha=0.05$` is alpha=0.05

Here is an equation:
```
\begin{equation} 
  f\left(k\right) = \binom{n}{k} p^k\left(1-p\right)^{n-k}
  (\#eq:binom)
\end{equation}
```

\begin{equation} 
  f\left(k\right) = \binom{n}{k} p^k\left(1-p\right)^{n-k}
  (\#eq:binom)
\end{equation}


You may refer to this equation using `\@ref(eq:binom)`.

### New pages and paragraphs

You can use `\newpage` to create new pages and `\par` to make paragraphs.

## Chunk options

There are a multitude of options to display the R chunks and their outputs. See all of them [here](https://bookdown.org/yihui/rmarkdown-cookbook/chunk-options.html). The ones will will most likely use are:

-   `echo=FALSE`: hides the code but not the results
-   `results='hide'`: hides the results but not the code
-   `include=FALSE`: does not include the chunk
-   `eval=FALSE`: does not run the chunk

Some of the ones below control figure placement and size:

-   `fig.height`
-   `fig.width`
-   `fig.align='center'`
-   `out.width` Extra: `results='hold', out.width='50%'`: will allow you to show 2 figure/plot outputs side by side.

## External images

When including external images, you need to indicate the location of such figure relative to your document: `![This is a Figure](figure_folder/avocado_image.jpeg)`

![This is a Figure. Source: <https://www.metmuseum.org/de/art/collection/search/436535>](figures/vangogh.jpeg)

For more additions like table, check the [RMarkdown cookbook](https://bookdown.org/yihui/rmarkdown-cookbook/tables.html).

Figures and tables *with captions* can also be cross-referenced from elsewhere using `\@ref(fig:chunk-label)` and `\@ref(tab:chunk-label)`, respectively.

Don't miss Table \@ref(tab:nice-tab). Reference tables using `\@ref(tab:nice-tab)`

```{r nice-tab, tidy=FALSE}
knitr::kable(
  head(pressure, 5), caption = 'Here is a nice table!',
  booktabs = TRUE
)
```

## References

Your report will very likely include some references. Each RMarkdown is accompanied by a *.bib* extension file. Indicate the name of that file in the beginning of the report (see example). You can see the one we have included in the example [here](https://github.com/hdsu-bioquant/bioinfo_MoBiMA/tree/main/report_example) too. If you open that file in a text editor, you can see that entries are structured like this:

```         
@book{ggplot2,
    author = {Hadley Wickham},
    isbn = {978-3-319-24277-4},
    publisher = {Springer-Verlag New York},
    title = {ggplot2: Elegant Graphics for Data Analysis},
    url = {https://ggplot2.tidyverse.org},
    year = {2016},
    bdsk-url-1 = {https://ggplot2.tidyverse.org}}
```

This citation format is compatible with citation managers like BibDesk (only available on macOS) or Mendeley. When you are using one of these, you are able to add in other references. Watch out for the reference key in your Markdown.

### Citations

Reference items present in your bibliography file(s) in the Markdown using `@key` or `[@key]`. For the previous example, key is actually *ggplot2*!

### Footnotes

Footnotes are put inside the square brackets after a caret `^[]`.

<!--chapter:end:03-reportInstructions.Rmd-->

